{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT: CAN THE COMPUTERS READ IT TOO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cache' # The folder we will use for storing data\n",
    "data_dir = os.path.join(cache_dir, 'data')\n",
    "model_dir = os.path.join(cache_dir, 'model')\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this notebook you will find some clean cells to clean variables in order to have the maximum space available while processing the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip already downloaded\n",
      "Archive:  blogger.zip\n",
      "replace blogs/1000331.female.37.indUnk.Leo.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n",
      "zip unziped\n"
     ]
    }
   ],
   "source": [
    "if 'blogger.zip' not in os.listdir('.'):\n",
    "    !wget -O blogger.zip http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip\n",
    "else:\n",
    "    print('zip already downloaded')\n",
    "!unzip -Z1 blogger.zip | head -1500 | sed 's| |\\\\ |g' | xargs unzip blogger.zip\n",
    "print('zip unziped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499\n"
     ]
    }
   ],
   "source": [
    "# print(len([name for name in os.listdir('blogs') if os.path.isfile(name)]))\n",
    "print(len(os.listdir('blogs/')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was pretty big (total of 19320 blogs) with maybe too much files (considering that inside each blog normally there is more than one post) so only a subset of it was unzipped. The subset selected is on the head part of the command, in this case resulted in 1499 blogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('blogs', '*.xml')\n",
    "files = glob.glob(path)\n",
    "\n",
    "with open(files[0], encoding='Windows-1252') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, each blog consists of different posts, which we will separate, as we are only interested in posts, and we don't want the dates or other info that could be inside the XML's files.\n",
    "\n",
    "As XML files they consist of different tags. We are only gonna take the **post** ones.\n",
    "    \n",
    "We used the Beautiful soup to get the texts, that because with other parsers like ElementTree it resulted in ParseError in some files, as there are some HTML elements (like &nspb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 119205: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7af62e0e2a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mblogs_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_blogs_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-7af62e0e2a7e>\u001b[0m in \u001b[0;36mread_blogs_data\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mblog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m        \u001b[0;31m# It's a file-type object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         elif len(markup) <= 256 and (\n\u001b[1;32m    288\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34mb'<'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 119205: invalid start byte"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_blogs_data(data_dir='blogs'):\n",
    "    data = []\n",
    "\n",
    "    path = os.path.join(data_dir, '*.xml')\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    for f in files:\n",
    "        blog = open(f)\n",
    "\n",
    "        soup = BeautifulSoup(blog, 'xml')\n",
    "        posts = soup.find_all('post')\n",
    "\n",
    "        for post in posts:\n",
    "            data.append(post.text)\n",
    "    return data\n",
    "\n",
    "blogs_data = read_blogs_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there is some error, something related to the decoding.\n",
    "After searching for some time we find a possible solution to which type are the files encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "blog_dir = os.path.join('blogs', os.listdir('blogs')[0])\n",
    "blog = open(blog_dir, 'rb')\n",
    "enco = chardet.detect(blog.read())['encoding']\n",
    "print(enco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we use this encoding into the parsing of all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_blogs_data(data_dir='blogs', encod = \"ISO-8859-1\" ):\n",
    "    data = []\n",
    "\n",
    "    path = os.path.join(data_dir, '*.xml')\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    for f in files:\n",
    "        blog = open(f, encoding=encod)\n",
    "\n",
    "        soup = BeautifulSoup(blog, 'xml')\n",
    "        posts = soup.find_all('post')\n",
    "\n",
    "        for post in posts:\n",
    "            data.append(post.text)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_data = read_blogs_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of posts = 176796\n"
     ]
    }
   ],
   "source": [
    "#number of posts\n",
    "print(\"# of posts = {}\".format(len(blogs_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post50:  \n",
      "\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t    urlLink Billmon  has a nice  urlLink takedown  of Barone's  urlLink latest column .  Man that guy rubs me the wrong way.   The best evidence last week's Democratic convention succeeded in accomplishing what Team Kerry set out to do is the Republican reaction, which is to insist the Boston infomercial was a false representation of both the party and the nominee.   To a certain extent, that's true - although as a progressive, I would argue it's not nearly as true as the conservative chattering class would have it. No matter how left-wing the delegates on the floor were on the hot-button issues (this was almost an hourly talking point on Fox and CNN) the face of the party establishment was pretty much the face we saw on the stage - culturally liberal on social issues, Clintonian centrists on economics, but most of all, completely pragmatic about elections and what it takes to win them.  This last quality, of course, is one the Mayberry Machiavellis also share. Which is why the criticism now coming from the right is so amusing: It mirrors almost exactly the Democratic complaints about the last Republican convention, in 2000. That, too, was derided as a false front. At times it seemed as if Rove and Co. had rounded up every non-white Republican official in the country and planted them on permanently on stage, while banishing the likes of Tom DeLay and Pat Robertson to the outer edges of prime time.  The most ridiculous example of this role reversal I've seen yet comes from Michael Barone, the effete conservative columnist turned Fox News talking head. In his convention week column, Barone railed against the Democrats for having the gall to complain about the divisive politics of the Rovian era:   Now Obama on Tuesday night, like Clinton on Monday night, said that the way to get rid of partisan division is to install the party that has vociferously promoted partisan division â that insists on the \"Bush lied\" theme ... It is like the man who murdered his mother and father and threw himself on the mercy of the court on the grounds that he was an orphan. The Democrats, having stirred up partisan turmoil and political division, now seek power on the grounds that they will eliminate them. What's amazing about Barone's diatribe is the complete absence of any awareness of how ironic it is for Republicans to complain about Democrats indulging in the politics of personal destruction on one hand, while promising to heal partisan wounds on the other.    This, of course, was precisely the political strategy pursued by the Rovians in 2000. If anything, they ran even further from the Clinton haters in their own party than the Democrats did from the Bush haters in Boston this year. And with good reason: Bush and Co. had the complete fiasco of the Clinton impeachment (and the congressional losses suffered by the GOP in 1998) to remind them how much swing voters hated the anti-Clinton moral hysteria.  Barone's bitching, in other words, sounds more like a claim of copyright infringement than a legimate gripe about Democratic hypocrisy. Imitation, after all, isn't just the most sincere form of flattery; it's also the heart and soul of competition.  It would seem, in other words, that both sides have learned the utility of the good cop/bad cop routine in an age when the left and the right both want red meat while the voters in the middle generally prefer their politics with a side order of Valium.   As the party out of power, the Democrats are naturally better positioned to put the technique to good use now. And, since the swing voters are a hell of lot more disturbed by the fiasco in Iraq and the partisan hype that got us into it than they ever were about Clinton's blow jobs, the Dems also don't need to as careful about keeping the Bush bashers out of sight and out of mind until November. If I were Barone, I'd be pissed, too.   Billmon's a mensch.  Barone is constantly setting up straw men and knocking them down, while simultaneously ingnoring the din of faux-conservative Republicans looting the cupboards behind him.  If the guy had a shred of sense, he'd be sprinting full speed towards  urlLink Kerry , who seems bound and determined to run a fundamentally conservative Clintonesque fiscal and budgetary policy as far as it can take him.  Instead, Barone is bitching about the political nature of politics and how unfair it is when the kids on the other side of the fence learn to play the game by the same rules, or lack thereof. \n",
      "\n",
      "\t\t\t\t\t\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check them\n",
    "print('post50: ', blogs_data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data parsed, which consists of 176.796 posts from the 1499 blogs used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the posts text in the 'blogs_data' array, so we will apply to it the processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To start, we shuffle all the posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post50 shuffled:  \n",
      "\n",
      "\t \n",
      "      . virtues and vices .  the new blogger user interface asked me if, because i push snooze on my alarm clock, i was lazy. nope. i typed.  just tired.   and if i were to go into it -- i'd go so far as to say that its much more of a symbol of my ambition in tension with my finitude.    it is my ambition that drives me to set my alarm clock to 5 or 5:30 every morning -- mostly so i can have writing time.  it is my exhaustion that often motivates me to take a few minutes of rest before i attack the day.   so laziness isn't my vice -- nor is ambition my virtue.   i'm quite confident, though that *distraction* is my vice -- is well roundedness the accompanying virtue?   ambition + distraction is the equivalent of a driverless vehicle careening forward on a full tank of gas.   peace~\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "blogs_data_shuffled = shuffle(blogs_data)\n",
    "print('post50 shuffled: ', blogs_data_shuffled[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply different processing steps:\n",
    "    - remove the \\n, \\t, whitespaces starting and ending\n",
    "    - remove possible HTML tags and other\n",
    "    - put all in lowercase\n",
    "    - remove accents in some chars\n",
    "    - separate the posts by words with the help of the indices\n",
    "    - get the words array from the indices\n",
    "    - ignore words that have chars not considered integers, letters nor punctuation\n",
    "    - also ignore words which are only integers or punctuation\n",
    "    - limit the posts to 500 words per posts (sentence)\n",
    "    - we ignore the words larger than supercalifragilisticexpialidocious\n",
    "    \n",
    "So finally, a list of words are returned. These words include punctuation, which will have to take into account later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len('supercalifragilisticexpialidocious'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def splitWithIndices(s, c=' '):\n",
    "    p = 0\n",
    "    for k, g in groupby(s, lambda x:x==c):\n",
    "        q = p + sum(1 for i in g)\n",
    "        if not k:\n",
    "            yield p, q # or p, q-1 if you are really sure you want that\n",
    "        p = q\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def post_to_words(post):\n",
    "    LIMIT_WORDS = 500\n",
    "\n",
    "    post = post.lstrip().rstrip()\n",
    "    post = BeautifulSoup(post, \"html.parser\").get_text()\n",
    "    post = post.lower()\n",
    "    \n",
    "    words_accents = []\n",
    "    for word in post:\n",
    "        words_accents.append(strip_accents(word))\n",
    "        \n",
    "    words_ind = splitWithIndices(words_accents)\n",
    "        \n",
    "    words_aux = []\n",
    "    for idx in words_ind:\n",
    "        words_aux.append(post[idx[0]:idx[1]])\n",
    "    \n",
    "    words = []\n",
    "    for idx, word in enumerate(words_aux):\n",
    "        isvalid = True\n",
    "        isdigit = True\n",
    "        ispunct = True\n",
    "\n",
    "        for letter in word:\n",
    "            if not letter.isdigit():\n",
    "                if letter not in string.punctuation:\n",
    "                    if not letter.isalpha():\n",
    "                        isvalid = False\n",
    "            \n",
    "            if not letter.isdigit():\n",
    "                isdigit = False\n",
    "            if letter not in string.punctuation:\n",
    "                ispunct = False\n",
    "                \n",
    "        if isvalid == True and isdigit == False and ispunct == False:\n",
    "            if len(word) <= len('supercalifragilisticexpialidocious') and idx <= LIMIT_WORDS:\n",
    "                words.append(word)\n",
    "        \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\t \n",
      "       UK - Do not blame Indiia for JOb Loss  Atleast somebody is thinking sensibly. It is also very important at these times that India (NASSCOM  GOvt.) does some brand building and PR to spread some goodwill and erase the 'anti-outsourcing-anti-india' sentiments.::  Asking Britain's trade unions to show a sense of fairness towards India, British Deputy Prime Minister John Prescott has said that it was \"unfair\" to blame India for loss of British jobs because of \"outsourcing\" and India must be gi ven a chance to develop its skills and markets.... [Source :  urlLink The Hindu ] \n",
      "     \n",
      "    \n",
      "\n",
      "['uk', 'do', 'not', 'blame', 'indiia', 'for', 'job', 'loss', 'atleast', 'somebody', 'is', 'thinking', 'sensibly.', 'it', 'is', 'also', 'very', 'important', 'at', 'these', 'times', 'that', 'india', '(nasscom', 'govt.)', 'does', 'some', 'brand', 'building', 'and', 'pr', 'to', 'spread', 'some', 'goodwill', 'and', 'erase', 'the', \"'anti-outsourcing-anti-india'\", 'sentiments.::', 'asking', \"britain's\", 'trade', 'unions', 'to', 'show', 'a', 'sense', 'of', 'fairness', 'towards', 'india,', 'british', 'deputy', 'prime', 'minister', 'john', 'prescott', 'has', 'said', 'that', 'it', 'was', '\"unfair\"', 'to', 'blame', 'india', 'for', 'loss', 'of', 'british', 'jobs', 'because', 'of', '\"outsourcing\"', 'and', 'india', 'must', 'be', 'gi', 'ven', 'a', 'chance', 'to', 'develop', 'its', 'skills', 'and', 'markets....', '[source', 'urllink', 'the', 'hindu']\n",
      "\n",
      "123456 ., -> []\n"
     ]
    }
   ],
   "source": [
    "# Check it\n",
    "words = post_to_words(blogs_data_shuffled[60])\n",
    "print(blogs_data_shuffled[60])\n",
    "print(words)\n",
    "\n",
    "word1 = '123456 .,'\n",
    "w1 = post_to_words(word1)\n",
    "print('\\n{} -> {}'.format(word1, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having the data as sentences, we will have it as words, as the input of the model will be word by word.\n",
    "\n",
    "The method below applies the post_to_words method for all the sentences in the dataset indicated. In addition, it caches the results. This is because performing this processing step can take a long time. This way if you are unable to complete the notebook in the current session, you can come back without needing to process the data a second time.\n",
    "\n",
    "We ignore the sentences which for whenever reason have length 0, meaning all the words has been ignored (following the rules stated in the post2words) and ignore the words with length less than 4 letters, as they will not add anything to the model once jumbled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def preprocess_blogs(data, cache_dir, cache_file=\"preprocessed_blogs2.pkl\"):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    file_dir = os.path.join(cache_dir, cache_file)\n",
    "\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", file_dir)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_data = []\n",
    "        perc = 0\n",
    "        \n",
    "        for idx_p, post in enumerate(data):\n",
    "            \n",
    "            if idx_p / len(data) >= perc:\n",
    "                print('{} / {} sentences = {}%'.format(idx_p, len(data), np.round(perc*100, decimals = 1)))\n",
    "                perc = perc+0.1\n",
    "            \n",
    "            words = post_to_words(post)\n",
    "            words_stemmed = [PorterStemmer().stem(w) for w in words] # stem\n",
    "            \n",
    "            words_len = [word for word in words if len(word) > 3] #ignore words with less than 4 letters\n",
    "            if len(words_len) != 0:\n",
    "                words_data += words_len\n",
    "                        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_data=words_data)\n",
    "            with open(file_dir, \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", file_dir)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_data = (cache_data['words_data'])\n",
    "    \n",
    "    return words_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: cache/preprocessed_blogs2.pkl\n",
      "The dataset consists of 16634430 words\n"
     ]
    }
   ],
   "source": [
    "# Process all the blogs\n",
    "if not os.path.exists(cache_dir): # Make sure that the folder exists, if not create it\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "words_data = preprocess_blogs(blogs_data_shuffled, cache_dir)\n",
    "print('The dataset consists of {} words'.format(len(words_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more than 16 milion words in 176.796 sentences. Remember that only a subset of 1499 blogs out of 13.000 were used, imagine if all the blogs were used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length word: 34\n",
      "min length word: 4\n",
      "mean length word: 5.903403002086636\n"
     ]
    }
   ],
   "source": [
    "# check the length of number of letters per word is adecuated\n",
    "maxlen_w = 0\n",
    "minlen_w = 1000\n",
    "\n",
    "for word in words_data:\n",
    "    if len(word) > maxlen_w:\n",
    "        maxlen_w = len(word)\n",
    "    if len(word) < minlen_w:\n",
    "        minlen_w = len(word)\n",
    "\n",
    "mean_w = np.mean([len(w) for w in words_data])\n",
    "\n",
    "print('max length word: {}'.format(maxlen_w))\n",
    "print('min length word: {}'.format(minlen_w))\n",
    "print('mean length word: {}'.format(mean_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dictionary from the words processed and from it we will extract the most common 5.000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_from_data(data, vocab_size = 10000):    \n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "\n",
    "    for word in data:\n",
    "        if word in word_count:\n",
    "            word_count[word]+=1\n",
    "        else:\n",
    "            word_count[word]=1\n",
    "            \n",
    "    print('Length of the dictionary: {}'.format(len(word_count)))\n",
    "    \n",
    "    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_words = [tupl[0] for tupl in sorted_words]\n",
    "\n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx                           # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we will have separated the training and test datasets. The training will consist of the 5000 words more common in the blogs read. On the other hand, the test will consist of 1000 words randomly selected within the blogs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dictionary: 810981\n",
      "{'that': 0, 'have': 1, 'with': 2, 'this': 3, 'just': 4, 'like': 5, 'about': 6, 'urllink': 7, 'they': 8, 'from': 9, 'what': 10, \"it's\": 11, 'when': 12, 'will': 13, 'some': 14, 'really': 15, 'your': 16, \"don't\": 17, 'there': 18, 'know': 19, 'then': 20, 'think': 21, 'would': 22, 'been': 23, 'more': 24, 'going': 25, 'time': 26, 'were': 27, 'good': 28, 'because': 29, 'people': 30, 'back': 31, 'only': 32, 'last': 33, 'want': 34, 'much': 35, 'which': 36, 'their': 37, 'even': 38, 'went': 39, 'after': 40, 'into': 41, \"i've\": 42, 'other': 43, 'very': 44, 'could': 45, 'over': 46, 'them': 47, 'still': 48, 'than': 49, 'make': 50, 'little': 51, 'love': 52, 'things': 53, 'first': 54, 'something': 55, 'never': 56, 'being': 57, 'feel': 58, 'well': 59, 'should': 60, \"i'll\": 61, 'here': 62, 'where': 63, 'need': 64, 'right': 65, 'also': 66, \"didn't\": 67, 'work': 68, \"can't\": 69, 'take': 70, 'around': 71, \"that's\": 72, 'down': 73, 'those': 74, 'most': 75, 'thing': 76, 'today': 77, 'said': 78, 'made': 79, 'these': 80, 'before': 81, 'come': 82, 'while': 83, 'since': 84, 'night': 85, 'life': 86, 'always': 87, 'next': 88, 'well,': 89, 'pretty': 90, 'getting': 91, 'maybe': 92, 'great': 93, 'another': 94, 'home': 95, 'find': 96, 'look': 97, 'many': 98, 'long': 99, 'thought': 100, 'through': 101, 'actually': 102, 'ever': 103, 'every': 104, 'better': 105, 'sure': 106, 'someone': 107, 'tell': 108, 'came': 109, 'school': 110, 'read': 111, 'doing': 112, 'best': 113, 'until': 114, 'hope': 115, 'nothing': 116, 'same': 117, 'having': 118, \"you're\": 119, 'everyone': 120, 'give': 121, 'guess': 122, 'myself': 123, 'might': 124, \"he's\": 125, 'such': 126, 'does': 127, 'probably': 128, 'keep': 129, 'told': 130, 'nice': 131, 'found': 132, 'whole': 133, 'anything': 134, 'took': 135, 'left': 136, 'trying': 137, 'blog': 138, \"doesn't\": 139, 'kind': 140, 'friends': 141, 'days': 142, 'world': 143, 'week': 144, 'looking': 145, 'talk': 146, 'least': 147, 'though': 148, 'year': 149, 'call': 150, 'away': 151, 'started': 152, 'happy': 153, 'years': 154, 'now.': 155, 'enough': 156, 'quite': 157, 'called': 158, 'wanted': 159, 'house': 160, 'part': 161, 'everything': 162, 'makes': 163, 'stuff': 164, 'place': 165, 'start': 166, 'without': 167, 'gonna': 168, \"there's\": 169, 'show': 170, 'post': 171, 'time.': 172, 'once': 173, 'again': 174, 'now,': 175, 'must': 176, 'done': 177, 'hard': 178, 'anyone': 179, 'both': 180, 'used': 181, 'each': 182, 'three': 183, 'hate': 184, \"wasn't\": 185, 'almost': 186, 'help': 187, 'seems': 188, 'believe': 189, 'finally': 190, 'that.': 191, 'person': 192, 'remember': 193, 'talking': 194, 'dont': 195, 'friend': 196, 'able': 197, 'thinking': 198, 'today.': 199, \"haven't\": 200, 'mean': 201, 'decided': 202, 'morning': 203, 'play': 204, \"isn't\": 205, 'feeling': 206, 'watch': 207, \"we're\": 208, 'making': 209, 'else': 210, 'already': 211, 'coming': 212, 'times': 213, 'write': 214, 'day.': 215, 'real': 216, \"won't\": 217, 'working': 218, 'live': 219, 'name': 220, 'them.': 221, 'movie': 222, 'hours': 223, 'anyway,': 224, 'reading': 225, 'wish': 226, 'head': 227, 'cool': 228, 'couple': 229, \"she's\": 230, 'during': 231, 'seen': 232, 'out.': 233, 'again.': 234, 'past': 235, 'felt': 236, 'different': 237, 'high': 238, 'stop': 239, 'book': 240, 'point': 241, 'you.': 242, 'guys': 243, 'between': 244, \"they're\": 245, 'well.': 246, 'leave': 247, 'hear': 248, 'fact': 249, 'half': 250, 'there.': 251, 'that,': 252, 'asked': 253, 'girl': 254, 'music': 255, 'mind': 256, 'yesterday': 257, 'miss': 258, 'tomorrow': 259, 'class': 260, 'watching': 261, 'comes': 262, 'free': 263, 'care': 264, 'check': 265, 'taking': 266, 'too.': 267, 'money': 268, 'family': 269, 'game': 270, 'room': 271, 'looks': 272, 'news': 273, 'rather': 274, 'heard': 275, 'full': 276, \"couldn't\": 277, 'wait': 278, 'time,': 279, 'song': 280, 'weekend': 281, 'lost': 282, 'change': 283, 'rest': 284, 'him.': 285, 'second': 286, 'says': 287, 'idea': 288, 'you,': 289, 'night.': 290, 'today,': 291, 'later': 292, 'story': 293, 'cause': 294, 'good.': 295, 'interesting': 296, 'gave': 297, 'sleep': 298, 'goes': 299, 'party': 300, 'seem': 301, 'spent': 302, 'knew': 303, 'sometimes': 304, 'gets': 305, 'less': 306, 'under': 307, 'playing': 308, 'reason': 309, 'kinda': 310, 'looked': 311, 'funny': 312, 'means': 313, 'saying': 314, 'minutes': 315, 'against': 316, 'damn': 317, 'stay': 318, 'course': 319, 'stupid': 320, 'phone': 321, 'small': 322, 'tried': 323, 'life.': 324, 'fucking': 325, \"wouldn't\": 326, 'know,': 327, 'kids': 328, 'watched': 329, 'writing': 330, 'hour': 331, 'along': 332, 'turn': 333, 'yeah,': 334, 'played': 335, 'all.': 336, 'thats': 337, 'please': 338, \"what's\": 339, 'hell': 340, 'computer': 341, 'yes,': 342, 'white': 343, 'friday': 344, 'early': 345, 'heart': 346, 'wonder': 347, 'favorite': 348, 'become': 349, 'open': 350, 'side': 351, 'sorry': 352, 'front': 353, 'tonight': 354, 'brought': 355, 'understand': 356, 'here.': 357, 'bush': 358, 'words': 359, 'face': 360, 'black': 361, 'food': 362, 'soon': 363, 'either': 364, 'sort': 365, 'happened': 366, 'wrong': 367, 'late': 368, 'sitting': 369, 'close': 370, 'together': 371, 'supposed': 372, 'living': 373, 'fun.': 374, 'instead': 375, 'thank': 376, 'day,': 377, 'wants': 378, 'gone': 379, 'work.': 380, 'move': 381, 'site': 382, 'totally': 383, 'turned': 384, 'bought': 385, 'again,': 386, 'parents': 387, 'weeks': 388, 'lots': 389, 'running': 390, 'though.': 391, 'eyes': 392, '\"the': 393, 'especially': 394, 'walk': 395, 'there,': 396, 'word': 397, 'thanks': 398, '(and': 399, 'talked': 400, 'american': 401, 'tired': 402, 'however,': 403, 'seeing': 404, 'girls': 405, 'waiting': 406, 'saturday': 407, 'group': 408, 'bring': 409, 'ready': 410, 'night,': 411, 'spend': 412, 'behind': 413, 'summer': 414, 'john': 415, 'although': 416, 'shit': 417, 'one.': 418, 'usually': 419, 'four': 420, 'hair': 421, 'wanna': 422, 'matter': 423, \"we'll\": 424, 'week.': 425, 'number': 426, 'except': 427, 'worth': 428, 'knows': 429, 'band': 430, 'dinner': 431, 'her.': 432, 'yeah': 433, 'drive': 434, 'completely': 435, 'taken': 436, 'listening': 437, \"here's\": 438, 'meet': 439, 'sound': 440, 'sunday': 441, 'break': 442, 'out,': 443, 'kept': 444, 'state': 445, 'problem': 446, 'know.': 447, 'birthday': 448, 'using': 449, 'outside': 450, 'pick': 451, 'months': 452, 'five': 453, 'all,': 454, 'then,': 455, 'several': 456, 'short': 457, 'home.': 458, 'email': 459, 'fall': 460, 'others': 461, 'picture': 462, 'though,': 463, 'water': 464, 'starting': 465, 'whatever': 466, 'city': 467, 'fuck': 468, 'people.': 469, 'this.': 470, 'middle': 471, 'ended': 472, 'pictures': 473, 'hand': 474, 'walked': 475, 'woke': 476, 'said,': 477, 'hold': 478, 'trip': 479, 'thing.': 480, 'enjoy': 481, 'true': 482, 'test': 483, 'across': 484, 'glad': 485, 'given': 486, 'say,': 487, 'list': 488, 'gotta': 489, 'sounds': 490, 'needs': 491, 'lunch': 492, 'light': 493, \"let's\": 494, 'way.': 495, 'course,': 496, 'case': 497, 'huge': 498, 'this,': 499, 'whether': 500, 'crazy': 501, 'entire': 502, 'inside': 503, 'needed': 504, 'line': 505, 'figure': 506, \"you've\": 507, 'woman': 508, 'them,': 509, 'way,': 510, 'realize': 511, 'baby': 512, 'sick': 513, 'weird': 514, 'finished': 515, 'sense': 516, 'exactly': 517, 'cant': 518, 'body': 519, 'cannot': 520, 'deal': 521, 'beautiful': 522, 'him,': 523, 'important': 524, 'year.': 525, 'telling': 526, 'type': 527, 'send': 528, \"you'll\": 529, 'forward': 530, 'power': 531, 'perhaps': 532, 'till': 533, 'office': 534, 'often': 535, 'mean,': 536, 'women': 537, 'learn': 538, 'hopefully': 539, 'realized': 540, 'plan': 541, 'wrote': 542, 'current': 543, 'christmas': 544, 'ones': 545, 'worked': 546, \"aren't\": 547, 'back.': 548, 'walking': 549, 'church': 550, 'definitely': 551, 'young': 552, 'takes': 553, 'cold': 554, 'paper': 555, 'stand': 556, 'chance': 557, 'listen': 558, 'busy': 559, 'like,': 560, 'moment': 561, 'stuff.': 562, 'question': 563, 'link': 564, 'online': 565, 'here,': 566, 'order': 567, 'easy': 568, 'dream': 569, 'blue': 570, 'happen': 571, 'sent': 572, 'english': 573, 'longer': 574, 'work,': 575, 'forget': 576, 'but,': 577, 'moving': 578, 'off.': 579, 'personal': 580, 'didnt': 581, 'upon': 582, 'month': 583, 'college': 584, 'team': 585, 'special': 586, 'seemed': 587, 'driving': 588, 'mother': 589, 'simply': 590, 'leaving': 591, 'hang': 592, 'alone': 593, 'days.': 594, 'kill': 595, 'eating': 596, 'rock': 597, 'article': 598, 'country': 599, 'giving': 600, 'information': 601, 'door': 602, 'liked': 603, 'school.': 604, 'certain': 605, 'much.': 606, 'loved': 607, \"we've\": 608, 'president': 609, 'known': 610, 'missed': 611, '(the': 612, 'support': 613, 'near': 614, 'you?': 615, 'apparently': 616, 'single': 617, 'written': 618, 'within': 619, 'songs': 620, 'internet': 621, 'amazing': 622, 'cool.': 623, 'wonderful': 624, 'visit': 625, 'life,': 626, 'brother': 627, 'catch': 628, 'people,': 629, 'random': 630, 'feels': 631, 'morning.': 632, 'blog.': 633, 'history': 634, 'finish': 635, 'excited': 636, 'public': 637, 'not.': 638, 'something.': 639, 'share': 640, 'books': 641, 'drink': 642, 'okay,': 643, 'things.': 644, 'mine': 645, 'town': 646, 'page': 647, 'tomorrow.': 648, 'wearing': 649, 'weekend.': 650, 'picked': 651, 'film': 652, 'home,': 653, 'good,': 654, 'perfect': 655, 'bunch': 656, 'shall': 657, 'human': 658, 'hurt': 659, 'monday': 660, 'awesome': 661, 'yourself': 662, 'gotten': 663, 'piece': 664, 'myself.': 665, 'sister': 666, 'wear': 667, 'lives': 668, 'and,': 669, 'house.': 670, 'meeting': 671, 'poor': 672, 'experience': 673, 'changed': 674, 'pain': 675, 'one,': 676, 'also,': 677, 'company': 678, 'video': 679, 'hands': 680, 'sweet': 681, 'moved': 682, 'movies': 683, 'students': 684, 'world.': 685, 'answer': 686, 'anyway': 687, 'mention': 688, 'dead': 689, 'man,': 690, 'street': 691, 'major': 692, 'death': 693, 'tonight.': 694, 'learned': 695, 'fine': 696, 'chris': 697, 'final': 698, 'store': 699, 'large': 700, 'away.': 701, 'thoughts': 702, '(which': 703, 'plus': 704, 'currently': 705, 'week,': 706, 'forgot': 707, 'anyway.': 708, 'fell': 709, 'better.': 710, 'business': 711, 'wake': 712, 'works': 713, 'green': 714, 'more.': 715, 'stopped': 716, 'super': 717, 'questions': 718, 'dark': 719, 'posted': 720, 'study': 721, 'caught': 722, 'absolutely': 723, 'right.': 724, 'comments': 725, 'asking': 726, 'speaking': 727, 'government': 728, 'happens': 729, 'voice': 730, 'meant': 731, 'noticed': 732, 'year,': 733, 'hoping': 734, 'deep': 735, 'strange': 736, 'stuck': 737, 'form': 738, 'sign': 739, 'morning,': 740, 'etc.': 741, 'complete': 742, 'save': 743, 'straight': 744, 'managed': 745, 'imagine': 746, 'space': 747, 'following': 748, 'national': 749, 'unless': 750, 'too,': 751, 'clean': 752, 'truly': 753, 'turns': 754, 'her,': 755, 'later.': 756, 'school,': 757, 'system': 758, 'friends.': 759, 'somewhere': 760, 'basically': 761, 'lose': 762, 'putting': 763, 'local': 764, 'social': 765, 'road': 766, 'pass': 767, 'bill': 768, 'children': 769, 'shows': 770, 'project': 771, 'mostly': 772, 'about.': 773, 'boys': 774, 'future': 775, 'beat': 776, 'simple': 777, 'total': 778, 'return': 779, 'ride': 780, 'nearly': 781, 'lack': 782, 'fight': 783, 'down.': 784, 'message': 785, 'shopping': 786, 'worst': 787, 'none': 788, 'crap': 789, 'comment': 790, 'level': 791, 'anymore.': 792, 'bad.': 793, 'thinks': 794, 'above': 795, 'dance': 796, 'onto': 797, 'fear': 798, 'political': 799, 'coffee': 800, 'possible': 801, 'strong': 802, 'soon.': 803, 'service': 804, 'seriously': 805, 'including': 806, 'really,': 807, 'third': 808, 'headed': 809, 'back,': 810, 'calling': 811, 'throw': 812, 'games': 813, 'hey,': 814, 'according': 815, 'dear': 816, 'bored': 817, 'expect': 818, 'towards': 819, 'extra': 820, 'over.': 821, 'problems': 822, 'thursday': 823, 'york': 824, 'cute': 825, 'park': 826, 'great.': 827, 'passed': 828, 'okay': 829, 'years.': 830, 'somehow': 831, 'rain': 832, 'conversation': 833, 'gives': 834, 'evil': 835, 'teacher': 836, 'million': 837, 'place.': 838, 'friends,': 839, 'yesterday.': 840, 'afraid': 841, 'continue': 842, 'likely': 843, 'standing': 844, 'speak': 845, 'knowing': 846, 'fast': 847, 'boring': 848, 'iraq': 849, 'report': 850, 'control': 851, 'evening': 852, 'main': 853, 'worry': 854, 'weather': 855, 'website': 856, 'stayed': 857, 'peace': 858, 'serious': 859, 'finding': 860, 'despite': 861, 'certainly': 862, \"it'll\": 863, 'anyways,': 864, 'tyke': 865, 'normal': 866, 'michael': 867, 'date': 868, \"shouldn't\": 869, 'hanging': 870, 'beginning': 871, 'based': 872, 'times.': 873, 'quick': 874, 'drove': 875, 'clear': 876, 'afternoon': 877, 'thing,': 878, 'interested': 879, 'attention': 880, 'child': 881, 'recently': 882, 'starts': 883, 'amount': 884, 'himself': 885, 'lead': 886, 'figured': 887, 'suppose': 888, 'update': 889, \"who's\": 890, 'shot': 891, 'general': 892, 'note': 893, 'minute': 894, 'blogger': 895, 'step': 896, 'relationship': 897, 'photo': 898, 'love,': 899, 'yet.': 900, 'off,': 901, 'kerry': 902, 'period': 903, 'click': 904, 'showed': 905, 'done.': 906, 'earlier': 907, 'say.': 908, 'plans': 909, 'posting': 910, 'stories': 911, 'star': 912, 'feet': 913, 'funny.': 914, 'explain': 915, 'planning': 916, 'touch': 917, 'lovely': 918, 'lady': 919, 'paid': 920, 'fire': 921, 'decide': 922, 'with.': 923, 'alot': 924, 'search': 925, 'while.': 926, 'view': 927, 'right?': 928, 'chicken': 929, 'yet,': 930, 'nice.': 931, 'laugh': 932, 'mood': 933, 'card': 934, 'issue': 935, 'radio': 936, 'united': 937, 'fact,': 938, 'ago,': 939, 'named': 940, 'camera': 941, 'blood': 942, 'george': 943, 'kick': 944, 'yay!': 945, 'god,': 946, 'media': 947, 'begin': 948, 'missing': 949, 'tells': 950, 'hung': 951, 'david': 952, 'mentioned': 953, 'ago.': 954, 'likes': 955, 'yesterday,': 956, 'fun,': 957, 'broke': 958, 'anything.': 959, 'french': 960, 'notice': 961, 'lets': 962, 'quote': 963, 'slept': 964, 'tuesday': 965, 'recent': 966, 'right,': 967, 'character': 968, 'eric': 969, 'wedding': 970, 'things,': 971, \"weren't\": 972, 'agree': 973, 'places': 974, 'calls': 975, 'began': 976, 'jesus': 977, 'worse': 978, 'became': 979, 'lord': 980, 'father': 981, 'america': 982, 'drinking': 983, 'album': 984, 'student': 985, 'smile': 986, 'round': 987, 'ways': 988, 'possibly': 989, 'states': 990, 'dreams': 991, 'suddenly': 992, 'themselves': 993, 'clothes': 994, 'sleeping': 995, 'nobody': 996, 'days,': 997, 'movie.': 998, 'building': 999, 'sleep.': 1000, \"hasn't\": 1001, 'drop': 1002, 'floor': 1003, 'learning': 1004, 'happy.': 1005, 'enjoyed': 1006, 'classes': 1007, 'area': 1008, '\"you': 1009, 'doubt': 1010, 'pull': 1011, 'shut': 1012, 'wednesday': 1013, 'later,': 1014, 'extremely': 1015, 'rick': 1016, 'brain': 1017, 'spending': 1018, 'among': 1019, 'held': 1020, 'pissed': 1021, 'proud': 1022, 'energy': 1023, 'consider': 1024, 'filled': 1025, 'forever': 1026, 'usual': 1027, 'did.': 1028, 'yeah.': 1029, 'practice': 1030, 'considering': 1031, 'added': 1032, 'cover': 1033, 'keeping': 1034, 'weight': 1035, 'original': 1036, 'interview': 1037, 'stick': 1038, \"you'd\": 1039, 'blog,': 1040, 'mark': 1041, 'beyond': 1042, 'vote': 1043, 'trust': 1044, 'self': 1045, 'see,': 1046, 'wondering': 1047, 'various': 1048, 'loves': 1049, 'actually,': 1050, 'truth': 1051, 'mike': 1052, 'scared': 1053, 'together.': 1054, 'everybody': 1055, 'train': 1056, 'version': 1057, 'cell': 1058, 'keeps': 1059, 'either.': 1060, 'was,': 1061, 'lol.': 1062, 'house,': 1063, 'title': 1064, 'before.': 1065, 'south': 1066, 'research': 1067, 'slow': 1068, 'paul': 1069, 'lived': 1070, \"today's\": 1071, 'down,': 1072, 'around.': 1073, 'situation': 1074, 'daily': 1075, 'not,': 1076, 'allowed': 1077, 'think.': 1078, 'mind.': 1079, 'choose': 1080, 'said.': 1081, 'that?': 1082, 'library': 1083, 'choice': 1084, 'wont': 1085, 'pulled': 1086, 'however': 1087, 'further': 1088, 'club': 1089, 'actual': 1090, 'died': 1091, 'first,': 1092, 'admit': 1093, 'helped': 1094, 'season': 1095, 'follow': 1096, 'class.': 1097, 'lucky': 1098, 'myself,': 1099, 'job.': 1100, 'july': 1101, 'grow': 1102, 'trouble': 1103, 'letter': 1104, 'broken': 1105, 'snow': 1106, 'falling': 1107, 'quickly': 1108, 'tonight,': 1109, 'older': 1110, 'drunk': 1111, 'issues': 1112, 'center': 1113, 'science': 1114, 'create': 1115, 'eventually': 1116, 'whenever': 1117, 'program': 1118, 'u.s.': 1119, 'matt': 1120, 'points': 1121, 'apartment': 1122, 'record': 1123, 'warm': 1124, 'color': 1125, 'feelings': 1126, 'join': 1127, 'posts': 1128, 'difficult': 1129, 'somewhat': 1130, 'wife': 1131, 'blogging': 1132, 'ball': 1133, 'links': 1134, 'holding': 1135, 'wind': 1136, 'obviously': 1137, 'beer': 1138, 'security': 1139, 'room.': 1140, 'window': 1141, 'spring': 1142, 'slightly': 1143, 'asleep': 1144, 'present': 1145, 'created': 1146, 'bed.': 1147, 'welcome': 1148, 'series': 1149, 'particular': 1150, 'regular': 1151, 'table': 1152, 'else.': 1153, 'was.': 1154, 'married': 1155, 'ideas': 1156, 'itself': 1157, 'game.': 1158, \"hadn't\": 1159, 'health': 1160, 'man.': 1161, 'horrible': 1162, 'quizilla': 1163, 'allow': 1164, 'stuff,': 1165, 'fair': 1166, 'grade': 1167, '(not': 1168, 'staying': 1169, 'singing': 1170, 'years,': 1171, 'soul': 1172, 'killed': 1173, 'involved': 1174, 'windows': 1175, 'police': 1176, 'north': 1177, 'whom': 1178, 'much,': 1179, 'turning': 1180, 'tomorrow,': 1181, 'pm):': 1182, 'previous': 1183, 'members': 1184, 'king': 1185, 'common': 1186, 'world,': 1187, 'sing': 1188, 'beach': 1189, 'growing': 1190, 'dave': 1191, 'wanting': 1192, 'concert': 1193, 'homework': 1194, 'miles': 1195, 'hours.': 1196, 'quiet': 1197, 'see.': 1198, 'before,': 1199, 'reality': 1200, 'cheese': 1201, 'haha.': 1202, 'june': 1203, 'hello': 1204, 'weekend,': 1205, 'e-mail': 1206, 'surprised': 1207, 'action': 1208, 'it...': 1209, 'process': 1210, 'really.': 1211, 'stage': 1212, 'james': 1213, 'born': 1214, 'what?': 1215, 'exciting': 1216, 'buying': 1217, 'are.': 1218, 'greatest': 1219, 'mail': 1220, 'blogs': 1221, 'wrong.': 1222, 'copy': 1223, 'former': 1224, 'entry': 1225, 'whose': 1226, 'interest': 1227, 'received': 1228, 'latest': 1229, 'cost': 1230, 'somebody': 1231, 'lot.': 1232, 'awesome.': 1233, 'head.': 1234, 'chocolate': 1235, 'meaning': 1236, 'sucks': 1237, 'university': 1238, 'shirt': 1239, 'luck': 1240, 'love.': 1241, 'easily': 1242, 'dropped': 1243, 'fish': 1244, 'dress': 1245, \"they've\": 1246, 'taste': 1247, 'angry': 1248, 'show.': 1249, 'waste': 1250, 'fill': 1251, 'photos': 1252, 'shower': 1253, 'track': 1254, 'showing': 1255, 'wow,': 1256, 'reasons': 1257, 'kiss': 1258, 'everything.': 1259, 'secret': 1260, 'smell': 1261, 'parts': 1262, 'person.': 1263, 'easier': 1264, 'community': 1265, 'sooo': 1266, 'day:': 1267, 'lazy': 1268, 'pray': 1269, 'something,': 1270, 'bad,': 1271, 'reach': 1272, 'shop': 1273, 'paying': 1274, 'arms': 1275, 'christian': 1276, 'names': 1277, 'offer': 1278, 'youth': 1279, 'ever.': 1280, 'sarah': 1281, 'lesson': 1282, 'west': 1283, 'math': 1284, 'interesting.': 1285, 'similar': 1286, 'me...': 1287, 'brown': 1288, 'arrived': 1289, 'hotel': 1290, 'holy': 1291, 'size': 1292, 'language': 1293, 'attempt': 1294, 'laughing': 1295, 'ahead': 1296, 'worried': 1297, 'nights': 1298, 'quality': 1299, 'empty': 1300, 'review': 1301, 'followed': 1302, 'blah': 1303, 'times,': 1304, 'subject': 1305, 'news,': 1306, \"he'll\": 1307, 'ground': 1308, 'apart': 1309, 'willing': 1310, 'focus': 1311, 'position': 1312, 'upset': 1313, 'effort': 1314, 'education': 1315, 'tears': 1316, 'style': 1317, 'happening': 1318, 'pink': 1319, 'suck': 1320, 'press': 1321, 'board': 1322, 'becoming': 1323, 'mouth': 1324, 'losing': 1325, 'cream': 1326, 'blame': 1327, 'respect': 1328, 'brian': 1329, 'shit.': 1330, 'fairly': 1331, 'tend': 1332, 'lines': 1333, 'gift': 1334, 'parking': 1335, 'scary': 1336, 'changes': 1337, 'holiday': 1338, 'available': 1339, 'biggest': 1340, 'earth': 1341, 'march': 1342, 'car.': 1343, 'silly': 1344, 'results': 1345, 'leaves': 1346, 'appreciate': 1347, '\"what': 1348, 'bottom': 1349, 'while,': 1350, 'military': 1351, 'hearing': 1352, 'barely': 1353, 'freaking': 1354, 'pool': 1355, 'address': 1356, 'chinese': 1357, 'away,': 1358, 'contact': 1359, 'ability': 1360, 'folks': 1361, 'then.': 1362, 'nothing.': 1363, 'football': 1364, 'boss': 1365, 'decent': 1366, 'slowly': 1367, 'long.': 1368, 'decision': 1369, 'husband': 1370, 'opened': 1371, 'land': 1372, 'bright': 1373, 'helping': 1374, 'force': 1375, 'discovered': 1376, 'tired.': 1377, 'scene': 1378, 'dirty': 1379, 'difference': 1380, 'why?': 1381, 'post.': 1382, 'court': 1383, 'field': 1384, 'opportunity': 1385, 'shoes': 1386, 'marriage': 1387, 'besides': 1388, 'bit.': 1389, 'closer': 1390, 'wall': 1391, 'brings': 1392, 'training': 1393, 'credit': 1394, 'hardly': 1395, 'god.': 1396, 'event': 1397, 'bitch': 1398, 'tour': 1399, 'anymore': 1400, 'teaching': 1401, 'over,': 1402, 'money.': 1403, 'annoying': 1404, 'twice': 1405, 'crying': 1406, 'doesnt': 1407, \"he'd\": 1408, 'teach': 1409, 'cool,': 1410, 'dunno': 1411, 'expected': 1412, 'camp': 1413, 'yes.': 1414, 'for.': 1415, 'weeks.': 1416, 'excellent': 1417, 'opening': 1418, 'east': 1419, 'count': 1420, \"'cause\": 1421, 'safe': 1422, 'family.': 1423, 'checked': 1424, 'cleaning': 1425, 'particularly': 1426, 'enough,': 1427, 'fighting': 1428, 'bloody': 1429, 'end.': 1430, 'faith': 1431, 'smart': 1432, 'knowledge': 1433, 'book.': 1434, 'popular': 1435, \"they'll\": 1436, 'okay.': 1437, 'story.': 1438, \"ain't\": 1439, 'access': 1440, 'neither': 1441, 'guess.': 1442, 'enough.': 1443, 'pages': 1444, 'international': 1445, 'double': 1446, 'sometime': 1447, 'section': 1448, 'pizza': 1449, 'details': 1450, 'lately': 1451, 'music.': 1452, 'doctor': 1453, 'loud': 1454, 'grand': 1455, 'movie,': 1456, 'term': 1457, 'harry': 1458, 'still,': 1459, 'carry': 1460, 'place,': 1461, 'include': 1462, 'friend,': 1463, 'you!': 1464, 'sell': 1465, 'fresh': 1466, 'letting': 1467, 'point.': 1468, 'positive': 1469, 'true.': 1470, 'party.': 1471, 'april': 1472, '\"i\\'m': 1473, 'pants': 1474, 'swear': 1475, 'handle': 1476, 'fingers': 1477, 'digital': 1478, 'events': 1479, 'other.': 1480, 'more,': 1481, 'jack': 1482, 'enjoying': 1483, 'account': 1484, 'rich': 1485, 'greg': 1486, 'are,': 1487, 'reminds': 1488, 'image': 1489, 'jump': 1490, 'sorry,': 1491, 'pair': 1492, 'memory': 1493, 'teachers': 1494, 'station': 1495, 'funny,': 1496, 'changing': 1497, 'cross': 1498, 'hand,': 1499, 'seven': 1500, 'accept': 1501, 'comments:': 1502, 'glass': 1503, 'corner': 1504, 'breakfast': 1505, 'related': 1506, 'below': 1507, 'thus': 1508, 'officially': 1509, 'kicked': 1510, 'price': 1511, 'episode': 1512, 'highly': 1513, 'considered': 1514, 'result': 1515, 'dancing': 1516, 'roll': 1517, 'incredibly': 1518, 'mall': 1519, 'jobs': 1520, 'room,': 1521, 'stood': 1522, 'higher': 1523, 'job,': 1524, 'like.': 1525, 'americans': 1526, 'necessary': 1527, 'day!': 1528, 'member': 1529, 'india': 1530, 'haha': 1531, 'invited': 1532, 'moments': 1533, 'average': 1534, 'screen': 1535, 'long,': 1536, 'loving': 1537, 'player': 1538, 'plenty': 1539, 'happen.': 1540, 'response': 1541, 'sure,': 1542, 'generally': 1543, \"we'd\": 1544, 'planned': 1545, 'forced': 1546, 'google': 1547, 'spot': 1548, 'guitar': 1549, 'dean': 1550, 'vacation': 1551, 'surprise': 1552, 'otherwise': 1553, 'person,': 1554, 'design': 1555, 'anybody': 1556, 'democratic': 1557, 'anything,': 1558, 'tiny': 1559, 'campaign': 1560, 'avoid': 1561, 'foot': 1562, 'society': 1563, 'throughout': 1564, 'adam': 1565, 'private': 1566, 'class,': 1567, 'havent': 1568, 'have.': 1569, 'skin': 1570, 'flying': 1571, 'software': 1572, 'provide': 1573, 'pieces': 1574, 'tree': 1575, 'around,': 1576, 'build': 1577, 'speech': 1578, 'about,': 1579, 'nature': 1580, 'others.': 1581, 'female': 1582, 'role': 1583, 'minutes.': 1584, 'mrs.': 1585, 'fun!': 1586, 'killing': 1587, 'reminded': 1588, 'opinion': 1589, \"one's\": 1590, 'immediately': 1591, 'jeff': 1592, 'freedom': 1593, 'kitchen': 1594, 'fully': 1595, 'studying': 1596, 'wasnt': 1597, 'toward': 1598, 'moment.': 1599, 'lately.': 1600, 'nice,': 1601, 'cheap': 1602, 'bother': 1603, 'technology': 1604, 'alone.': 1605, 'physical': 1606, 'happened.': 1607, 'discussion': 1608, 'bigger': 1609, 'desire': 1610, 'friend.': 1611, 'friday,': 1612, 'book,': 1613, 'now!': 1614, 'ends': 1615, 'bottle': 1616, 'tickets': 1617, 'excuse': 1618, 'lights': 1619, 'machine': 1620, 'acting': 1621, 'stress': 1622, 'steve': 1623, 'site.': 1624, 'idea.': 1625, 'great,': 1626, 'release': 1627, 'think,': 1628, 'cars': 1629, 'threw': 1630, '*sigh*': 1631, 'attack': 1632, 'summer.': 1633, 'travel': 1634, 'foreign': 1635, 'ordered': 1636, 'sports': 1637, 'source': 1638, 'heavy': 1639, 'daughter': 1640, 'sucks.': 1641, 'guy.': 1642, 'left.': 1643, 'winter': 1644, 'unfortunately,': 1645, 'exam': 1646, 'music,': 1647, 'better,': 1648, 'boyfriend': 1649, 'says,': 1650, 'everyday': 1651, 'month.': 1652, 'covered': 1653, 'sad.': 1654, 'signed': 1655, 'memories': 1656, 'failed': 1657, 'example': 1658, 'race': 1659, 'hide': 1660, 'girlfriend': 1661, 'describe': 1662, 'chicago': 1663, 'characters': 1664, 'text': 1665, 'network': 1666, 'hospital': 1667, 'whats': 1668, 'ourselves': 1669, 'famous': 1670, 'rights': 1671, 'bathroom': 1672, 'soooo': 1673, 'listened': 1674, 'point,': 1675, 'promise': 1676, 'plays': 1677, 'code': 1678, 'british': 1679, 'bear': 1680, 'quiz': 1681, 'finally,': 1682, 'fine.': 1683, 'friday.': 1684, 'tough': 1685, 'mood:': 1686, 'face.': 1687, 'beauty': 1688, '(for': 1689, 'bless': 1690, 'official': 1691, 'stars': 1692, 'car,': 1693, 'senior': 1694, 'honestly': 1695, 'ring': 1696, 'lying': 1697, 'reached': 1698, 'mass': 1699, 'wrong,': 1700, 'legs': 1701, 'dying': 1702, 'purpose': 1703, 'laughed': 1704, 'entirely': 1705, 'natural': 1706, 'fellow': 1707, 'sure.': 1708, 'now...': 1709, 'push': 1710, 'heading': 1711, 'ladies': 1712, 'confused': 1713, 'done,': 1714, 'this:': 1715, 'becomes': 1716, 'bands': 1717, 'hall': 1718, 'with,': 1719, 'emotional': 1720, 'male': 1721, 'comfortable': 1722, 'mind,': 1723, 'seriously,': 1724, 'closed': 1725, 'administration': 1726, 'nick': 1727, 'offered': 1728, 'market': 1729, 'rush': 1730, 'creative': 1731, 'dumb': 1732, 'terrible': 1733, 'constantly': 1734, 'texas': 1735, 'smoke': 1736, 'built': 1737, 'normally': 1738, 'mess': 1739, 'clearly': 1740, 'obvious': 1741, 'checking': 1742, 'remembered': 1743, 'anywhere': 1744, 'quit': 1745, 'washington': 1746, 'crowd': 1747, 'notes': 1748, 'indian': 1749, 'heat': 1750, 'terms': 1751, 'career': 1752, 'bringing': 1753, 'hurts': 1754, 'poetry': 1755, 'food.': 1756, 'own.': 1757, 'dressed': 1758, 'too!': 1759, 'blow': 1760, 'japanese': 1761, 'forgotten': 1762, 'saturday,': 1763, 'flight': 1764, 'dogs': 1765, 'cash': 1766, 'guy,': 1767, 'ryan': 1768, 'dollars': 1769, 'whatever.': 1770, 'note,': 1771, 'huh?': 1772, 'together,': 1773, 'end,': 1774, 'august': 1775, 'show,': 1776, 'goal': 1777, 'name.': 1778, 'pics': 1779, 'guys,': 1780, 'culture': 1781, 'ass.': 1782, 'staff': 1783, 'well...': 1784, 'content': 1785, 'topic': 1786, 'joined': 1787, 'lower': 1788, 'game,': 1789, 'fucked': 1790, 'department': 1791, 'yellow': 1792, 'performance': 1793, 'skills': 1794, 'sold': 1795, 'reads': 1796, 'dude': 1797, 'desk': 1798, 'moon': 1799, 'lies': 1800, 'sexual': 1801, 'schedule': 1802, 'rules': 1803, 'asks': 1804, 'plastic': 1805, 'months.': 1806, 'taught': 1807, 'drama': 1808, 'basic': 1809, 'pack': 1810, 'gone.': 1811, 'dinner.': 1812, 'semester': 1813, 'lyrics': 1814, 'downtown': 1815, 'traffic': 1816, 'why.': 1817, 'did,': 1818, 'apple': 1819, 'deserve': 1820, 'stomach': 1821, 'runs': 1822, 'bout': 1823, 'care.': 1824, 'burn': 1825, 'match': 1826, 'problem.': 1827, 'breaking': 1828, 'this?': 1829, 'crazy.': 1830, 'orange': 1831, 'reports': 1832, 'cousin': 1833, 'course.': 1834, 'bible': 1835, 'religious': 1836, 'lonely': 1837, 'restaurant': 1838, 'express': 1839, 'herself': 1840, 'seconds': 1841, 'weird.': 1842, 'money,': 1843, 'harder': 1844, 'potential': 1845, 'election': 1846, 'spirit': 1847, '(who': 1848, 'television': 1849, 'bank': 1850, 'block': 1851, 'brand': 1852, 'journal': 1853, 'family,': 1854, 'monday.': 1855, 'uncle': 1856, 'wide': 1857, 'load': 1858, 'hours,': 1859, 'value': 1860, 'majority': 1861, 'speed': 1862, 'magazine': 1863, 'wild': 1864, 'download': 1865, 'central': 1866, 'waited': 1867, 'reason,': 1868, 'words,': 1869, 'girl,': 1870, 'ugly': 1871, 'eight': 1872, \"people's\": 1873, 'typing': 1874, 'mine.': 1875, 'first.': 1876, 'numbers': 1877, 'aware': 1878, 'effect': 1879, 'plain': 1880, 'throwing': 1881, 'chat': 1882, 'caused': 1883, 'hole': 1884, 'bike': 1885, 'everyone.': 1886, 'laura': 1887, 'picking': 1888, \"mom's\": 1889, 'peter': 1890, 'remind': 1891, 'screaming': 1892, 'setting': 1893, 'nation': 1894, 'kids.': 1895, 'mental': 1896, 'remain': 1897, 'aside': 1898, 'compared': 1899, 'soon,': 1900, 'seat': 1901, 'post,': 1902, 'manage': 1903, 'anyways': 1904, 'shoot': 1905, 'name,': 1906, 'prove': 1907, 'battle': 1908, 'california': 1909, 'drug': 1910, 'plane': 1911, 'far,': 1912, 'girl.': 1913, 'passing': 1914, 'thought.': 1915, 'nervous': 1916, 'waking': 1917, 'republican': 1918, 'iraqi': 1919, 'why,': 1920, \"god's\": 1921, 'score': 1922, \"she'll\": 1923, 'released': 1924, 'army': 1925, 'wow.': 1926, 'giant': 1927, 'draw': 1928, 'spell': 1929, 'guys.': 1930, 'spoke': 1931, 'theme': 1932, 'robert': 1933, 'prefer': 1934, 'late.': 1935, 'spanish': 1936, 'kevin': 1937, 'afternoon.': 1938, 'tons': 1939, 'theory': 1940, 'sending': 1941, 'expecting': 1942, 'bowl': 1943, 'theres': 1944, 'recommend': 1945, 'drinks': 1946, 'grown': 1947, 'hell,': 1948, 'isnt': 1949, 'fans': 1950, 'saturday.': 1951, 'directly': 1952, 'mainly': 1953, 'serve': 1954, 'groups': 1955, 'reason.': 1956, 'cast': 1957, 'screw': 1958, 'says:': 1959, 'cable': 1960, 'ticket': 1961, 'continued': 1962, 'diet': 1963, 'rule': 1964, 'else,': 1965, 'returned': 1966, 'understanding': 1967, 'appears': 1968, 'paint': 1969, 'grew': 1970, 'bed,': 1971, 'musical': 1972, 'soft': 1973, 'treat': 1974, 'moment,': 1975, 'sunday,': 1976, 'cards': 1977, 'old.': 1978, 'personality': 1979, 'joke': 1980, 'thomas': 1981, 'schools': 1982, 'aunt': 1983, 'case,': 1984, 'already.': 1985, 'modern': 1986, 'crappy': 1987, 'everytime': 1988, 'change.': 1989, 'song.': 1990, 'hits': 1991, 'eyes.': 1992, 'kinds': 1993, 'far.': 1994, 'answers': 1995, \"dad's\": 1996, 'chose': 1997, 'development': 1998, 'file': 1999, 'advice': 2000, 'politics': 2001, 'cake': 2002, 'leading': 2003, 'nose': 2004, \"they'd\": 2005, 'poem': 2006, 'awful': 2007, 'river': 2008, 'september': 2009, 'breath': 2010, 'wore': 2011, 'example,': 2012, 'younger': 2013, 'burning': 2014, 'talks': 2015, 'sites': 2016, 'loss': 2017, 'sick.': 2018, 'regarding': 2019, 'info': 2020, '\"it\\'s': 2021, 'complain': 2022, 'pressure': 2023, 'sunday.': 2024, 'services': 2025, 'therefore': 2026, 'strength': 2027, 'policy': 2028, 'today!': 2029, 'channel': 2030, 'selling': 2031, 'sounded': 2032, 'literally': 2033, 'charge': 2034, 'manager': 2035, 'soccer': 2036, 'alive': 2037, 'cried': 2038, 'afford': 2039, 'favourite': 2040, 'alright': 2041, 'enter': 2042, 'amazing.': 2043, 'claim': 2044, 'plus,': 2045, 'lake': 2046, 'appear': 2047, 'legal': 2048, 'now?': 2049, 'food,': 2050, 'mary': 2051, 'ending': 2052, 'jason': 2053, 'baseball': 2054, 'head,': 2055, 'indeed': 2056, 'evening.': 2057, 'heart.': 2058, 'grocery': 2059, 'hell.': 2060, 'honest': 2061, 'collection': 2062, 'laid': 2063, 'sleep,': 2064, 'future.': 2065, 'birthday.': 2066, 'sometimes,': 2067, 'christ': 2068, 'celebrate': 2069, 'teeth': 2070, 'raise': 2071, 'hard.': 2072, 'ignore': 2073, 'will.': 2074, 'grab': 2075, 'fixed': 2076, 'companies': 2077, 'crap.': 2078, 'saved': 2079, 'fourth': 2080, 'personally': 2081, 'possible.': 2082, 'degree': 2083, 'alex': 2084, 'out!': 2085, \"year's\": 2086, 'proper': 2087, 'staring': 2088, 'debate': 2089, 'rent': 2090, 'comic': 2091, 'hitting': 2092, 'heads': 2093, 'awake': 2094, 'andrew': 2095, 'liberal': 2096, 'other,': 2097, 'passion': 2098, 'and/or': 2099, 'individual': 2100, 'smoking': 2101, 'read.': 2102, 'butt': 2103, 'computer.': 2104, 'happy,': 2105, 'pretend': 2106, 'spread': 2107, 'percent': 2108, 'puts': 2109, 'connection': 2110, 'want.': 2111, 'hehe': 2112, 'animal': 2113, 'massive': 2114, 'cats': 2115, 'swimming': 2116, 'updated': 2117, 'sudden': 2118, 'city.': 2119, 'cleaned': 2120, 'depressed': 2121, 'tests': 2122, 'pure': 2123, 'milk': 2124, 'expensive': 2125, 'packed': 2126, 'material': 2127, 'again!': 2128, 'responsible': 2129, 'included': 2130, 'couch': 2131, 'letters': 2132, 'wash': 2133, 'bit,': 2134, 'uses': 2135, 'perfectly': 2136, 'prepared': 2137, 'concept': 2138, 'opposite': 2139, 'correct': 2140, 'monday,': 2141, 'wait,': 2142, 'dating': 2143, 'concerned': 2144, 'convinced': 2145, 'weeks,': 2146, 'hair.': 2147, 'thrown': 2148, 'music:': 2149, 'which,': 2150, 'rice': 2151, 'wine': 2152, '\"this': 2153, 'nasty': 2154, 'song,': 2155, 'unlike': 2156, 'surely': 2157, 'specific': 2158, 'riding': 2159, 'kate': 2160, 'laptop': 2161, 'same.': 2162, 'chair': 2163, 'airport': 2164, 'truck': 2165, 'matrix': 2166, 'potter': 2167, 'door.': 2168, 'hated': 2169, 'candy': 2170, 'office.': 2171, 'microsoft': 2172, 'projects': 2173, 'hehe.': 2174, 'statement': 2175, 'trade': 2176, 'crush': 2177, 'tape': 2178, 'help.': 2179, 'sorry.': 2180, 'fake': 2181, 'andy': 2182, 'news.': 2183, 'nine': 2184, 'attend': 2185, 'impossible': 2186, 'relationships': 2187, 'religion': 2188, 'saddam': 2189, 'powerful': 2190, 'country.': 2191, 'happiness': 2192, 'november': 2193, 'scott': 2194, 'unfortunately': 2195, '(with': 2196, 'kelly': 2197, 'mission': 2198, 'discuss': 2199, 'sexy': 2200, 'apply': 2201, 'different.': 2202, 'london': 2203, '(like': 2204, 'signs': 2205, 'conference': 2206, 'mixed': 2207, 'can.': 2208, 'medical': 2209, 'programs': 2210, \"everyone's\": 2211, 'brief': 2212, 'stupid.': 2213, 'awhile': 2214, 'healthy': 2215, 'financial': 2216, 'industry': 2217, 'time!': 2218, 'stands': 2219, 'thousand': 2220, 'fantastic': 2221, 'assume': 2222, 'mountain': 2223, 'hopes': 2224, 'laundry': 2225, 'familiar': 2226, 'classic': 2227, 'katie': 2228, 'experience.': 2229, 'story,': 2230, 'true,': 2231, 'gold': 2232, 'apparently,': 2233, 'shit,': 2234, 'thought,': 2235, 'guide': 2236, 'lunch.': 2237, 'searching': 2238, 'promised': 2239, 'standard': 2240, 'golden': 2241, 'everywhere': 2242, 'overall': 2243, 'path': 2244, 'fashion': 2245, 'messed': 2246, 'boring.': 2247, '(but': 2248, 'heaven': 2249, 'helps': 2250, \"she'd\": 2251, \"(i'm\": 2252, 'cook': 2253, 'judge': 2254, 'federal': 2255, 'calm': 2256, 'water.': 2257, 'negative': 2258, 'thru': 2259, 'drank': 2260, 'addition': 2261, 'sucked': 2262, 'falls': 2263, 'author': 2264, 'hundred': 2265, 'receive': 2266, 'magic': 2267, 'grace': 2268, 'that!': 2269, 'afternoon,': 2270, 'winning': 2271, 'bucks': 2272, 'shake': 2273, 'distance': 2274, 'professional': 2275, 'doing.': 2276, 'october': 2277, 'nathan': 2278, 'island': 2279, 'faces': 2280, 'readers': 2281, 'base': 2282, 'december': 2283, 'come.': 2284, 'edge': 2285, 'papers': 2286, 'dealing': 2287, 'trip.': 2288, 'hill': 2289, 'screwed': 2290, 'crack': 2291, 'director': 2292, 'visiting': 2293, 'data': 2294, 'sees': 2295, '(that': 2296, 'rate': 2297, 'great!': 2298, 'greater': 2299, 'through.': 2300, 'evidence': 2301, 'tight': 2302, 'essay': 2303, 'streets': 2304, 'developed': 2305, 'that...': 2306, 'audience': 2307, 'sugar': 2308, 'exact': 2309, 'hour.': 2310, 'fail': 2311, 'naked': 2312, 'guilty': 2313, 'couldnt': 2314, 'parties': 2315, 'rolling': 2316, 'for,': 2317, 'images': 2318, 'friendship': 2319, 'lately,': 2320, 'old,': 2321, '(this': 2322, 'leader': 2323, 'typical': 2324, 'anna': 2325, 'suggest': 2326, 'left,': 2327, 'placed': 2328, 'garden': 2329, 'iraq.': 2330, 'square': 2331, 'significant': 2332, 'list.': 2333, 'minutes,': 2334, 'constant': 2335, 'wait.': 2336, 'suggested': 2337, \"bush's\": 2338, 'finger': 2339, 'adult': 2340, 'german': 2341, 'computers': 2342, 'words.': 2343, 'lisa': 2344, 'challenge': 2345, 'unable': 2346, 'sore': 2347, 'moral': 2348, 'rose': 2349, 'chapter': 2350, 'visited': 2351, 'fault': 2352, 'protect': 2353, 'ears': 2354, 'late,': 2355, 'master': 2356, 'bread': 2357, 'shared': 2358, 'birth': 2359, 'civil': 2360, 'metal': 2361, 'defense': 2362, 'johnny': 2363, 'heck': 2364, 'yung': 2365, 'drugs': 2366, 'party,': 2367, 'what,': 2368, 'heh.': 2369, 'side.': 2370, 'side,': 2371, 'boy,': 2372, 'least,': 2373, 'thousands': 2374, 'queen': 2375, 'writer': 2376, 'eyes,': 2377, 'leads': 2378, 'tired,': 2379, 'button': 2380, 'josh': 2381, 'exams': 2382, 'sharing': 2383, 'have,': 2384, 'goodbye': 2385, 'democrats': 2386, 'hungry': 2387, 'going.': 2388, 'site,': 2389, 'loose': 2390, 'risk': 2391, 'lot,': 2392, 'silence': 2393, 'feed': 2394, 'so...': 2395, 'bedroom': 2396, 'required': 2397, 'argument': 2398, 'recall': 2399, 'january': 2400, 'background': 2401, 'boston': 2402, 'lessons': 2403, 'moore': 2404, 'flat': 2405, 'junior': 2406, 'jumped': 2407, 'all!': 2408, 'walks': 2409, 'clinton': 2410, 'success': 2411, 'lives.': 2412, 'product': 2413, 'richard': 2414, 'forth': 2415, 'solid': 2416, 'brothers': 2417, 'blind': 2418, 'good!': 2419, 'cares': 2420, 'yourself.': 2421, 'raised': 2422, 'monkey': 2423, 'whoever': 2424, 'friendly': 2425, 'prayer': 2426, 'canada': 2427, 'test.': 2428, 'lang': 2429, 'shots': 2430, 'yours': 2431, 'regret': 2432, 'die.': 2433, 'ultimate': 2434, 'chick': 2435, 'hates': 2436, 'finds': 2437, 'unique': 2438, 'global': 2439, 'anger': 2440, 'conservative': 2441, 'sean': 2442, 'know?': 2443, 'model': 2444, 'war.': 2445, 'steps': 2446, 'adding': 2447, \"someone's\": 2448, 'application': 2449, 'inner': 2450, 'inspired': 2451, 'sometimes.': 2452, 'emily': 2453, 'shape': 2454, \"friend's\": 2455, 'grandma': 2456, 'toilet': 2457, 'part.': 2458, 'sigh.': 2459, 'jackson': 2460, 'published': 2461, 'outta': 2462, 'dare': 2463, 'two.': 2464, 'clue': 2465, 'break.': 2466, 'line.': 2467, 'italian': 2468, \"'the\": 2469, 'loads': 2470, 'shes': 2471, 'trees': 2472, 'worse.': 2473, 'sang': 2474, 'sense.': 2475, 'recognize': 2476, 'rings': 2477, 'bored.': 2478, 'matter.': 2479, 'forever.': 2480, 'successful': 2481, 'finishing': 2482, 'dinner,': 2483, 'invite': 2484, 'forgive': 2485, 'phone.': 2486, 'bothered': 2487, 'hollywood': 2488, 'pulling': 2489, 'fuckin': 2490, 'needless': 2491, 'beside': 2492, 'angel': 2493, 'rarely': 2494, 'driver': 2495, 'journey': 2496, 'tall': 2497, 'instead,': 2498, 'increase': 2499, 'clock': 2500, 'practically': 2501, 'england': 2502, 'lunch,': 2503, 'today...': 2504, 'summer,': 2505, 'silver': 2506, 'insurance': 2507, 'there!': 2508, 'city,': 2509, 'goin': 2510, 'brilliant': 2511, \"it'd\": 2512, 'best.': 2513, 'twenty': 2514, 'church.': 2515, 'thinking,': 2516, 'multiple': 2517, 'direct': 2518, 'agreed': 2519, 'cooking': 2520, 'shitty': 2521, 'feature': 2522, 'players': 2523, 'begins': 2524, 'sight': 2525, 'dollar': 2526, 'asian': 2527, 'months,': 2528, 'emotions': 2529, 'meal': 2530, 'steal': 2531, 'lol,': 2532, 'corporate': 2533, 'comfort': 2534, 'sad,': 2535, 'escape': 2536, 'smaller': 2537, 'flash': 2538, 'piss': 2539, 'word.': 2540, 'countries': 2541, 'served': 2542, 'least.': 2543, 'freak': 2544, 'stressed': 2545, 'excited.': 2546, 'suit': 2547, 'professor': 2548, 'porn': 2549, 'creating': 2550, 'hand.': 2551, 'sales': 2552, 'presidential': 2553, 'saying,': 2554, 'colors': 2555, 'novel': 2556, 'midnight': 2557, 'location:': 2558, 'extreme': 2559, 'weapons': 2560, 'shooting': 2561, 'town.': 2562, 'fine,': 2563, 'lame': 2564, 'exchange': 2565, 'remove': 2566, 'attitude': 2567, 'rant': 2568, 'print': 2569, 'provided': 2570, 'counting': 2571, 'prepare': 2572, 'kids,': 2573, 'soldiers': 2574, 'informed': 2575, 'shift': 2576, 'believed': 2577, 'views': 2578, 'shame': 2579, 'rise': 2580, 'boat': 2581, 'wasted': 2582, 'cancer': 2583, 'messages': 2584, 'pointed': 2585, 'bags': 2586, 'meat': 2587, 'accepted': 2588, 'candidate': 2589, 'management': 2590, 'wouldnt': 2591, 'walls': 2592, 'sorts': 2593, 'incredible': 2594, 'lol!': 2595, 'once.': 2596, 'studio': 2597, 'spiritual': 2598, 'shown': 2599, 'economic': 2600, 'anymore,': 2601, 'wave': 2602, 'features': 2603, 'everyone,': 2604, 'appointment': 2605, 'baby,': 2606, 'commercial': 2607, 'online.': 2608, 'jessica': 2609, 'exercise': 2610, 'war,': 2611, 'mouse': 2612, 'complaining': 2613, 'presents': 2614, 'honor': 2615, 'florida': 2616, 'convince': 2617, 'alarm': 2618, 'tech': 2619, 'studies': 2620, 'stone': 2621, 'p.m.': 2622, 'designed': 2623, 'limited': 2624, 'degrees': 2625, 'mistake': 2626, 'target': 2627, 'yelling': 2628, 'hidden': 2629, 'drag': 2630, 'sorta': 2631, 'rough': 2632, 'stole': 2633, 'jokes': 2634, 'nowhere': 2635, 'advantage': 2636, 'santa': 2637, 'faster': 2638, 'storm': 2639, 'piano': 2640, 'host': 2641, 'does.': 2642, 'includes': 2643, 'p.s.': 2644, 'basketball': 2645, 'pounds': 2646, 'split': 2647, 'scream': 2648, 'annoyed': 2649, 'erin': 2650, 'doors': 2651, 'books.': 2652, 'planet': 2653, 'charles': 2654, 'once,': 2655, 'pride': 2656, 'graduation': 2657, 'mom,': 2658, 'sale': 2659, 'southern': 2660, 'worship': 2661, 'types': 2662, 'part,': 2663, 'movies,': 2664, 'impressed': 2665, 'struck': 2666, 'realised': 2667, 'others,': 2668, 'hot.': 2669, 'pushing': 2670, 'sunny': 2671, 'tries': 2672, 'had.': 2673, 'throat': 2674, 'marry': 2675, 'cutting': 2676, 'emails': 2677, 'a.m.': 2678, 'entertainment': 2679, 'millions': 2680, 'jumping': 2681, 'fallen': 2682, 'fantasy': 2683, 'forces': 2684, 'evening,': 2685, 'union': 2686, '(you': 2687, 'allows': 2688, 'pero': 2689, 'question.': 2690, 'page.': 2691, 'presentation': 2692, 'grabbed': 2693, 'more!': 2694, 'terribly': 2695, 'month,': 2696, 'badly': 2697, 'active': 2698, 'romantic': 2699, 'howard': 2700, 'bits': 2701, '\"why': 2702, 'counter': 2703, 'upcoming': 2704, 'everything,': 2705, 'session': 2706, 'reference': 2707, 'chances': 2708, 'moves': 2709, 'alright,': 2710, 'writes': 2711, 'free.': 2712, 'computer,': 2713, 'hair,': 2714, 'drawing': 2715, 'jeans': 2716, 'pushed': 2717, 'drew': 2718, \"y'all\": 2719, 'necessarily': 2720, 'matters': 2721, 'continues': 2722, 'crew': 2723, 'rare': 2724, 'hmm.': 2725, 'alone,': 2726, 'neck': 2727, 'tear': 2728, 'treated': 2729, 'tony': 2730, 'btw,': 2731, 'claims': 2732, 'approach': 2733, 'flowers': 2734, 'thursday,': 2735, 'face,': 2736, 'alcohol': 2737, 'hard,': 2738, 'coolest': 2739, 'impact': 2740, 'discussing': 2741, 'sisters': 2742, 'catching': 2743, 'thanksgiving': 2744, 'reply': 2745, 'melissa': 2746, 'todd': 2747, 'saving': 2748, 'thin': 2749, 'glasses': 2750, 'appropriate': 2751, 'actions': 2752, 'responsibility': 2753, 'bell': 2754, 'eaten': 2755, 'death.': 2756, 'luckily': 2757, 'billion': 2758, 'refuse': 2759, 'presence': 2760, 'minor': 2761, 'phrase': 2762, 'butter': 2763, 'direction': 2764, 'animals': 2765, 'read,': 2766, '\"how': 2767, 'yard': 2768, 'dangerous': 2769, 'graduate': 2770, 'instead.': 2771, 'pleasure': 2772, 'films': 2773, 'hilarious': 2774, 'articles': 2775, 'hearts': 2776, 'system.': 2777, 'effects': 2778, 'locked': 2779, 'besides,': 2780, 'separate': 2781, 'explained': 2782, 'carrying': 2783, 'fancy': 2784, 'voted': 2785, 'items': 2786, 'trash': 2787, 'progress': 2788, 'william': 2789, 'switch': 2790, 'originally': 2791, 'weak': 2792, 'theater': 2793, 'soup': 2794, 'gain': 2795, 'thankful': 2796, 'complex': 2797, 'books,': 2798, 'insane': 2799, 'again...': 2800, 'absolute': 2801, 'exist': 2802, 'deal.': 2803, 'situation.': 2804, 'anime': 2805, 'strike': 2806, 'play.': 2807, 'drawn': 2808, '\"well,': 2809, 'repeat': 2810, 'breaks': 2811, 'grass': 2812, 'primary': 2813, 'plot': 2814, 'haha!': 2815, 'pain.': 2816, 'introduced': 2817, 'chem': 2818, 'damned': 2819, 'western': 2820, 'disappointed': 2821, 'lips': 2822, 'intelligence': 2823, 'physics': 2824, 'mile': 2825, 'tune': 2826, 'remains': 2827, 'steven': 2828, 'quarter': 2829, 'company.': 2830, 'area.': 2831, \"judge's\": 2832, 'vision': 2833, 'mini': 2834, \"don't.\": 2835, 'kicking': 2836, 'touched': 2837, 'golf': 2838, 'paper.': 2839, 'circle': 2840, 'troops': 2841, 'tied': 2842, 'body.': 2843, 'scheduled': 2844, 'photographer:': 2845, 'dick': 2846, 'prom': 2847, 'conversations': 2848, 'february': 2849, 'reagan': 2850, 'ones.': 2851, 'afterwards': 2852, 'partner': 2853, 'fabulous': 2854, 'another.': 2855, 'easter': 2856, 'benefit': 2857, 'punk': 2858, 'movies.': 2859, 'purchase': 2860, 'server': 2861, 'college.': 2862, 'reaction': 2863, 'noise': 2864, 'guest': 2865, 'heather': 2866, 'singer': 2867, 'ridiculous': 2868, 'problems.': 2869, 'silent': 2870, 'two,': 2871, 'you...': 2872, 'because,': 2873, 'para': 2874, 'surgery': 2875, 'cafe': 2876, 'balls': 2877, 'darn': 2878, 'water,': 2879, 'files': 2880, 'suffering': 2881, 'wondered': 2882, 'band.': 2883, 'games.': 2884, 'sets': 2885, 'sentence': 2886, 'answered': 2887, 'larger': 2888, 'detail': 2889, 'leave.': 2890, 'darkness': 2891, 'spare': 2892, 'threat': 2893, 'coast': 2894, 'seek': 2895, 'lauren': 2896, 'leaders': 2897, 'floor.': 2898, 'opposed': 2899, 'time...': 2900, 'goals': 2901, 'rocks': 2902, 'mood.': 2903, 'baby.': 2904, 'bird': 2905, 'republicans': 2906, 'cold,': 2907, 'comedy': 2908, 'interesting,': 2909, 'village': 2910, 'short,': 2911, 'balance': 2912, 'minds': 2913, 'urge': 2914, 'argue': 2915, 'ocean': 2916, 'quotes': 2917, 'settle': 2918, 'discover': 2919, 'definately': 2920, 'described': 2921, 'yeah!': 2922, 'billy': 2923, 'pathetic': 2924, 'bowling': 2925, 'group.': 2926, 'variety': 2927, 'customer': 2928, 'range': 2929, 'stop.': 2930, 'care,': 2931, 'merely': 2932, 'purple': 2933, \"world's\": 2934, 'upper': 2935, 'blew': 2936, 'geo.': 2937, 'is...': 2938, 'shock': 2939, 'often.': 2940, 'store.': 2941, 'outside.': 2942, 'fool': 2943, 'greek': 2944, 'innocent': 2945, 'useful': 2946, 'dead.': 2947, 'announced': 2948, 'remember,': 2949, 'happens.': 2950, 'praying': 2951, 'halloween': 2952, 'reporting': 2953, 'please,': 2954, 'blessed': 2955, 'upstairs': 2956, 'understood': 2957, 'neat': 2958, 'blonde': 2959, 'experiences': 2960, 'bite': 2961, 'perform': 2962, 'decisions': 2963, 'website.': 2964, 'experienced': 2965, 'relatively': 2966, 'idiot': 2967, 'amanda': 2968, 'honestly,': 2969, 'tuesday,': 2970, 'activities': 2971, 'thursday.': 2972, 'mean.': 2973, 'scenes': 2974, 'cold.': 2975, 'congrats': 2976, 'talent': 2977, 'boxes': 2978, 'hey!': 2979, 'here!': 2980, 'seriously.': 2981, 'regardless': 2982, 'require': 2983, 'theatre': 2984, 'handed': 2985, 'neighborhood': 2986, 'houses': 2987, 'wise': 2988, 'template': 2989, 'buddy': 2990, 'improve': 2991, 'reported': 2992, 'mine,': 2993, 'carried': 2994, 'apologize': 2995, 'crash': 2996, 'costs': 2997, 'production': 2998, 'office,': 2999, 'canadian': 3000, 'chosen': 3001, 'raining': 3002, '(yes,': 3003, '(even': 3004, 'holds': 3005, 'second,': 3006, 'easy.': 3007, 'understand.': 3008, 'independent': 3009, 'tennis': 3010, 'skip': 3011, 'period.': 3012, 'opinions': 3013, 'team.': 3014, 'smiling': 3015, 'coach': 3016, 'belong': 3017, 'closest': 3018, 'areas': 3019, 'beautiful.': 3020, 'replace': 3021, 'film.': 3022, 'suck.': 3023, 'works.': 3024, 'arts': 3025, 'appeared': 3026, 'birthday,': 3027, 'wisdom': 3028, 'justice': 3029, 'convention': 3030, 'business.': 3031, 'wednesday,': 3032, 'early.': 3033, 'tank': 3034, 'possibility': 3035, 'iraq,': 3036, 'develop': 3037, 'note:': 3038, 'facts': 3039, 'swing': 3040, 'entries': 3041, 'testing': 3042, 'hour,': 3043, 'jesse': 3044, 'life?': 3045, 'pleased': 3046, 'breathe': 3047, 'anyways.': 3048, 'highest': 3049, 'prolly': 3050, 'grey': 3051, 'today:': 3052, 'digest': 3053, 'want,': 3054, 'artist': 3055, 'marks': 3056, 'traditional': 3057, 'street.': 3058, 'sake': 3059, 'downstairs': 3060, 'bridge': 3061, 'weekly': 3062, 'dream.': 3063, 'parents.': 3064, 'babies': 3065, 'burned': 3066, 'hurt.': 3067, 'mirror': 3068, 'triumph': 3069, \"(it's\": 3070, 'day...': 3071, 'hook': 3072, 'removed': 3073, 'himself.': 3074, 'ever,': 3075, 'gained': 3076, 'gorgeous': 3077, 'arrive': 3078, 'usual.': 3079, 'families': 3080, 'anyone.': 3081, 'gone,': 3082, 'pregnant': 3083, 'band,': 3084, 'universe': 3085, 'county': 3086, 'basically,': 3087, 'kung': 3088, 'drives': 3089, 'usual,': 3090, 'obsessed': 3091, 'mom.': 3092, 'bitter': 3093, 'surrounded': 3094, 'actually.': 3095, 'supposedly': 3096, 'haha,': 3097, 'wrapped': 3098, '\"oh,': 3099, 'affect': 3100, 'attached': 3101, 'chest': 3102, 'campus': 3103, 'accident': 3104, 'cute.': 3105, 'yell': 3106, 'nuclear': 3107, 'extended': 3108, 'used:': 3109, 'reviews': 3110, 'curious': 3111, 'attacks': 3112, \"that'll\": 3113, 'annual': 3114, 'heart,': 3115, 'fast.': 3116, 'children.': 3117, 'little.': 3118, 'salt': 3119, 'lift': 3120, 'country,': 3121, 'junk': 3122, 'history.': 3123, 'proof': 3124, 'pour': 3125, 'internet.': 3126, 'writing.': 3127, 'them!': 3128, 'budget': 3129, 'conclusion': 3130, '100%': 3131, 'laugh.': 3132, 'activity': 3133, 'paris': 3134, 'newspaper': 3135, 'fruit': 3136, 'tuesday.': 3137, 'realise': 3138, 'closing': 3139, 'false': 3140, 'door,': 3141, 'beating': 3142, 'martin': 3143, 'trips': 3144, 'funniest': 3145, 'painful': 3146, 'eat.': 3147, 'change,': 3148, 'status': 3149, 'painting': 3150, '9/11': 3151, 'requires': 3152, 'hurting': 3153, 'hmm...': 3154, 'critical': 3155, 'laying': 3156, 'them?': 3157, 'causing': 3158, '(although': 3159, 'live.': 3160, 'movement': 3161, 'goes.': 3162, 'shocked': 3163, 'belief': 3164, 'option': 3165, 'hiding': 3166, 'corsair': 3167, 'records': 3168, 'damn,': 3169, 'filling': 3170, 'disney': 3171, 'voting': 3172, 'flew': 3173, 'flip': 3174, 'resources': 3175, 'license': 3176, 'grad': 3177, 'birds': 3178, 'say?': 3179, 'vegas': 3180, 'sides': 3181, 'rolled': 3182, 'hundreds': 3183, 'salad': 3184, 'themselves.': 3185, 'teams': 3186, 'there?': 3187, 'someone.': 3188, 'garbage': 3189, 'anyhow,': 3190, \"ya'll\": 3191, 'respond': 3192, 'project.': 3193, 'justin': 3194, 'church,': 3195, 'yelled': 3196, 'spoken': 3197, 'pile': 3198, 'stores': 3199, 'songs.': 3200, \"mother's\": 3201, 'past,': 3202, 'discussed': 3203, 'swim': 3204, 'task': 3205, 'workout': 3206, 'will,': 3207, 'frustrated': 3208, 'ages': 3209, 'gifts': 3210, 'yes!': 3211, 'itself.': 3212, 'naman': 3213, 'efforts': 3214, 'inches': 3215, 'amusing': 3216, 'from.': 3217, 'history,': 3218, 'rachel': 3219, 'chemistry': 3220, 'wednesday.': 3221, 'finals': 3222, 'competition': 3223, 'cultural': 3224, 'award': 3225, 'reading.': 3226, 'stock': 3227, 'ought': 3228, 'choices': 3229, 'package': 3230, 'league': 3231, 'france': 3232, 'coke': 3233, 'angels': 3234, 'cry.': 3235, 'display': 3236, 'college,': 3237, 'cool!': 3238, 'dropping': 3239, 'praise': 3240, 'chatting': 3241, 'stairs': 3242, 'careful': 3243, 'influence': 3244, 'equal': 3245, 'provides': 3246, 'proceeded': 3247, 'stuffed': 3248, 'associated': 3249, 'amazed': 3250, 'stepped': 3251, 'daniel': 3252, 'wishing': 3253, 'frank': 3254, 'chill': 3255, 'smith': 3256, 'jealous': 3257, 'marketing': 3258, 'relax': 3259, 'impression': 3260, 'hands.': 3261, 'completed': 3262, 'returning': 3263, 'whilst': 3264, 'worn': 3265, 'working.': 3266, 'slight': 3267, 'precious': 3268, 'freaked': 3269, 'growth': 3270, 'jazz': 3271, 'intelligent': 3272, \"would've\": 3273, '\"but': 3274, 'yea,': 3275, 'phone,': 3276, 'sits': 3277, 'past.': 3278, 'painted': 3279, 'generation': 3280, 'damage': 3281, 'instant': 3282, 'very,': 3283, 'thick': 3284, 'definition': 3285, 'depends': 3286, 'wings': 3287, 'anytime': 3288, 'applied': 3289, 'struggle': 3290, 'adventure': 3291, 'frozen': 3292, 'ugh.': 3293, 'determined': 3294, 'happened,': 3295, 'town,': 3296, 'busy.': 3297, 'mobile': 3298, 'hooked': 3299, 'crossed': 3300, 'terrorist': 3301, 'addicted': 3302, 'unit': 3303, 'wow!': 3304, 'look.': 3305, 'humor': 3306, 'destroy': 3307, 'favor': 3308, 'please.': 3309, 'blog!': 3310, 'slip': 3311, 'games,': 3312, 'yay.': 3313, 'flag': 3314, 'nite': 3315, 'population': 3316, 'childhood': 3317, 'picture.': 3318, 'supporting': 3319, 'lost.': 3320, 'appearance': 3321, 'dad,': 3322, 'korean': 3323, 'someone,': 3324, 'pictures.': 3325, 'jennifer': 3326, 'deeply': 3327, 'weekends': 3328, 'depending': 3329, 'gray': 3330, 'entered': 3331, 'europe': 3332, 'yeah...': 3333, 'bound': 3334, 'pleasant': 3335, 'depressing': 3336, 'suicide': 3337, 'halfway': 3338, 'realizing': 3339, 'studied': 3340, 'dog.': 3341, 'turkey': 3342, 'yahoo': 3343, 'inside.': 3344, 'opportunities': 3345, 'case.': 3346, 'useless': 3347, 'prime': 3348, 'early,': 3349, 'smells': 3350, 'updates': 3351, 'systems': 3352, 'boy.': 3353, 'grandmother': 3354, 'air.': 3355, 'sweet.': 3356, 'homework.': 3357, 'seen.': 3358, 'attention.': 3359, 'levels': 3360, 'headache': 3361, 'solution': 3362, 'amounts': 3363, \"guy's\": 3364, 'anniversary': 3365, 'entertaining': 3366, 'either,': 3367, 'plate': 3368, 'you.\"': 3369, 'questions.': 3370, 'festival': 3371, 'night!': 3372, 'drivers': 3373, 'lord,': 3374, 'shoulder': 3375, 'settled': 3376, 'think?': 3377, 'offers': 3378, 'killer': 3379, 'sooooo': 3380, 'acts': 3381, \"man's\": 3382, 'contest': 3383, 'forms': 3384, 'physically': 3385, 'holidays': 3386, '(see': 3387, 'cheer': 3388, 'duty': 3389, 'pete': 3390, 'out...': 3391, 'laws': 3392, 'jane': 3393, 'catholic': 3394, 'stops': 3395, 'someday': 3396, 'stupid,': 3397, 'seats': 3398, 'buzz': 3399, 'kyle': 3400, 'capable': 3401, 'feeling.': 3402, 'academic': 3403, 'goodness': 3404, 'phil': 3405, 'vice': 3406, 'girls,': 3407, 'html': 3408, 'know...': 3409, 'century': 3410, 'parents,': 3411, 'roommate': 3412, 'bomb': 3413, 'productive': 3414, 'teenage': 3415, 'stare': 3416, 'state.': 3417, 'environment': 3418, 'russian': 3419, 'hilarious.': 3420, 'apartment.': 3421, 'awards': 3422, 'here?': 3423, 'communication': 3424, 'still.': 3425, 'doctors': 3426, 'wasting': 3427, 'girls.': 3428, 'prevent': 3429, 'warning': 3430, 'road.': 3431, 'episodes': 3432, 'starbucks': 3433, 'packing': 3434, 'owner': 3435, 'remember.': 3436, 'scare': 3437, 'garage': 3438, 'safety': 3439, 'linux': 3440, 'hero': 3441, 'hindi': 3442, 'suffer': 3443, 'humans': 3444, 'shout': 3445, 'socks': 3446, 'am):': 3447, 'decides': 3448, 'fried': 3449, 'maintain': 3450, 'along.': 3451, 'european': 3452, 'produce': 3453, 'clark': 3454, 'largest': 3455, 'freakin': 3456, 'hmm,': 3457, 'encourage': 3458, 'oscar': 3459, 'look,': 3460, 'cousins': 3461, 'flow': 3462, 'keys': 3463, 'utterly': 3464, 'mistakes': 3465, '\"and': 3466, 'it.\"': 3467, 'specifically': 3468, 'candidates': 3469, 'allan': 3470, 'focused': 3471, 'dedicated': 3472, 'season.': 3473, 't-shirt': 3474, \"didn't.\": 3475, 'purchased': 3476, 'horse': 3477, 'alternative': 3478, 'neighbors': 3479, 'prince': 3480, 'illegal': 3481, '(well,': 3482, 'incident': 3483, 'stephen': 3484, 'wished': 3485, 'japan': 3486, 'survive': 3487, 'technical': 3488, 'blocks': 3489, 'unknown': 3490, 'juice': 3491, 'blah.': 3492, 'wars': 3493, 'equally': 3494, 'causes': 3495, 'relate': 3496, 'clouds': 3497, 'machines': 3498, 'line,': 3499, 'serving': 3500, 'keyboard': 3501, 'back!': 3502, 'advertising': 3503, 'toronto': 3504, 'prior': 3505, 'sand': 3506, 'users': 3507, 'properly': 3508, 'died.': 3509, \"night's\": 3510, 'happen,': 3511, 'refer': 3512, 'benefits': 3513, 'listed': 3514, 'grades': 3515, 'sooner': 3516, 'crime': 3517, 'suspect': 3518, 'basis': 3519, 'reader': 3520, 'memorial': 3521, 'combination': 3522, 'guard': 3523, 'writers': 3524, 'contract': 3525, 'washing': 3526, 'products': 3527, \"brother's\": 3528, 'mexican': 3529, 'christmas.': 3530, 'philosophy': 3531, 'street,': 3532, 'bills': 3533, 'allowing': 3534, 'bush,': 3535, 'portion': 3536, 'crazy,': 3537, 'bars': 3538, 'coverage': 3539, 'days!': 3540, 'daddy': 3541, '(they': 3542, 'refused': 3543, 'too...': 3544, 'driven': 3545, 'user': 3546, 'this...': 3547, 'gmail': 3548, 'stopping': 3549, 'officials': 3550, 'mere': 3551, 'wood': 3552, 'going,': 3553, 'hope.': 3554, \"women's\": 3555, 'liking': 3556, 'china': 3557, 'wrap': 3558, 'fired': 3559, 'thinking.': 3560, 'started.': 3561, 'length': 3562, 'employees': 3563, 'sick,': 3564, 'attending': 3565, 'offering': 3566, 'outside,': 3567, 'tongue': 3568, 'actor': 3569, 'dreaming': 3570, 'wierd': 3571, 'paper,': 3572, 'gang': 3573, 'endless': 3574, 'merry': 3575, 'concern': 3576, 'valley': 3577, 'hubby': 3578, 'java': 3579, 'larry': 3580, 'facing': 3581, 'abuse': 3582, 'supreme': 3583, 'call.': 3584, 'confidence': 3585, 'previously': 3586, 'stretch': 3587, 'subway': 3588, 'forum': 3589, 'compare': 3590, 'replaced': 3591, 'secretary': 3592, 'everyone!': 3593, 'ruin': 3594, 'freshman': 3595, 'wireless': 3596, 'inch': 3597, 'disease': 3598, 'thingy': 3599, 'failure': 3600, 'electric': 3601, 'problem,': 3602, 'directed': 3603, 'america.': 3604, 'sticking': 3605, 'shirts': 3606, 'weird,': 3607, 'own,': 3608, 'mystery': 3609, 'one!': 3610, 'whatever,': 3611, 'today?': 3612, 'store,': 3613, 'jacket': 3614, 'knock': 3615, 'time?': 3616, 'therefore,': 3617, 'chad': 3618, 'chatted': 3619, 'fireworks': 3620, 'alas,': 3621, 'engine': 3622, 'property': 3623, 'overly': 3624, 'blogged': 3625, 'fiction': 3626, 'alright.': 3627, 'relationship.': 3628, 'leadership': 3629, 'songs,': 3630, 'joining': 3631, '\"don\\'t': 3632, 'you\"': 3633, 'there...': 3634, 'attended': 3635, 'lock': 3636, 'owned': 3637, 'cuts': 3638, 'economy': 3639, 'stronger': 3640, 'leather': 3641, 'woman.': 3642, 'children,': 3643, 'cookies': 3644, 'exciting.': 3645, 'rode': 3646, 'guess,': 3647, 'fits': 3648, 'idea,': 3649, 'atleast': 3650, 'tool': 3651, 'beautiful,': 3652, 'washed': 3653, 'method': 3654, 'wishes': 3655, 'get.': 3656, 'deserves': 3657, 'seeking': 3658, 'dad.': 3659, 'shoes.': 3660, 'knocked': 3661, 'hardest': 3662, 'choir': 3663, 'signing': 3664, 'labor': 3665, 'literacy': 3666, 'blast': 3667, 'murder': 3668, 'issue.': 3669, 'climb': 3670, '(though': 3671, 'tone': 3672, 'popped': 3673, 'start.': 3674, 'jimmy': 3675, 'prices': 3676, 'michelle': 3677, 'asleep.': 3678, 'woman,': 3679, 'core': 3680, 'mighty': 3681, 'solo': 3682, 'anyway...': 3683, 'nothing,': 3684, 'shortaznpwer06:': 3685, 'blowing': 3686, 'smile.': 3687, 'dozen': 3688, 'attractive': 3689, 'aint': 3690, 'dawn': 3691, 'question,': 3692, \"else's\": 3693, 'violence': 3694, 'sandwich': 3695, 'covers': 3696, 'effective': 3697, 'run.': 3698, 'who,': 3699, 'poll': 3700, 'test,': 3701, 'fought': 3702, 'albums': 3703, 'boring,': 3704, 'smiled': 3705, 'kurt': 3706, 'request': 3707, 'rain.': 3708, 'celebrity': 3709, 'age.': 3710, 'scale': 3711, 'pain,': 3712, 'excitement': 3713, 'preparing': 3714, 'choice.': 3715, 'ways.': 3716, 'service.': 3717, 'breathing': 3718, 'brother,': 3719, 'intended': 3720, 'bath': 3721, 'updating': 3722, 'online,': 3723, 'move.': 3724, 'sister,': 3725, 'bar.': 3726, 'yep,': 3727, 'graduated': 3728, 'hence': 3729, 'word,': 3730, 'michigan': 3731, 'also.': 3732, 'date.': 3733, 'horror': 3734, 'voice.': 3735, 'deny': 3736, 'location': 3737, 'minister': 3738, 'drunken': 3739, \"children's\": 3740, 'perspective': 3741, 'talk.': 3742, 'worse,': 3743, 'gross': 3744, 'presented': 3745, 'about?': 3746, 'big,': 3747, 'habit': 3748, 'hot,': 3749, 'light.': 3750, 'such.': 3751, 'occasionally': 3752, 'closet': 3753, 'earth.': 3754, 'fifth': 3755, 'floating': 3756, 'urban': 3757, 'sleepy': 3758, 'states.': 3759, 'intense': 3760, 'complicated': 3761, 'kid.': 3762, 'wonderful.': 3763, 'worrying': 3764, 'meow': 3765, 'draft': 3766, 'edit': 3767, 'desperate': 3768, 'individuals': 3769, 'all...': 3770, 'emailed': 3771, 'adults': 3772, 'prison': 3773, 'tomorrow!': 3774, 'glory': 3775, 'yummy': 3776, 'him!': 3777, 'perfect.': 3778, 'description': 3779, 'beats': 3780, 'struggling': 3781, 'additional': 3782, 'profile': 3783, \"where's\": 3784, 'tastes': 3785, 'feel.': 3786, 'break,': 3787, 'rude': 3788, 'thoughts.': 3789, 'strip': 3790, 'man!': 3791, 'nations': 3792, 'pointless': 3793, 'process.': 3794, 'awkward': 3795, 'scary.': 3796, 'developing': 3797, \"(that's\": 3798, 'pattern': 3799, 'clothing': 3800, 'wicked': 3801, 'answer.': 3802, 'real.': 3803, 'bugs': 3804, 'control.': 3805, 'kissed': 3806, 'museum': 3807, 'kicks': 3808, 'emergency': 3809, '\"when': 3810, 'rented': 3811, 'covering': 3812, \"(don't\": 3813, 'located': 3814, 'teen': 3815, 'writing,': 3816, '(just': 3817, 'centre': 3818, 'worry,': 3819, 'standards': 3820, 'retarded': 3821, 'alive.': 3822, 'can,': 3823, 'peanut': 3824, 'exception': 3825, 'connected': 3826, 'what.': 3827, 'iron': 3828, \"sister's\": 3829, '2004.': 3830, 'blank': 3831, 'houston': 3832, 'miserable': 3833, 'firm': 3834, 'bodies': 3835, 'watch.': 3836, 'film,': 3837, 'window.': 3838, 'jewish': 3839, 'dust': 3840, 'recently,': 3841, 'park.': 3842, 'folks,': 3843, 'sauce': 3844, 'ripped': 3845, 'britney': 3846, 'roof': 3847, 'begun': 3848, 'entitled': 3849, 'order.': 3850, 'powers': 3851, 'demand': 3852, 'workers': 3853, 'creepy': 3854, 'ancient': 3855, 'meanwhile,': 3856, 'abortion': 3857, 'linked': 3858, 'register': 3859, 'women.': 3860, 'voices': 3861, 'cycle': 3862, 'important.': 3863, 'new,': 3864, 'sharp': 3865, 'artists': 3866, 'editor': 3867, 'highlight': 3868, 'assistant': 3869, 'organization': 3870, 'shoe': 3871, 'chief': 3872, 'senate': 3873, 'power.': 3874, 'officer': 3875, 'for?': 3876, 'vacation.': 3877, 'worthy': 3878, 'confident': 3879, 'dates': 3880, 'wing': 3881, 'awesome!': 3882, 'thier': 3883, 'item': 3884, 'initial': 3885, 'jones': 3886, 'wheel': 3887, 'issues.': 3888, 'on...': 3889, 'said:': 3890, 'shorts': 3891, 'peace,': 3892, 'try.': 3893, 'partly': 3894, 'shot.': 3895, 'assuming': 3896, 'happier': 3897, 'speaks': 3898, 'noon': 3899, 'senator': 3900, 'points.': 3901, 'been.': 3902, 'less.': 3903, 'function': 3904, 'win.': 3905, 'organized': 3906, 'longest': 3907, 'stated': 3908, 'jerry': 3909, 'jeremy': 3910, 'know!': 3911, 'attempts': 3912, 'cities': 3913, 'disturbing': 3914, 'dislike': 3915, 'spin': 3916, 'plant': 3917, 'shep': 3918, 'replied': 3919, 'relaxing': 3920, \"america's\": 3921, 'committed': 3922, 'violent': 3923, 'customers': 3924, \"valentine's\": 3925, 'cooked': 3926, 'script': 3927, 'scored': 3928, 'awesome,': 3929, 'parade': 3930, 'dream,': 3931, '(one': 3932, 'need.': 3933, 'rainy': 3934, 'diary': 3935, 'solve': 3936, 'otherwise,': 3937, 'importance': 3938, 'marching': 3939, 'limit': 3940, 'options': 3941, 'tina': 3942, 'providing': 3943, 'chain': 3944, 'believes': 3945, 'flower': 3946, 'maybe,': 3947, 'everywhere.': 3948, 'quickly.': 3949, 'regularly': 3950, \"life's\": 3951, 'already,': 3952, 'ralph': 3953, 'audio': 3954, 'album,': 3955, 'new.': 3956, 'attempting': 3957, 'reading,': 3958, 'dirt': 3959, 'loser': 3960, 'factor': 3961, 'bye.': 3962, 'anyway?': 3963, 'bullshit': 3964, 'achieve': 3965, 'christians': 3966, 'victory': 3967, 'park,': 3968, 'charlie': 3969, 'little,': 3970, 'strongly': 3971, 'opens': 3972, '(from': 3973, 'dude,': 3974, 'basement': 3975, 'naturally': 3976, 'potato': 3977, 'root': 3978, 'bass': 3979, 'african': 3980, 'edwards': 3981, 'immediate': 3982, 'palm': 3983, \"week's\": 3984, 'crap,': 3985, 'rooms': 3986, 'existence': 3987, 'deleted': 3988, 'permanent': 3989, 'pity': 3990, 'shaking': 3991, 'group,': 3992, 'congress': 3993, 'sex.': 3994, 'week!': 3995, 'yourself,': 3996, 'trial': 3997, 'somehow,': 3998, 'jassi': 3999, '(except': 4000, 'truth.': 4001, 'hella': 4002, 'bush.': 4003, \"kerry's\": 4004, 'amazingly': 4005, 'continuing': 4006, 'poems': 4007, 'behavior': 4008, 'torn': 4009, 'suggestions': 4010, 'guys!': 4011, 'mouth.': 4012, 'delicious': 4013, 'wendy': 4014, 'again?': 4015, 'space.': 4016, 'shower.': 4017, 'open.': 4018, 'mode': 4019, 'grant': 4020, 'carefully': 4021, 'traveling': 4022, 'citizens': 4023, 'depression': 4024, 'historical': 4025, \"freakin'\": 4026, 'transfer': 4027, 'table.': 4028, 'district': 4029, 'aaron': 4030, 'deeper': 4031, 'jackie': 4032, 'hopefully,': 4033, 'relief': 4034, 'expression': 4035, 'numerous': 4036, 'feedback': 4037, 'reaching': 4038, 'ultimately': 4039, 'goes,': 4040, 'concentrate': 4041, 'reflect': 4042, 'laughter': 4043, 'exhausted': 4044, 'shopping.': 4045, 'literature': 4046, 'eggs': 4047, 'jenny': 4048, 'attempted': 4049, 'ignored': 4050, 'ministry': 4051, 'advanced': 4052, 'truth,': 4053, 'write.': 4054, 'sweat': 4055, 'desperately': 4056, 'granted': 4057, 'patient': 4058, 'freezing': 4059, 'matthew': 4060, 'ground.': 4061, 'sport': 4062, 'knows.': 4063, 'furniture': 4064, 'prize': 4065, 'chips': 4066, 'austin': 4067, 'virus': 4068, 'were.': 4069, 'phase': 4070, 'virginia': 4071, 'gathering': 4072, 'honest,': 4073, 'meets': 4074, 'voters': 4075, 'company,': 4076, 'rest.': 4077, 'biology': 4078, 'night...': 4079, 'cases': 4080, 'measure': 4081, 'come,': 4082, 'age,': 4083, 'belly': 4084, 'susan,': 4085, 'sheer': 4086, 'hehe!': 4087, 'vast': 4088, 'simon': 4089, 'hire': 4090, 'explains': 4091, 'idol': 4092, 'topics': 4093, 'treatment': 4094, 'increased': 4095, 'road,': 4096, 'weather.': 4097, 'farm': 4098, 'it..': 4099, 'nuts': 4100, 'resume': 4101, 'ship': 4102, 'elizabeth': 4103, 'sex,': 4104, 'electronic': 4105, 'princess': 4106, 'happily': 4107, 'amber': 4108, 'aids': 4109, 'randomly': 4110, 'blown': 4111, 'essentially': 4112, 'pretending': 4113, 'quest': 4114, 'remaining': 4115, 'accidentally': 4116, 'awhile.': 4117, 'directions': 4118, 'beneath': 4119, 'wherever': 4120, 'volume': 4121, 'beer.': 4122, 'free,': 4123, 'explaining': 4124, 'moment:': 4125, 'torture': 4126, 'moderate': 4127, 'lecture': 4128, 'camp.': 4129, 'selfish': 4130, 'frame': 4131, 'clothes,': 4132, 'enemy': 4133, 'surrounding': 4134, 'trick': 4135, 'terror': 4136, 'sixth': 4137, 'intellectual': 4138, 'tower': 4139, 'routine': 4140, 'drive.': 4141, 'america,': 4142, 'feet.': 4143, 'mommy': 4144, 'answering': 4145, '\"hey,': 4146, 'clever': 4147, 'drink.': 4148, 'classes.': 4149, 'involves': 4150, 'bloggers': 4151, 'capital': 4152, 'strange.': 4153, 'men.': 4154, 'thirty': 4155, 'error': 4156, 'matter,': 4157, 'you:': 4158, 'pointing': 4159, 'registered': 4160, 'recognized': 4161, 'made.': 4162, 'confusing': 4163, 'chorus': 4164, 'conversation.': 4165, 'visual': 4166, 'does,': 4167, 'up...': 4168, 'vanilla': 4169, 'ghost': 4170, 'surprisingly': 4171, 'hoped': 4172, 'mad.': 4173, 'experience,': 4174, 'construction': 4175, 'cartoon': 4176, 'situations': 4177, 'brad': 4178, 'monster': 4179, 'longer.': 4180, 'eddie': 4181, 'goddamn': 4182, 'aspect': 4183, 'death,': 4184, 'chris,': 4185, 'touching': 4186, 'anyone,': 4187, 'scheme': 4188, 'publish': 4189, 'rock.': 4190, 'me..': 4191, 'shining': 4192, 'inspiration': 4193, 'gary': 4194, 'videos': 4195, 'supply': 4196, 'english,': 4197, 'dennis': 4198, 'install': 4199, 'maths': 4200, 'dunno.': 4201, 'spelling': 4202, 'waves': 4203, 'camping': 4204, 'often,': 4205, 'shell': 4206, 'indeed.': 4207, 'men,': 4208, 'sucked.': 4209, 'play,': 4210, 'brush': 4211, 'bizarre': 4212, 'terrorists': 4213, 'julie': 4214, 'neighbor': 4215, 'capture': 4216, 'apparent': 4217, 'best,': 4218, 'knees': 4219, 'reasonable': 4220, 'passes': 4221, 'insane.': 4222, 'governor': 4223, 'pocket': 4224, 'here...': 4225, 'stolen': 4226, 'remembering': 4227, 'system,': 4228, '\"hey': 4229, 'plan.': 4230, 'mentally': 4231, 'proved': 4232, 'cookie': 4233, 'peace.': 4234, 'captain': 4235, 'shortly': 4236, 'ohio': 4237, '\"no,': 4238, 'marten': 4239, 'cheers': 4240, 'but...': 4241, 'installed': 4242, 'emotion': 4243, 'hockey': 4244, 'seemingly': 4245, \"father's\": 4246, 'internal': 4247, 'editing': 4248, 'celebration': 4249, 'brave': 4250, 'pouring': 4251, 'reasons.': 4252, 'winner': 4253, 'clothes.': 4254, 'floor,': 4255, 'bitch.': 4256, 'shine': 4257, 'action.': 4258, 'wears': 4259, 'attacked': 4260, 'pillow': 4261, 'printed': 4262, 'danced': 4263, 'keith': 4264, 'ask?': 4265, 'name:': 4266, 'cruise': 4267, 'arguments': 4268, 'collect': 4269, 'produced': 4270, 'hunt': 4271, 'democracy': 4272, 'claimed': 4273, 'coffee.': 4274, 'grateful': 4275, 'occasional': 4276, 'mother,': 4277, 'kidding': 4278, 'latin': 4279, 'rolls': 4280, 'educational': 4281, 'panic': 4282, 'luck.': 4283, 'dallas': 4284, 'identify': 4285, 'monitor': 4286, 'fate': 4287, 'interviews': 4288, 'indeed,': 4289, 'out?': 4290, 'beloved': 4291, 'burger': 4292, 'small,': 4293, 'knee': 4294, 'hosting': 4295, 'brother.': 4296, \"80's\": 4297, 'elected': 4298, 'genius': 4299, 'colour': 4300, '(she': 4301, 'holes': 4302, 'help,': 4303, 'volunteer': 4304, 'instance,': 4305, \"everything's\": 4306, 'elementary': 4307, 'string': 4308, 'council': 4309, '\"that': 4310, 'sensitive': 4311, 'dragged': 4312, 'next.': 4313, 'thanks,': 4314, 'annoying.': 4315, 'breast': 4316, 'downloaded': 4317, 'fears': 4318, 'run,': 4319, 'cheesy': 4320, 'tools': 4321, 'fifteen': 4322, 'changed.': 4323, 'high.': 4324, 'charged': 4325, 'talented': 4326, 'switched': 4327, '\"that\\'s': 4328, 'cared': 4329, 'lane': 4330, 'beef': 4331, 'dull': 4332, 'program.': 4333, 'caring': 4334, 'engaged': 4335, 'lives,': 4336, 'lighter': 4337, 'involving': 4338, 'pound': 4339, 'slam': 4340, 'copies': 4341, 'news:': 4342, 'drum': 4343, 'god!': 4344, 'die,': 4345, 'buildings': 4346, 'faced': 4347, 'promises': 4348, 'uncomfortable': 4349, 'shawn': 4350, 'poster': 4351, 'pants.': 4352, 'route': 4353, 'recording': 4354, 'doing,': 4355, 'typed': 4356, 'gentle': 4357, 'cloud': 4358, 'sounding': 4359, 'karissa': 4360, 'patience': 4361, 'sources': 4362, 'slide': 4363, 'ones,': 4364, 'light,': 4365, 'state,': 4366, 'edition': 4367, 'referring': 4368, 'agent': 4369, 'use.': 4370, 'smooth': 4371, 'reality.': 4372, 'area,': 4373, 'future,': 4374, 'information.': 4375, 'afterwards,': 4376, 'offended': 4377, 'happening.': 4378, 'geek': 4379, 'cops': 4380, 'porch': 4381, 'wandering': 4382, 'not?': 4383, 'tradition': 4384, 'values': 4385, 'wall.': 4386, 'coming.': 4387, 'mins': 4388, 'combined': 4389, 'defend': 4390, 'bar,': 4391, 'forgetting': 4392, 'wins': 4393, 'weekend!': 4394, 'highway': 4395, 'sink': 4396, 'protest': 4397, 'funeral': 4398, 'well!': 4399, 'twisted': 4400, 'featured': 4401, 'cough': 4402, 'arent': 4403, 'sadly': 4404, 'else?': 4405, 'students.': 4406, 'level.': 4407, 'wal-mart': 4408, 'recorded': 4409, 'nicely': 4410, 'possible,': 4411, 'guns': 4412, 'underneath': 4413, 'analysis': 4414, 'dreams.': 4415, 'maybe.': 4416, 'fault.': 4417, 'folks.': 4418, 'sweet,': 4419, 'personally,': 4420, 'dressing': 4421, 'luckily,': 4422, 'landed': 4423, 'nicole': 4424, 'cute,': 4425, 'accomplished': 4426, 'cigarette': 4427, 'funny!': 4428, 'both.': 4429, 'plants': 4430, 'earn': 4431, 'frequently': 4432, 'had,': 4433, 'ceremony': 4434, 'strikes': 4435, 'wedding.': 4436, 'jonah': 4437, 'relevant': 4438, 'delete': 4439, 'year!': 4440, 'jake': 4441, 'medicine': 4442, 'strategy': 4443, 'expected.': 4444, 'body,': 4445, 'another,': 4446, 'assignment': 4447, 'chasing': 4448, 'soul.': 4449, 'damn.': 4450, 'youre': 4451, 'desert': 4452, 'twins': 4453, 'talk,': 4454, 'and...': 4455, 'borrow': 4456, 'life...': 4457, 'normal.': 4458, 'income': 4459, 'somewhere.': 4460, 'committee': 4461, 'kills': 4462, 'though...': 4463, 'mean?': 4464, 'punch': 4465, 'always,': 4466, 'close.': 4467, 'knife': 4468, 'bottles': 4469, 'concerning': 4470, 'zero': 4471, 'navy': 4472, 'inside,': 4473, 'ideal': 4474, 'rather,': 4475, 'coat': 4476, 'appeal': 4477, 'internet,': 4478, 'destruction': 4479, 'programming': 4480, 'ignoring': 4481, 'quietly': 4482, 'courage': 4483, 'them...': 4484, 'automatically': 4485, 'guessing': 4486, 'homeless': 4487, 'spinning': 4488, 'eastern': 4489, 'do...': 4490, 'reflection': 4491, 'cancelled': 4492, 'documentary': 4493, 'trip,': 4494, 'confirmed': 4495, 'lasted': 4496, 'assumed': 4497, 'sunshine': 4498, 'card.': 4499, 'ass,': 4500, 'seattle': 4501, 'like...': 4502, 'ready.': 4503, 'chuck': 4504, 'actors': 4505, 'practical': 4506, 'colin': 4507, 'frankly,': 4508, 'gods': 4509, 'australia': 4510, 'emotionally': 4511, 'motion': 4512, 'bothers': 4513, 'object': 4514, 'sacrifice': 4515, 'cheaper': 4516, 'thoroughly': 4517, 'english.': 4518, 'dish': 4519, 'connect': 4520, 'went.': 4521, '486-6380': 4522, 'chicks': 4523, 'bond': 4524, 'sara': 4525, 'this!': 4526, 'crashed': 4527, 'marshall': 4528, 'utter': 4529, 'secure': 4530, 'society.': 4531, 'fire.': 4532, '\"you\\'re': 4533, 'pace': 4534, 'sun.': 4535, 'building.': 4536, 'stays': 4537, 'tables': 4538, 'ashley': 4539, 'layout': 4540, 'determine': 4541, 'survey': 4542, 'shoulders': 4543, 'were,': 4544, 'scientific': 4545, 'performed': 4546, 'newly': 4547, 'terry': 4548, 'says.': 4549, 'thing...': 4550, 'live,': 4551, 'native': 4552, 'nearby': 4553, 'sweater': 4554, 'frustrating': 4555, 'sense,': 4556, 'courtesy': 4557, 'shadow': 4558, 'mexico': 4559, 'website,': 4560, 'eternal': 4561, 'cure': 4562, 'revealed': 4563, 'sushi': 4564, 'women,': 4565, 'skinny': 4566, 'aspects': 4567, 'joan': 4568, 'performing': 4569, 'later!': 4570, 'bored,': 4571, 'authority': 4572, 'thanks.': 4573, 'banana': 4574, 'pays': 4575, 'one?': 4576, 'taco': 4577, 'set.': 4578, 'advance': 4579, 'forest': 4580, 'rides': 4581, 'danger': 4582, 'kisses': 4583, 'crystal': 4584, 'unusual': 4585, 'bare': 4586, 'greatly': 4587, 'reveal': 4588, 'page,': 4589, 'janet': 4590, 'fuck.': 4591, 'places.': 4592, 'format': 4593, 'him...': 4594, 'technically': 4595, 'referred': 4596, 'involve': 4597, 'examples': 4598, \"can't.\": 4599, 'meetings': 4600, 'taxes': 4601, 'muscle': 4602, 'label': 4603, 'hmmm...': 4604, 'amongst': 4605, 'horribly': 4606, 'condition': 4607, 'misses': 4608, 'questions,': 4609, 'drops': 4610, 'guessed': 4611, 'irish': 4612, '(because': 4613, '\"all': 4614, 'spam': 4615, 'mate': 4616, 'rushed': 4617, 'tale': 4618, 'witness': 4619, 'chip': 4620, 'cheney': 4621, 'him?': 4622, 'rain,': 4623, 'busy,': 4624, 'child,': 4625, 'voice,': 4626, 'remote': 4627, 'johnson': 4628, 'beliefs': 4629, 'lor.': 4630, 'trail': 4631, 'viewing': 4632, 'commitment': 4633, 'stranger': 4634, 'entry.': 4635, '...and': 4636, 'phones': 4637, 'tracks': 4638, 'column': 4639, 'hurry': 4640, 'enjoyable': 4641, 'soon!': 4642, 'disgusting': 4643, 'wound': 4644, 'helpful': 4645, 'ordinary': 4646, 'northern': 4647, 'kitty': 4648, 'sticks': 4649, 'beach.': 4650, 'much!': 4651, 'penny': 4652, 'domestic': 4653, 'jersey': 4654, 'approximately': 4655, 'honey': 4656, 'sadly,': 4657, 'joey': 4658, 'featuring': 4659, 'chance.': 4660, 'young,': 4661, 'lied': 4662, 'practice.': 4663, 'follows': 4664, 'ashamed': 4665, 'alan': 4666, 'jess': 4667, 'manner': 4668, 'revolution': 4669, 'teacher,': 4670, 'rising': 4671, 'relaxed': 4672, 'worries': 4673, 'avoiding': 4674, 'style.': 4675, 'laugh,': 4676, 'tips': 4677, 'email.': 4678, 'interests': 4679, 'ruined': 4680, 'gather': 4681, 'proposed': 4682, 'skirt': 4683, 'pepper': 4684, 'executive': 4685, 'toys': 4686, 'megan': 4687, 'alien': 4688, 'bruce': 4689, 'supported': 4690, 'lesbian': 4691, 'distant': 4692, 'define': 4693, 'joke.': 4694, 'hugs': 4695, 'granted,': 4696, 'hired': 4697, 'curse': 4698, 'failing': 4699, 'vehicle': 4700, 'chase': 4701, 'academy': 4702, 'believing': 4703, 'cups': 4704, 'everyday.': 4705, 'parked': 4706, 'kissing': 4707, 'pictures,': 4708, 'comments.': 4709, 'surfing': 4710, 'context': 4711, 'hurts.': 4712, 'carpet': 4713, 'votes': 4714, 'roads': 4715, 'indie': 4716, 'leave,': 4717, 'henry': 4718, 'problems,': 4719, 'being.': 4720, 'box.': 4721, 'guardian': 4722, 'report.': 4723, 'sucks,': 4724, 'formal': 4725, 'team,': 4726, '(including': 4727, \"how's\": 4728, 'needing': 4729, 'nails': 4730, 'germany': 4731, 'hands,': 4732, 'kid,': 4733, 'dark,': 4734, 'stared': 4735, 'armed': 4736, 'explore': 4737, 'represent': 4738, 'deciding': 4739, 'recently.': 4740, 'mother.': 4741, 'grandparents': 4742, 'royal': 4743, 'contains': 4744, 'shower,': 4745, 'funky': 4746, 'costume': 4747, 'table,': 4748, 'cruel': 4749, 'pirates': 4750, 'kingdom': 4751, 'through,': 4752, 'one...': 4753, 'overall,': 4754, 'stage.': 4755, 'extent': 4756, 'people!': 4757, 'mars': 4758, 'attracted': 4759, 'peaceful': 4760, 'it).': 4761, 'taylor': 4762, 'admire': 4763, 'randy': 4764, 'couples': 4765, 'buried': 4766, '(all': 4767, 'although,': 4768, 'storage': 4769, 'soundtrack': 4770, 'top.': 4771, 'president,': 4772, 'protection': 4773, 'expressed': 4774, 'backed': 4775, 'kidding.': 4776, 'semester.': 4777, 'hehe,': 4778, 'romance': 4779, 'sammy': 4780, 'study.': 4781, 'album.': 4782, 'me.\"': 4783, 'business,': 4784, 'association': 4785, 'document': 4786, 'event.': 4787, 'ensure': 4788, 'eat,': 4789, 'disagree': 4790, 'criminal': 4791, 'feelings.': 4792, '(what': 4793, 'celebrating': 4794, 'peoples': 4795, 'asshole': 4796, 'dead,': 4797, 'learnt': 4798, 'commission': 4799, 'creation': 4800, 'penis': 4801, 'accused': 4802, 'though!': 4803, 'real,': 4804, 'visit.': 4805, 'sun,': 4806, 'battery': 4807, 'corn': 4808, 'thus,': 4809, 'twelve': 4810, 'ebay': 4811, 'burst': 4812, 'nap.': 4813, 'rehearsal': 4814, 'pisses': 4815, 'smiles': 4816, 'zone': 4817, 'command': 4818, 'resolution': 4819, '2003.': 4820, 'verse': 4821, 'increasingly': 4822, '(i.e.': 4823, 'debt': 4824, 'oldest': 4825, '\"are': 4826, 'imagined': 4827, 'father,': 4828, 'scares': 4829, 'intend': 4830, 'high,': 4831, 'crawl': 4832, 'well..': 4833, 'slice': 4834, 'wooden': 4835, 'thesis': 4836, 'stories.': 4837, '\"there': 4838, 'occurred': 4839, 'mountains': 4840, 'instruction': 4841, 'general.': 4842, 'different,': 4843, 'meeting.': 4844, 'promote': 4845, 'glorious': 4846, 'patrick': 4847, 'client': 4848, 'fields': 4849, 'education.': 4850, 'resist': 4851, 'williams': 4852, 'policies': 4853, 'ford': 4854, 'applications': 4855, 'launch': 4856, 'natalie': 4857, 'participate': 4858, 'crowded': 4859, 'states,': 4860, 'congratulations': 4861, 'downloading': 4862, 'motivation': 4863, 'cooler': 4864, 'recipe': 4865, 'casual': 4866, 'ironic': 4867, 'props': 4868, 'yet?': 4869, 'easy,': 4870, \"yesterday's\": 4871, 'francisco': 4872, 'affected': 4873, 'satisfied': 4874, 'hills': 4875, 'surface': 4876, 'minute.': 4877, 'soap': 4878, 'lightning': 4879, 'slipped': 4880, 'miller': 4881, 'canon': 4882, 'soda': 4883, 'stealing': 4884, 'select': 4885, 'off!': 4886, 'next,': 4887, 'adventures': 4888, 'twin': 4889, 'dramatic': 4890, 'blogs.': 4891, 'dishes': 4892, 'sheet': 4893, 'fortune': 4894, 'ways,': 4895, 'boys.': 4896, 'fall.': 4897, 'president.': 4898, 'breakfast.': 4899, 'list,': 4900, 'stream': 4901, 'itself,': 4902, 'cubs': 4903, 'chilled': 4904, 'racing': 4905, 'gore': 4906, 'recommended': 4907, 'dreamed': 4908, 'graphic': 4909, 'same,': 4910, 'severe': 4911, \"could've\": 4912, 'people?': 4913, 'translation': 4914, 'practicing': 4915, 'trapped': 4916, 'grade.': 4917, 'working,': 4918, 'situation,': 4919, 'skipped': 4920, 'element': 4921, 'suffered': 4922, 'child.': 4923, 'colored': 4924, 'susan': 4925, 'strangely': 4926, 'song:': 4927, 'ideas.': 4928, 'dance.': 4929, 'marked': 4930, 'temporary': 4931, 'bastard': 4932, 'ease': 4933, 'drunk.': 4934, 'arrested': 4935, 'virtually': 4936, 'frankly': 4937, 'along,': 4938, 'spotted': 4939, 'essential': 4940, 'oddly': 4941, 'largely': 4942, 'domain': 4943, 'pulls': 4944, 'employee': 4945, 'wonders': 4946, 'heavily': 4947, 'prayers': 4948, 'classes,': 4949, 'night?': 4950, 'flesh': 4951, 'obviously,': 4952, 'bend': 4953, 'foundation': 4954, 'concert.': 4955, 'haha..': 4956, 'lists': 4957, 'surprising': 4958, 'operating': 4959, 'hmmm': 4960, 'receiving': 4961, 'victim': 4962, 'universal': 4963, '=350=350\">': 4964, 'evan': 4965, 'needed.': 4966, 'places,': 4967, 'day?': 4968, 'trouble.': 4969, 'underwear': 4970, 'gay.': 4971, 'latter': 4972, 'vietnam': 4973, 'raising': 4974, 'conflict': 4975, 'grandfather': 4976, 'describes': 4977, 'rubber': 4978, 'trained': 4979, 'desk.': 4980, 'engineering': 4981, 'rape': 4982, 'hottie': 4983, 'confused.': 4984, 'tube': 4985, 'figures': 4986, '21st': 4987, 'lifetime': 4988, 'relatives': 4989, 'outlook': 4990, 'smile,': 4991, 'shed': 4992, 'way...': 4993, 'shirt.': 4994, 'virtual': 4995, 'parent': 4996, 'devil': 4997, 'improved': 4998, 'leaning': 4999, 'opinion,': 5000, 'steak': 5001, 'davis': 5002, 'carolina': 5003, 'spelled': 5004, 'holla': 5005, 'suppose.': 5006, 'medium': 5007, 'survived': 5008, 'delivered': 5009, 'form.': 5010, 'entering': 5011, 'embrace': 5012, 'yahoo!': 5013, 'dreamt': 5014, 'lawn': 5015, 'dog,': 5016, 'happens,': 5017, 'jail': 5018, 'rite': 5019, 'messing': 5020, 'blues': 5021, 'work!': 5022, 'article,': 5023, 'whore': 5024, 'cat.': 5025, 'monthly': 5026, 'are:': 5027, 'wife,': 5028, 'acted': 5029, 'comics': 5030, 'wanted.': 5031, 'secrets': 5032, 'sour': 5033, 'yup,': 5034, 'justify': 5035, 'bears': 5036, 'stir': 5037, 'explanation': 5038, 'fond': 5039, 'bday': 5040, 'woohoo!': 5041, 'earlier.': 5042, 'yoga': 5043, 'thoughts,': 5044, 'politicians': 5045, 'folk': 5046, 'means,': 5047, 'lean': 5048, 'fund': 5049, 'loaded': 5050, 'ipod': 5051, 'slap': 5052, 'fast,': 5053, 'telephone': 5054, 'coffee,': 5055, 'outfit': 5056, 'gate': 5057, 'pledge': 5058, 'handful': 5059, 'impressive': 5060, 'carol': 5061, 'grandpa': 5062, 'fade': 5063, 'message.': 5064, 'deliver': 5065, 'commit': 5066, 'becuase': 5067, 'temperature': 5068, 'posts.': 5069, 'meaningful': 5070, 'contain': 5071, 'australian': 5072, 'rage': 5073, 'minimum': 5074, 'picks': 5075, 'tasted': 5076, 'ronald': 5077, '(after': 5078, 'wait!': 5079, 'screen.': 5080, 'clip': 5081, 'series.': 5082, 'assigned': 5083, 'trailer': 5084, 'exit': 5085, 'absence': 5086, 'rambling': 5087, '\"get': 5088, 'permission': 5089, 'nail': 5090, 'ask.': 5091, 'differences': 5092, 'earlier,': 5093, 'mama': 5094, 'oven': 5095, 'accurate': 5096, '\"well': 5097, \"other's\": 5098, 'homework,': 5099, 'boots': 5100, 'anxious': 5101, 'choosing': 5102, 'defined': 5103, 'robin': 5104, 'structure': 5105, 'pool.': 5106, 'nurse': 5107, 'hatred': 5108, 'oral': 5109, 'introduce': 5110, 'hahaha': 5111, 'dump': 5112, 'number.': 5113, 'closely': 5114, 'onion': 5115, 'democrat': 5116, 'behind.': 5117, 'drink,': 5118, 'lost,': 5119, 'joint': 5120, 'atmosphere': 5121, 'life!': 5122, 'frustration': 5123, 'bonus': 5124, 'cents': 5125, 'electricity': 5126, 'loyal': 5127, '\"just': 5128, 'knitting': 5129, 'harm': 5130, 'cracked': 5131, 'temple': 5132, 'beach,': 5133, 'ever!': 5134, 'married.': 5135, 'expectations': 5136, 'stuff...': 5137, 'second.': 5138, 'periods': 5139, 'sneak': 5140, 'celebrated': 5141, 'club.': 5142, 'desktop': 5143, 'newest': 5144, 'girlfriend.': 5145, 'art.': 5146, 'concerns': 5147, 'jamie': 5148, 'christmas,': 5149, 'fascinating': 5150, 'yall': 5151, 'important,': 5152, 'opinion.': 5153, 'wandered': 5154, 'gallery': 5155, 'last.': 5156, 'son,': 5157, 'scan': 5158, 'lighting': 5159, 'logical': 5160, 'bubble': 5161, 'ask,': 5162, 'feeding': 5163, 'detailed': 5164, 'cinema': 5165, 'dunno,': 5166, 'machine.': 5167, 'himself,': 5168, 'fridge': 5169, 'enjoy.': 5170, 'update:': 5171, 'running.': 5172, 'established': 5173, 'owns': 5174, 'classroom': 5175, 'accomplish': 5176, 'submit': 5177, 'thing?': 5178, 'pastor': 5179, 'tell.': 5180, 'believe.': 5181, 'archives': 5182, 'captured': 5183, 'wrestling': 5184, 'many,': 5185, \"fuckin'\": 5186, '\"yeah,': 5187, 'hmmm.': 5188, 'direction.': 5189, 'cry,': 5190, 'pills': 5191, 'concrete': 5192, 'scratch': 5193, 'adds': 5194, 'sony': 5195, 'sample': 5196, 'one)': 5197, 'would.': 5198, 'date,': 5199, 'anthony': 5200, 'gettin': 5201, 'spoil': 5202, 'bang': 5203, 'admitted': 5204, 'roughly': 5205, 'lemon': 5206, 'art,': 5207, 'half.': 5208, 'unhappy': 5209, 'special.': 5210, 'elements': 5211, 'climbing': 5212, 'bones': 5213, 'outsourcing': 5214, 'blah,': 5215, 'anne': 5216, 'classical': 5217, 'mysterious': 5218, 'to...': 5219, 'deals': 5220, 'teacher.': 5221, 'korea': 5222, 'becca': 5223, 'blogging.': 5224, 'objects': 5225, 'bent': 5226, 'project,': 5227, 'present.': 5228, 'dining': 5229, 'wilson': 5230, 'destroyed': 5231, 'steel': 5232, 'muscles': 5233, 'commentary': 5234, 'empire': 5235, 'fact.': 5236, 'forcing': 5237, 'patch': 5238, 'say:': 5239, 'rock,': 5240, 'intention': 5241, 'guilt': 5242, 'pretty.': 5243, 'harsh': 5244, 'burnt': 5245, 'trek': 5246, 'highlights': 5247, 'stop,': 5248, 'gossip': 5249, 'gosh,': 5250, 'power,': 5251, 'sheets': 5252, 'unfortunate': 5253, \"school's\": 5254, 'strawberry': 5255, 'picture,': 5256, 'government.': 5257, 'metro': 5258, 'obsession': 5259, 'websites': 5260, 'hurt,': 5261, 'scene.': 5262, 'donald': 5263, 'wipe': 5264, 'air,': 5265, 'dated': 5266, '(well': 5267, 'adams': 5268, 'responded': 5269, 'flies': 5270, 'titles': 5271, 'courses': 5272, 'kasi': 5273, 'lewis': 5274, 'marriage.': 5275, 'proven': 5276, 'sadness': 5277, 'shoes,': 5278, 'unexpected': 5279, 'go...': 5280, 'tends': 5281, 'martha': 5282, 'singapore': 5283, 'shook': 5284, 'argh.': 5285, 'hospital.': 5286, 'happy!': 5287, 'hunting': 5288, 'nightmare': 5289, 'blows': 5290, 'more...': 5291, 'ties': 5292, 'kong': 5293, 'constitution': 5294, \"girl's\": 5295, 'nature.': 5296, 'hyper': 5297, 'imagination': 5298, 'decision.': 5299, 'you?\"': 5300, 'write,': 5301, 'contribute': 5302, 'circumstances': 5303, 'statements': 5304, 'roses': 5305, 'open,': 5306, 'sister.': 5307, 'noble': 5308, 'gathered': 5309, 'service,': 5310, 'won.': 5311, 'lounge': 5312, 'selection': 5313, 'lover': 5314, 'disc': 5315, 'reporters': 5316, 'red,': 5317, 'shadows': 5318, 'shops': 5319, 'jesus,': 5320, 'crying.': 5321, 'straight.': 5322, 'anonymous': 5323, 'miami': 5324, 'section.': 5325, 'queer': 5326, 'starring': 5327, 'exists': 5328, 'experiencing': 5329, 'apartment,': 5330, 'identity': 5331, 'thunder': 5332, 'calendar': 5333, 'danny': 5334, 'eye.': 5335, 'right!': 5336, 'bone': 5337, 'operation': 5338, 'one:': 5339, 'understand,': 5340, 'thrilled': 5341, 'apart.': 5342, 'ceiling': 5343, \"tyke's\": 5344, 'stations': 5345, 'lovely.': 5346, 'devoted': 5347, 'minus': 5348, 'digest:': 5349, 'pitch': 5350, 'blog...': 5351, 'division': 5352, 'mothers': 5353, 'exist.': 5354, 'pops': 5355, 'documents': 5356, 'gear': 5357, 'cracking': 5358, 'overcome': 5359, 'baked': 5360, 'earned': 5361, 'beaten': 5362, 'discovery': 5363, 'chemical': 5364, 'twist': 5365, '(aka': 5366, \"don't,\": 5367, \"person's\": 5368, 'fights': 5369, 'bitching': 5370, 'blue.': 5371, 'belt': 5372, 'support.': 5373, 'valuable': 5374, \"tomorrow's\": 5375, 'twice.': 5376, 'ride.': 5377, 'hardcore': 5378, 'lacking': 5379, 'smoked': 5380, 'increasing': 5381, 'neil': 5382, 'walk.': 5383, 'some.': 5384, 'layer': 5385, 'thankfully': 5386, 'terrorism': 5387, 'drunk,': 5388, 'york,': 5389, 'site:': 5390, 'guests': 5391, 'cat,': 5392, 'subjects': 5393, 'anywho,': 5394, 'elections': 5395, 'interviewed': 5396, 'hannah': 5397, 'reality,': 5398, 'persons': 5399, 'pearl': 5400, 'fixing': 5401, 'announce': 5402, 'peeps': 5403, 'iowa': 5404, 'jonathan': 5405, 'blog?': 5406, 'beer,': 5407, 'beth': 5408, 'difference.': 5409, 'scientists': 5410, 'station.': 5411, 'rocked': 5412, 'chaos': 5413, 'relationship,': 5414, 'bobby': 5415, 'rocks.': 5416, 'relative': 5417, 'equipment': 5418, 'eventually,': 5419, 'knows,': 5420, 'louis': 5421, 'window,': 5422, 'somewhere,': 5423, 'stumbled': 5424, 'tiger': 5425, 'throws': 5426, 'asked,': 5427, 'invented': 5428, 'fries': 5429, 'formed': 5430, 'virgin': 5431, 'amendment': 5432, 'question:': 5433, 'heh,': 5434, 'shiny': 5435, 'hating': 5436, 'launched': 5437, 'applying': 5438, 'digest.': 5439, 'camera.': 5440, 'eager': 5441, 'crossing': 5442, 'means.': 5443, 'smart,': 5444, 'dentist': 5445, 'rained': 5446, 'warned': 5447, 'logic': 5448, 'promptly': 5449, 'character.': 5450, 'shows.': 5451, 'abandoned': 5452, 'results.': 5453, 'makeup': 5454, 'bleeding': 5455, 'christ.': 5456, 'season,': 5457, 'device': 5458, 'fame': 5459, 'cherry': 5460, 'boyfriend.': 5461, 'jesus.': 5462, 'exposed': 5463, 'speakers': 5464, 'breeze': 5465, 'melbourne': 5466, 'sings': 5467, 'bothering': 5468, 'talking,': 5469, 'elvis': 5470, 'joking': 5471, 'steady': 5472, 'secretly': 5473, 'skool': 5474, 'poet': 5475, 'hmmm,': 5476, 'mainstream': 5477, 'card,': 5478, 'victoria': 5479, 'laughs': 5480, 'feel,': 5481, 'after.': 5482, 'serves': 5483, 'information,': 5484, 'whereas': 5485, 'earth,': 5486, 'digging': 5487, 'bunny': 5488, 'conservatives': 5489, '\"good': 5490, 'reunion': 5491, 'bus.': 5492, 'issue,': 5493, 'adorable': 5494, 'bacon': 5495, 'phoenix': 5496, 'institute': 5497, 'equivalent': 5498, 'lawyer': 5499, 'amazing,': 5500, 'conditions': 5501, 'boys,': 5502, 'position.': 5503, 'stories,': 5504, 'fence': 5505, 'delivery': 5506, 'arguing': 5507, 'involved.': 5508, 'spit': 5509, 'bench': 5510, '\"who': 5511, 'ours': 5512, '\"they': 5513, 'conversation,': 5514, 'field.': 5515, 'rainbow': 5516, 'bryan': 5517, 'hussein': 5518, 'joy.': 5519, 'dumped': 5520, 'reception': 5521, 'ignorant': 5522, 'vodka': 5523, 'record,': 5524, 'declared': 5525, 'auto': 5526, 'understands': 5527, 'deserved': 5528, 'asses': 5529, 'community.': 5530, 'minute,': 5531, 'carl': 5532, 'here:': 5533, 'shirt,': 5534, 'reporter': 5535, 'admit,': 5536, 'white,': 5537, 'puppy': 5538, 'existing': 5539, 'attention,': 5540, 'gently': 5541, 'agency': 5542, 'funding': 5543, 'notion': 5544, 'hint': 5545, 'charges': 5546, 'moms': 5547, 'capacity': 5548, 'olive': 5549, 'experiment': 5550, 'odd.': 5551, '(when': 5552, 'comment.': 5553, 'judging': 5554, 'boot': 5555, 'lined': 5556, 'rely': 5557, 'branch': 5558, 'tail': 5559, 'good...': 5560, 'language.': 5561, 'most.': 5562, 'roger': 5563, 'gospel': 5564, 'b-day': 5565, 'confusion': 5566, 'trend': 5567, 'garlic': 5568, 'angry.': 5569, 'regards': 5570, 'miracle': 5571, 'collective': 5572, 'cricket': 5573, 'meds': 5574, 'baby!': 5575, 'commercials': 5576, 'smelled': 5577, 'lemme': 5578, 'crisis': 5579, 'dreams,': 5580, 'entrance': 5581, 'motivated': 5582, 'deal,': 5583, 'later...': 5584, 'issues,': 5585, 'swear,': 5586, 'photography': 5587, 'pill': 5588, 'view.': 5589, 'snow.': 5590, 'upload': 5591, 'shade': 5592, 'evil.': 5593, 'suits': 5594, 'blocked': 5595, 'environmental': 5596, 'dragon': 5597, 'period,': 5598, \"cd's\": 5599, 'untill': 5600, 'positions': 5601, 'beans': 5602, 'diego': 5603, 'paranoid': 5604, 'plug': 5605, 'nate': 5606, 'mature': 5607, 'taxi': 5608, 'independence': 5609, 'tales': 5610, 'bold': 5611, 'roman': 5612, 'fuel': 5613, 'midst': 5614, 'tuna': 5615, 'hello,': 5616, 'suggests': 5617, 'frequent': 5618, 'souls': 5619, 'started,': 5620, 'mayor': 5621, 'themselves,': 5622, 'tire': 5623, 'cent': 5624, '(thanks': 5625, 'consisted': 5626, 'so-called': 5627, 'safely': 5628, 'liberals': 5629, 'meals': 5630, 'foods': 5631, 'guts': 5632, 'toss': 5633, '7:30': 5634, 'materials': 5635, 'afterwards.': 5636, \"woman's\": 5637, 'shorter': 5638, 'luke': 5639, 'irony': 5640, 'saying.': 5641, 'annoy': 5642, '\"not': 5643, 'overwhelming': 5644, 'pork': 5645, 'karen': 5646, 'encouraged': 5647, 'literary': 5648, 'laden': 5649, \"o'clock\": 5650, 'anxiety': 5651, 'magical': 5652, 'pissing': 5653, 'checks': 5654, 'liquid': 5655, 'meantime,': 5656, '8:30': 5657, 'see...': 5658, 'brains': 5659, '(sorry': 5660, 'inform': 5661, 'lifestyle': 5662, 'passionate': 5663, 'otherwise.': 5664, 'missions': 5665, 'robot': 5666, 'divine': 5667, 'temp': 5668, 'copyright': 5669, 'tribute': 5670, 'jordan': 5671, 'type.': 5672, 'john,': 5673, 'fever': 5674, 'gaming': 5675, 'excited,': 5676, 'something?': 5677, 'drive,': 5678, 'ideas,': 5679, 'ahead.': 5680, 'thai': 5681, 'choice,': 5682, 'freaky': 5683, 'pursue': 5684, 'menu': 5685, 'ourselves.': 5686, 'short.': 5687, 'claiming': 5688, 'wala': 5689, 'encounter': 5690, 'joel': 5691, 'broadcast': 5692, 'shopping,': 5693, 'tempted': 5694, 'dies': 5695, \"'til\": 5696, 'speak.': 5697, 'spots': 5698, 'simple.': 5699, 'tasty': 5700, 'stressful': 5701, 'space,': 5702, 'blue,': 5703, 'matching': 5704, 'time)': 5705, 'love:': 5706, 'genuinely': 5707, 'introduction': 5708, 'algebra': 5709, 'homes': 5710, 'sound.': 5711, 'leslie': 5712, 'orders': 5713, 'faithful': 5714, 'printer': 5715, 'successfully': 5716, '10th': 5717, 'reminder': 5718, 'banned': 5719, 'pump': 5720, 'nader': 5721, 'bore': 5722, 'fat,': 5723, 'deck': 5724, 'horrible.': 5725, 'importantly,': 5726, 'heres': 5727, 'spray': 5728, 'could.': 5729, 'contained': 5730, 'woods': 5731, 'general,': 5732, 'craving': 5733, 'call,': 5734, 'finished.': 5735, 'that)': 5736, 'comp': 5737, 'cleared': 5738, 'from,': 5739, 'category': 5740, 'anywhere.': 5741, 'challenging': 5742, 'anyone?': 5743, 'iraqis': 5744, 'see?': 5745, 'viewed': 5746, 'noted': 5747, 'underground': 5748, 'mall.': 5749, 'gates': 5750, 'donate': 5751, 'work...': 5752, 'booth': 5753, 'surf': 5754, 'sucking': 5755, 'borrowed': 5756, 'account.': 5757, 'publishing': 5758, 'chairs': 5759, 'hosted': 5760, 'brain.': 5761, 'realization': 5762, 'education,': 5763, 'bye!': 5764, 'remained': 5765, 'weight.': 5766, 'library.': 5767, 'ghetto': 5768, 'mood,': 5769, 'scores': 5770, 'craig': 5771, 'carter': 5772, 'ridiculous.': 5773, 'set,': 5774, 'represents': 5775, 'like:': 5776, 'center.': 5777, 'reduce': 5778, \"that'd\": 5779, 'home!': 5780, 'toes': 5781, 'boyfriend,': 5782, 'charlotte': 5783, 'mask': 5784, 'gym.': 5785, 'snap': 5786, 'cape': 5787, 'home...': 5788, 'herself.': 5789, 'weblog': 5790, 'channels': 5791, 'africa': 5792, 'return.': 5793, 'suddenly,': 5794, 'eats': 5795, 'yankees': 5796, 'fan,': 5797, 'building,': 5798, 'witty': 5799, 'surprise.': 5800, 'now..': 5801, 'embarrassed': 5802, 'hillary': 5803, 'pasta': 5804, 'screamed': 5805, 'cigarettes': 5806, 'married,': 5807, '20th': 5808, 'names.': 5809, 'paragraph': 5810, '5:30': 5811, 'accounts': 5812, 'nearest': 5813, 'race.': 5814, 'black,': 5815, 'fuzzy': 5816, 'tragic': 5817, 'resort': 5818, 'returns': 5819, 'beers': 5820, 'methods': 5821, 'election.': 5822, 'love?': 5823, 'speaker': 5824, 'angeles': 5825, 'expert': 5826, 'should.': 5827, 'shuchomouff:': 5828, 'jobs,': 5829, 'doing?': 5830, 'fishing': 5831, 'spears': 5832, 'visitors': 5833, 'out:': 5834, 'tell,': 5835, 'commented': 5836, 'scared.': 5837, 'her!': 5838, '\"your': 5839, 'helen': 5840, 'sends': 5841, 'essays': 5842, 'fair.': 5843, 'fitting': 5844, 'loan': 5845, 'then...': 5846, 'such,': 5847, 'borders': 5848, 'big.': 5849, 'talking.': 5850, 'diane': 5851, 'been,': 5852, 'shrek': 5853, 'avenue': 5854, '6:30': 5855, '\"what\\'s': 5856, 'fun...': 5857, 'program,': 5858, 'strange,': 5859, 'laughing.': 5860, 'pressed': 5861, 'heal': 5862, 'artistic': 5863, 'rushing': 5864, 'entertain': 5865, 'partially': 5866, 'tendency': 5867, 'seconds.': 5868, 'blanket': 5869, 'treasure': 5870, 'matches': 5871, 'grasp': 5872, 'cream.': 5873, 'you)': 5874, 'crashing': 5875, \"parents'\": 5876, 'pirate': 5877, 'favorites': 5878, 'was...': 5879, 'grows': 5880, 'rating': 5881, '9:30': 5882, 'tournament': 5883, 'hectic': 5884, 'arnold': 5885, 'creates': 5886, 'maria': 5887, 'atlanta': 5888, 'stuff!': 5889, 'bean': 5890, 'fortunately,': 5891, 'close,': 5892, 'burden': 5893, 'some,': 5894, 'spider': 5895, 'written.': 5896, 'threatening': 5897, 'idiot.': 5898, 'strangers': 5899, 'ross': 5900, 'year?': 5901, 'nope': 5902, 'sigh': 5903, 'challenges': 5904, 'sky.': 5905, 'club,': 5906, 'public.': 5907, 'bathroom.': 5908, 'defeat': 5909, 'border': 5910, 'all?': 5911, 'communicate': 5912, 'itunes': 5913, 'basis.': 5914, 'hurricane': 5915, 'subtle': 5916, 'rejected': 5917, 'hoo!': 5918, 'co-workers': 5919, 'typically': 5920, 'disorder': 5921, 'duck': 5922, 'connections': 5923, 'port': 5924, 'breakfast,': 5925, 'accent': 5926, 'resource': 5927, 'lines.': 5928, 'apologies': 5929, 'christina': 5930, 'winds': 5931, 'acceptance': 5932, 'chicago,': 5933, 'comparison': 5934, 'society,': 5935, 'simple,': 5936, 'bullshit.': 5937, 'loud.': 5938, 'mario': 5939, 'humble': 5940, 'figuring': 5941, \"driver's\": 5942, 'poured': 5943, 'tonight!': 5944, 'yang': 5945, 'skill': 5946, 'jen,': 5947, 'california.': 5948, 'pale': 5949, 'hungry.': 5950, 'mess.': 5951, 'lazy.': 5952, 'shocking': 5953, 'politics,': 5954, 'circles': 5955, 'purpose.': 5956, 'stage,': 5957, 'wallet': 5958, 'locker': 5959, 'smack': 5960, 'cake.': 5961, 'shouting': 5962, 'purely': 5963, 'enormous': 5964, 'haircut': 5965, 'hall.': 5966, 'article.': 5967, 'squeeze': 5968, 'plans.': 5969, 'forever,': 5970, 'edited': 5971, 'tooth': 5972, '\"one': 5973, 'nerve': 5974, 'blogger.': 5975, 'browser': 5976, 'discussions': 5977, 'occur': 5978, 'marine': 5979, 'minority': 5980, 'regard': 5981, 'indiana': 5982, '(yes': 5983, 'marathon': 5984, 'fan.': 5985, 'booked': 5986, 'busted': 5987, \"mcdonald's\": 5988, 'students,': 5989, 'law.': 5990, 'july.': 5991, 'effectively': 5992, 'symbol': 5993, 'work?': 5994, \"john's\": 5995, 'answer,': 5996, 'rounds': 5997, 'spiderman': 5998, 'selected': 5999, 'faculty': 6000, 'detroit': 6001, 'week:': 6002, 'surprise,': 6003, 'clean.': 6004, 'days...': 6005, 'something...': 6006, 'difficult.': 6007, 'invitation': 6008, 'consumed': 6009, 'heaven.': 6010, 'journalism': 6011, 'sunset': 6012, 'profound': 6013, 'debating': 6014, 'strong,': 6015, 'graphics': 6016, 'details.': 6017, 'jews': 6018, 'height': 6019, 'rental': 6020, 'timing': 6021, 'shouldnt': 6022, 'fortunate': 6023, 'conscious': 6024, 'lousy': 6025, 'yay!!': 6026, 'taped': 6027, 'complained': 6028, 'countless': 6029, 'encouraging': 6030, 'dealt': 6031, 'blessing': 6032, 'retreat': 6033, 'spot.': 6034, 'shows,': 6035, '[listening': 6036, 'stepping': 6037, 'fear,': 6038, 'rescue': 6039, 'fire,': 6040, 'kennedy': 6041, 'toast': 6042, 'models': 6043, 'events.': 6044, 'arizona': 6045, 'patterns': 6046, 'poets': 6047, 'email,': 6048, 'realli': 6049, 'initially': 6050, 'lie.': 6051, 'joseph': 6052, '(especially': 6053, 'accepting': 6054, 'elevator': 6055, 'courtney': 6056, 'madness': 6057, 'approached': 6058, 'invisible': 6059, 'arriving': 6060, 'loud,': 6061, 'nuts.': 6062, 'above.': 6063, 'thing!': 6064, 'rally': 6065, 'relations': 6066, 'roskilly': 6067, 'evil,': 6068, 'her?': 6069, 'ok...': 6070, 'update.': 6071, 'pants,': 6072, 'like?': 6073, 'sat.': 6074, 'contacts': 6075, 'proves': 6076, 'believe,': 6077, 'tears.': 6078, 'owners': 6079, 'success.': 6080, 'asked.': 6081, 'passage': 6082, 'color:': 6083, 'appreciated': 6084, 'interview.': 6085, 'subject.': 6086, 'lowest': 6087, 'superior': 6088, 'preparation': 6089, 'weather,': 6090, 'soooooo': 6091, 'bake': 6092, 'hope,': 6093, 'idiots': 6094, 'fred': 6095, 'combat': 6096, 'searched': 6097, 'faith.': 6098, 'cells': 6099, 'lion': 6100, 'platform': 6101, 'career.': 6102, 'shaped': 6103, 'below.': 6104, 'teenagers': 6105, 'gosh': 6106, 'etc...': 6107, 'allison': 6108, 'versions': 6109, 'far:': 6110, 'visits': 6111, 'gender': 6112, 'anyhoo,': 6113, 'questioning': 6114, 'schedule.': 6115, 'government,': 6116, 'recieved': 6117, 'acceptable': 6118, 'israel': 6119, 'quoted': 6120, 'commenting': 6121, 'illinois': 6122, 'fort': 6123, 'flaming': 6124, 'xmas': 6125, 'saudi': 6126, 'poker': 6127, 'opera': 6128, 'accident.': 6129, 'worlds': 6130, 'ridiculously': 6131, 'skills.': 6132, 'potatoes': 6133, 'charming': 6134, 'engage': 6135, 'region': 6136, 'como': 6137, 'barry': 6138, 'reasons,': 6139, 'diamond': 6140, 'shallow': 6141, 'republic': 6142, 'blair': 6143, 'paul,': 6144, 'fairy': 6145, 'lines,': 6146, 'vote.': 6147, 'depth': 6148, 'last,': 6149, 'infamous': 6150, 'fifty': 6151, 'alert': 6152, '\"for': 6153, 'self.': 6154, 'always.': 6155, 'instructions': 6156, 'grill': 6157, 'silly.': 6158, 'co-worker': 6159, 'black.': 6160, 'consequences': 6161, 'effect.': 6162, 'odds': 6163, 'mentioning': 6164, 'allen': 6165, 'chew': 6166, 'offense': 6167, 'treating': 6168, 'father.': 6169, 'threatened': 6170, 'mornings': 6171, 'factory': 6172, 'lol..': 6173, 'cowboy': 6174, 'vampire': 6175, 'blast.': 6176, '(where': 6177, 'serious.': 6178, 'units': 6179, 'made,': 6180, 'globe': 6181, 'chicago.': 6182, 'link.': 6183, 'upside': 6184, 'campaign.': 6185, 'dylan': 6186, 'unlimited': 6187, 'tested': 6188, 'graduating': 6189, 'playing.': 6190, 'jobs.': 6191, 'messenger': 6192, 'bag.': 6193, 'whining': 6194, 'friggin': 6195, 'flipping': 6196, 'buddies': 6197, 'habits': 6198, 'bull': 6199, 'references': 6200, 'skin.': 6201, 'called.': 6202, 'easily.': 6203, 'bus,': 6204, 'insult': 6205, 'wire': 6206, 'opposition': 6207, 'instantly': 6208, 'radiohead': 6209, 'insisted': 6210, 'heck,': 6211, 'elaborate': 6212, 'comments,': 6213, 'lance': 6214, 'fantastic.': 6215, 'it!!': 6216, 'christ,': 6217, 'tuned': 6218, 'disaster': 6219, 'ground,': 6220, 'science,': 6221, 'bill.': 6222, 'prayed': 6223, 'distracted': 6224, 'start,': 6225, '2003,': 6226, 'squirrel': 6227, 'accompanied': 6228, 'mail.': 6229, 'guarantee': 6230, 'luck,': 6231, 'chili': 6232, 'background.': 6233, 'demanding': 6234, 'process,': 6235, 'rules.': 6236, 'angela': 6237, 'adopted': 6238, 'spill': 6239, 'gloria': 6240, 'react': 6241, 'share.': 6242, 'keen': 6243, 'liberty': 6244, \"should've\": 6245, 'beatles': 6246, 'offensive': 6247, 'abby': 6248, 'lifted': 6249, 'called,': 6250, '(maybe': 6251, 'repeatedly': 6252, 'euro': 6253, 'wife.': 6254, 'warm,': 6255, 'insight': 6256, 'georgia': 6257, 'segment': 6258, 'flame': 6259, 'focusing': 6260, 'goodnight': 6261, 'check.': 6262, 'bathroom,': 6263, 'weigh': 6264, 'erin,': 6265, 'desk,': 6266, 'percentage': 6267, 'depend': 6268, 'assured': 6269, 'cheese.': 6270, 'candle': 6271, 'demands': 6272, 'address.': 6273, 'dresses': 6274, 'wheels': 6275, 'approval': 6276, 'handy': 6277, 'amused': 6278, 'mild': 6279, 'fear.': 6280, 'nasa': 6281, 'poop': 6282, 'climbed': 6283, 'say...': 6284, 'works,': 6285, 'york.': 6286, 'candles': 6287, 'supports': 6288, 'sentences': 6289, 'ahh,': 6290, 'humanity': 6291, 'legend': 6292, 'sleeping.': 6293, 'nope.': 6294, 'not!': 6295, 'handling': 6296, 'lord.': 6297, 'roller': 6298, '(more': 6299, 'pentagon': 6300, 'stewart': 6301, 'rural': 6302, 'awhile,': 6303, 'kitten': 6304, 'mutual': 6305, 'grammar': 6306, 'series,': 6307, 'stones': 6308, 'paste': 6309, 'criticism': 6310, 'weapon': 6311, 'pairs': 6312, 'instructor': 6313, 'barbara': 6314, 'mount': 6315, 'clean,': 6316, 'bust': 6317, 'fall,': 6318, 'fair,': 6319, 'dvds': 6320, 'feeling:': 6321, 'yep.': 6322, 'shot,': 6323, 'yeay!': 6324, 'depressed.': 6325, 'nope,': 6326, 'died,': 6327, 'here)': 6328, 'marten,': 6329, 'mike,': 6330, 'tunes': 6331, 'perfect,': 6332, '\"i\\'ve': 6333, 'video.': 6334, 'sandy': 6335, 'her...': 6336, 'portrayed': 6337, 'potentially': 6338, 'facial': 6339, 'cancel': 6340, 'fight.': 6341, 'slow,': 6342, 'alice': 6343, 'churches': 6344, 'get,': 6345, 'august.': 6346, 'awful.': 6347, 'white.': 6348, 'burns': 6349, 'overwhelmed': 6350, 'coalition': 6351, 'funds': 6352, 'noodles': 6353, 'venture': 6354, 'soldier': 6355, 'control,': 6356, 'proceed': 6357, 'iran': 6358, 'donna': 6359, 'towers': 6360, 'castle': 6361, 'conspiracy': 6362, 'seminar': 6363, 'briefly': 6364, 'glimpse': 6365, 'enjoys': 6366, 'genuine': 6367, 'cart': 6368, 'tree.': 6369, 'narrow': 6370, 'feet,': 6371, 'sword': 6372, 'embarrassing': 6373, 'order,': 6374, 'mixture': 6375, 'goddess': 6376, 'sometime.': 6377, 'win,': 6378, 'carries': 6379, 'editorial': 6380, 'easier.': 6381, 'duke': 6382, 'lynn': 6383, 'wander': 6384, 'arman': 6385, 'dumb.': 6386, 'kind.': 6387, 'actress': 6388, 'popcorn': 6389, 'action,': 6390, 'mungo': 6391, 'dollars.': 6392, 'fuck,': 6393, 'may.': 6394, 'think)': 6395, 'warmth': 6396, 'wealth': 6397, 'it....': 6398, 'doom': 6399, 'front,': 6400, 'dried': 6401, 'rocky': 6402, 'vent': 6403, 'baking': 6404, 'titled': 6405, 'meeting,': 6406, 'nicki': 6407, '10:30': 6408, 'repair': 6409, 'valentines': 6410, 'clicking': 6411, 'camera,': 6412, 'since.': 6413, 'reduced': 6414, 'posted.': 6415, 'flowing': 6416, 'pedro': 6417, 'event,': 6418, 'dvd.': 6419, 'promise.': 6420, 'e-mails': 6421, 'department.': 6422, 'cotton': 6423, 'boards': 6424, 'rear': 6425, \"day's\": 6426, 'character,': 6427, 'thumb': 6428, 'brick': 6429, 'stack': 6430, 'simpsons': 6431, 'versus': 6432, 'color.': 6433, 'delay': 6434, 'feeling,': 6435, 'week...': 6436, 'tossed': 6437, 'islamic': 6438, 'law,': 6439, 'remotely': 6440, 'mouth,': 6441, 'ordering': 6442, 'school!': 6443, 'board.': 6444, 'purse': 6445, 'educated': 6446, 'front.': 6447, 'track.': 6448, 'boredom': 6449, 'shit!': 6450, 'blood.': 6451, 'creatures': 6452, 'signal': 6453, 'ignorance': 6454, 'memory.': 6455, 'rang': 6456, 'quiet,': 6457, 'radical': 6458, 'proposal': 6459, 'quiet.': 6460, 'teaches': 6461, 'hehehe': 6462, 'critics': 6463, 'box,': 6464, 'attacking': 6465, 'grilled': 6466, 'monkeys': 6467, 'dude.': 6468, 'way!': 6469, 'shop.': 6470, 'entry,': 6471, 'primarily': 6472, 'ball.': 6473, 'agenda': 6474, 'rocking': 6475, '(since': 6476, 'name?': 6477, 'theyre': 6478, 'presidents': 6479, 'uniform': 6480, 'practice,': 6481, 'subject:': 6482, '2004,': 6483, 'polls': 6484, 'came.': 6485, 'therapy': 6486, 'petty': 6487, 'statue': 6488, 'sqee!': 6489, 'cameras': 6490, 'move,': 6491, 'cheated': 6492, 'crown': 6493, 'begging': 6494, 'relieved': 6495, 'exam.': 6496, 'shelter': 6497, 'wonderful,': 6498, 'jerk': 6499, 'practiced': 6500, 'dudes': 6501, 'wow...': 6502, 'title.': 6503, 'cereal': 6504, 'woken': 6505, 'ratings': 6506, 'panel': 6507, 'secondary': 6508, 'marcus': 6509, 'pissed.': 6510, 'travelling': 6511, 'payment': 6512, 'popping': 6513, 'waving': 6514, 'marc': 6515, 'extend': 6516, 'tommy': 6517, 'shares': 6518, 'nung': 6519, 'clay': 6520, 'hotties': 6521, 'kaya': 6522, 'now)': 6523, 'ringing': 6524, 'filed': 6525, 'hahaha.': 6526, 'chopped': 6527, 'politics.': 6528, 'picnic': 6529, 'mailing': 6530, 'tent': 6531, 'rebecca': 6532, 'scroll': 6533, 'girlfriend,': 6534, 'database': 6535, 'rumsfeld': 6536, 'satan': 6537, 'suggestion': 6538, 'category:': 6539, 'slim': 6540, 'spectacular': 6541, 'annie': 6542, 'vocal': 6543, 'mates': 6544, 'dialogue': 6545, 'qualified': 6546, 'acknowledge': 6547, 'chapters': 6548, 'seniors': 6549, 'green,': 6550, 'friendships': 6551, 'perhaps,': 6552, 'approaching': 6553, 'melt': 6554, 'massachusetts': 6555, 'communications': 6556, 'possibilities': 6557, 'etc,': 6558, 'station,': 6559, 'cheese,': 6560, 'fish.': 6561, 'fortunately': 6562, 'rates': 6563, 'culture,': 6564, 'pool,': 6565, 'muslim': 6566, '11th': 6567, 'youngest': 6568, 'quote:': 6569, 'nights.': 6570, 'texas.': 6571, 'aside,': 6572, 'announcement': 6573, 'backing': 6574, 'acoustic': 6575, 'yay,': 6576, 'residents': 6577, 'upset.': 6578, 'arranged': 6579, 'browsing': 6580, 'madison': 6581, 'snack': 6582, 'amusing.': 6583, 'arrived.': 6584, 'plan,': 6585, 'bullet': 6586, 'employment': 6587, 'seeds': 6588, 'racial': 6589, 'walk,': 6590, 'insert': 6591, 'sarah,': 6592, 'supporters': 6593, 'practices': 6594, 'um...': 6595, 'ponder': 6596, 'university.': 6597, 'would,': 6598, 'counted': 6599, 'witch': 6600, 'neglected': 6601, 'breasts': 6602, 'everywhere,': 6603, 'matt,': 6604, 'yours.': 6605, 'those.': 6606, 'cancer.': 6607, 'cnn.com': 6608, 'bill,': 6609, 'being,': 6610, 'obscure': 6611, 'radio.': 6612, 'nicer': 6613, 'resting': 6614, 'wiped': 6615, 'counts': 6616, 'holiday.': 6617, 'half,': 6618, 'beings': 6619, 'pass.': 6620, 'christopher': 6621, 'buttons': 6622, 'ranting': 6623, 'venue': 6624, 'polite': 6625, 'cars.': 6626, 'ankle': 6627, 'inevitable': 6628, 'longing': 6629, 'diana': 6630, 'adore': 6631, 'lungs': 6632, 'litter': 6633, 'things:': 6634, \"everybody's\": 6635, 'triple': 6636, 'blogs,': 6637, 'madonna': 6638, 'divided': 6639, 'freedom.': 6640, 'slipping': 6641, 'difficulty': 6642, 'repeated': 6643, 'aforementioned': 6644, 'household': 6645, 'perth': 6646, 'nervous.': 6647, 'registration': 6648, 'pacific': 6649, 'sized': 6650, 'certificate': 6651, 'doug': 6652, 'mad,': 6653, 'intro': 6654, 'suffice': 6655, 'corner.': 6656, '\"where': 6657, 'denied': 6658, 'affair': 6659, 'float': 6660, 'yum.': 6661, 'intimate': 6662, 'replied,': 6663, 'doin': 6664, 'symptoms': 6665, 'hear.': 6666, 'maximum': 6667, 'topic.': 6668, 'refreshing': 6669, 'chin': 6670, 'issued': 6671, 'tomato': 6672, 'invention': 6673, 'shelf': 6674, 'limits': 6675, 'style,': 6676, 'divorce': 6677, 'fat.': 6678, 'present,': 6679, 'stereo': 6680, 'he/she': 6681, 'into.': 6682, 'back...': 6683, 'wayne': 6684, 'stern': 6685, \"president's\": 6686, 'bcuz': 6687, 'revenge': 6688, 'orlando': 6689, 'tyler': 6690, 'eleven': 6691, 'stadium': 6692, 'agreement': 6693, 'people...': 6694, 'happiness.': 6695, 'act.': 6696, 'attack.': 6697, '\"she': 6698, 'replacement': 6699, 'cabin': 6700, 'dose': 6701, 'now:': 6702, 'generous': 6703, 'sorted': 6704, 'nonetheless.': 6705, 'seasons': 6706, 'disappear': 6707, 'safe.': 6708, 'magazines': 6709, 'xbox': 6710, 'inability': 6711, 'span': 6712, 'number,': 6713, 'avoided': 6714, '-the': 6715, 'loss.': 6716, 'cheering': 6717, 'stable': 6718, 'changes.': 6719, 'spite': 6720, 'photos.': 6721, 'things...': 6722, 'housing': 6723, 'preview': 6724, 'organize': 6725, 'drums': 6726, 'linda': 6727, 'planet.': 6728, 'deepest': 6729, 'nancy': 6730, 'evolution': 6731, 'headache.': 6732, '4:30': 6733, 'lotsa': 6734, 'audience.': 6735, 'sky,': 6736, 'flames': 6737, 'arse': 6738, 'watch,': 6739, 'banner': 6740, 'tiring': 6741, 'row.': 6742, 'bride': 6743, 'tapes': 6744, 'alive,': 6745, 'tummy': 6746, 'mercy': 6747, 'glow': 6748, 'zealand': 6749, 'bummed': 6750, 'kitchen.': 6751, 'racist': 6752, 'eating,': 6753, 'weekend?': 6754, 'language,': 6755, 'nerd': 6756, 'tops': 6757, 'make.': 6758, 'they?': 6759, 'sympathy': 6760, 'arrival': 6761, 'inviting': 6762, 'secondly,': 6763, 'world?': 6764, 'nose.': 6765, 'recovery': 6766, 'yard.': 6767, 'rapidly': 6768, 'vague': 6769, 'seen,': 6770, 'birthday!': 6771, 'remembers': 6772, 'may,': 6773, 'affairs': 6774, 'backwards': 6775, 'terrible.': 6776, '(yeah,': 6777, 'better!': 6778, 'decade': 6779, 'warming': 6780, 'i.e.': 6781, 'designer': 6782, 'stunning': 6783, 'firmly': 6784, 'nevertheless,': 6785, 'worker': 6786, 'reserve': 6787, 'collected': 6788, 'modest': 6789, 'calls.': 6790, 'refuses': 6791, 'tearing': 6792, 'religion,': 6793, 'prospect': 6794, 'backup': 6795, 'trace': 6796, 'three.': 6797, 'went,': 6798, 'regime': 6799, 'slave': 6800, \"men's\": 6801, 'could,': 6802, 'vegetable': 6803, 'loudly': 6804, 'healing': 6805, 'representative': 6806, 'amazon': 6807, 'brandon': 6808, 'chunk': 6809, 'environment.': 6810, 'fundamental': 6811, 'flights': 6812, 'reaches': 6813, 'knocking': 6814, 'seth': 6815, 'soul,': 6816, 'mistake.': 6817, 'roots': 6818, 'consists': 6819, \"wendy's\": 6820, 'gay,': 6821, 'flags': 6822, 'performance.': 6823, 'dang': 6824, 'erase': 6825, 'outdoor': 6826, 'flipped': 6827, 'way?': 6828, 'interested.': 6829, 'fist': 6830, 'lovers': 6831, 'legally': 6832, 'assembly': 6833, '(about': 6834, 'coughing': 6835, 'view,': 6836, 'sticker': 6837, 'rate,': 6838, 'sidewalk': 6839, 'ruled': 6840, 'doll': 6841, 'pizza.': 6842, 'approved': 6843, 'backs': 6844, 'injured': 6845, 'talkin': 6846, 'act,': 6847, 'louder': 6848, 'available.': 6849, 'sums': 6850, 'thou': 6851, 'bumped': 6852, 'lindsay': 6853, 'hers': 6854, 'washington,': 6855, \"moore's\": 6856, 'errands': 6857, 'grave': 6858, 't-shirts': 6859, 'blink': 6860, 'kami': 6861, 'managing': 6862, 'perception': 6863, 'mucho': 6864, 'nursing': 6865, 'genre': 6866, 'expand': 6867, 'ingredients': 6868, 'figure.': 6869, \"anyone's\": 6870, 'bowl.': 6871, 'offshore': 6872, 'pause': 6873, 'simpson': 6874, 'clint': 6875, 'insanely': 6876, 'whipped': 6877, 'attorney': 6878, 'thanx': 6879, 'stomach.': 6880, 'leap': 6881, 'halo': 6882, 'creature': 6883, 'hardware': 6884, 'soaked': 6885, 'julia': 6886, 'enjoy!': 6887, 'barnes': 6888, 'teenager': 6889, 'came,': 6890, 'example.': 6891, 'tuition': 6892, 'tea.': 6893, 'fascinated': 6894, 'waitress': 6895, '(sp?)': 6896, 'none.': 6897, 'addresses': 6898, 'sacred': 6899, 'loses': 6900, 'thread': 6901, 'sophia': 6902, 'really?': 6903, 'empty.': 6904, 'mmm...': 6905, 'level,': 6906, 'compete': 6907, 'valid': 6908, 'watches': 6909, 'refrain': 6910, 'boo,': 6911, '(how': 6912, 'badly.': 6913, 'dance,': 6914, 'shore': 6915, \"nation's\": 6916, 'trivial': 6917, 'admin': 6918, 'photographs': 6919, 'troy': 6920, 'associate': 6921, 'date:': 6922, 'assure': 6923, 'has.': 6924, 'liquor': 6925, 'coming,': 6926, 'after,': 6927, 'authors': 6928, 'gwen': 6929, '\"i\\'ll': 6930, 'dependent': 6931, 'grip': 6932, 'tremendous': 6933, 'pains': 6934, 'excellent.': 6935, 'overnight': 6936, 'kerry,': 6937, 'leaf': 6938, 'intentions': 6939, 'troubles': 6940, 'unsure': 6941, 'hello.': 6942, 'lotr': 6943, 'shop,': 6944, 'sight.': 6945, 'pepsi': 6946, 'in...': 6947, 'addiction': 6948, 'son.': 6949, 'quizzes': 6950, 'any.': 6951, 'backyard': 6952, 'treats': 6953, 'contemporary': 6954, 'strive': 6955, 'albeit': 6956, 'describing': 6957, 'appreciation': 6958, 'eyed': 6959, 'hotel.': 6960, 'culture.': 6961, 'realistic': 6962, 'excuses': 6963, 'worry.': 6964, 'thoughtful': 6965, 'hunger': 6966, 'memorable': 6967, 'freaks': 6968, 'glance': 6969, 'constitutional': 6970, 'drinks.': 6971, 'promising': 6972, 'supplies': 6973, 'angry,': 6974, 'journal.': 6975, 'speech.': 6976, 'beast': 6977, '...i': 6978, 'couch.': 6979, 'librarians': 6980, 'fahrenheit': 6981, 'energy.': 6982, 'drugs,': 6983, 'off...': 6984, 'overseas': 6985, 'lasts': 6986, 'orientation': 6987, 'filter': 6988, 'promoting': 6989, 'tasks': 6990, 'cage': 6991, 'walmart': 6992, 'result,': 6993, 'jenn': 6994, 'yarn': 6995, 'phantom': 6996, 'arrange': 6997, 'arab': 6998, 'cute!': 6999, 'content.': 7000, 'messy': 7001, 'aching': 7002, 'liza': 7003, 'depressing.': 7004, 'fires': 7005, 'italy': 7006, 'etc.)': 7007, 'asleep,': 7008, 'fewer': 7009, 'however.': 7010, 'wonderfully': 7011, 'player.': 7012, 'land.': 7013, 'charity': 7014, 'sober': 7015, 'concert,': 7016, 'mock': 7017, 'exhibit': 7018, 'principles': 7019, 'sessions': 7020, 'password': 7021, 'nice!': 7022, 'colours': 7023, 'then?': 7024, 'spoiled': 7025, 'time:': 7026, 'honors': 7027, 'older,': 7028, 'got.': 7029, 'strong.': 7030, 'engagement': 7031, 'center,': 7032, 'posters': 7033, 'pregnancy': 7034, 'spain': 7035, 'radio,': 7036, '$100': 7037, 'grade,': 7038, 'tickets.': 7039, \"rick's\": 7040, 'cards.': 7041, 'thankfully,': 7042, 'numb': 7043, 'alternate': 7044, 'creativity': 7045, 'hehehe.': 7046, 'reserved': 7047, '\"yes,': 7048, 'from:': 7049, 'heroes': 7050, 'species': 7051, 'buck': 7052, 'cave': 7053, 'airport.': 7054, 'broad': 7055, 'credits': 7056, 'tonite': 7057, 'touch.': 7058, 'relation': 7059, 'explain.': 7060, 'lame.': 7061, 'seed': 7062, 'driveway': 7063, 'wall,': 7064, 'honesty': 7065, 'sharon': 7066, 'clicked': 7067, 'stating': 7068, 'disappeared': 7069, 'slow.': 7070, 'darling': 7071, 'drain': 7072, 'kind,': 7073, 'texas,': 7074, 'nicht': 7075, 'throat.': 7076, 'resolve': 7077, 'dozens': 7078, 'memories.': 7079, 'heard.': 7080, 'skating': 7081, 'india.': 7082, 'rants': 7083, 'met.': 7084, 'resident': 7085, 'cracks': 7086, 'punishment': 7087, 'living.': 7088, 'reminding': 7089, 'feelings,': 7090, 'progress.': 7091, 'progressive': 7092, 'sticky': 7093, 'her:': 7094, 'tomorrow...': 7095, 'television.': 7096, 'doubts': 7097, 'investment': 7098, 'take.': 7099, 'contents': 7100, 'manages': 7101, 'external': 7102, 'wondering,': 7103, 'loop': 7104, 'poorly': 7105, 'silence.': 7106, 'rocket': 7107, 'fault,': 7108, 'invasion': 7109, 'ciao': 7110, 'yet...': 7111, 'technology.': 7112, 'screwing': 7113, 'powder': 7114, 'thing:': 7115, 'cards,': 7116, 'chance,': 7117, 'elephant': 7118, 'beginning.': 7119, 'back?': 7120, 'waiting.': 7121, 'monica': 7122, 'collecting': 7123, 'conducted': 7124, 'haha...': 7125, 'nephew': 7126, 'it?\"': 7127, 'be...': 7128, 'too?': 7129, 'normal,': 7130, 'quickly,': 7131, 'definite': 7132, 'logo': 7133, 'greeted': 7134, 'engineer': 7135, 'interested,': 7136, 'away!': 7137, 'fathers': 7138, 'sons': 7139, 'fellowship': 7140, 'hats': 7141, 'over!': 7142, 'biological': 7143, 'them)': 7144, 'names,': 7145, 'politically': 7146, 'stripes': 7147, 'pressing': 7148, 'events,': 7149, 'crowd.': 7150, 'survivor': 7151, 'cried.': 7152, 'trading': 7153, 'drugs.': 7154, 'find.': 7155, 'relationships.': 7156, 'decline': 7157, 'officers': 7158, 'wrist': 7159, 'world!': 7160, 'izzy': 7161, 'jury': 7162, 'nominated': 7163, 'sherry': 7164, 'combine': 7165, 'basket': 7166, 'worked.': 7167, 'note.': 7168, 'spreading': 7169, 'feb.': 7170, 'manhattan': 7171, 'organizations': 7172, 'attract': 7173, 'good?': 7174, 'revision': 7175, 'school...': 7176, 'man...': 7177, 'epic': 7178, 'wanted,': 7179, 'starving': 7180, 'wonder,': 7181, 'excited!': 7182, 'rumors': 7183, 'butterfly': 7184, 'unlikely': 7185, 'mark,': 7186, '[resource-type:': 7187, 'knows?': 7188, 'strict': 7189, 'audrey': 7190, 'truely': 7191, 'pretty,': 7192, 'exhausted.': 7193, 'crawling': 7194, 'cynical': 7195, 'weighed': 7196, 'orleans': 7197, 'entertaining.': 7198, 'expressing': 7199, 'sole': 7200, 'baghdad': 7201, 'race,': 7202, 'foster': 7203, 'crushed': 7204, 'kitchen,': 7205, 'philosophical': 7206, 'stats': 7207, 'year...': 7208, 'bucks.': 7209, 'vital': 7210, 'improvement': 7211, 'condo': 7212, 'canada.': 7213, 'circuit': 7214, 'eye,': 7215, 'principal': 7216, 'echo': 7217, 'awake.': 7218, 'gross.': 7219, 'gals': 7220, 'solutions': 7221, 'agree.': 7222, 'pondering': 7223, 'stupidity': 7224, 'habe': 7225, 'respective': 7226, 'pilot': 7227, 'hong': 7228, 'woo!': 7229, 'steph': 7230, 'disk': 7231, 'psychological': 7232, 'hail': 7233, 'maintenance': 7234, 'job!': 7235, 'influenced': 7236, 'points,': 7237, 'bumper': 7238, 'ring.': 7239, 'drummer': 7240, 'resulting': 7241, 'poetry.': 7242, 'husband,': 7243, 'outcome': 7244, 'homosexual': 7245, 'requested': 7246, 'vaguely': 7247, 'duties': 7248, 'bad!': 7249, 'ache': 7250, 'bump': 7251, 'awareness': 7252, 'nerves': 7253, 'gimme': 7254, 'shove': 7255, 'london,': 7256, 'seat.': 7257, 'skipping': 7258, 'dan,': 7259, 'sorrow': 7260, 'wizard': 7261, 'nightmares': 7262, 'bounce': 7263, 'occupied': 7264, 'christine': 7265, 'alot.': 7266, 'waters': 7267, 'peak': 7268, 'joke,': 7269, 'single,': 7270, 'seventh': 7271, 'despise': 7272, 'drama.': 7273, 'anticipation': 7274, 'caffeine': 7275, 'acid': 7276, 'use,': 7277, 'reasonably': 7278, 'hungry,': 7279, 'openly': 7280, 'nemo': 7281, 'winter.': 7282, 'odd,': 7283, 'columbia': 7284, 'sought': 7285, 'three,': 7286, 'found.': 7287, 'reward': 7288, 'me!!': 7289, 'universe.': 7290, 'jean': 7291, 'afghanistan': 7292, 'resulted': 7293, 'gigantic': 7294, 'ugh,': 7295, 'forget.': 7296, 'foolish': 7297, 'graham': 7298, 'pursuit': 7299, 'message,': 7300, 'sweaty': 7301, 'chased': 7302, 'moving.': 7303, 'rational': 7304, 'toll': 7305, 'piece.': 7306, 'hill,': 7307, 'scene,': 7308, 'friendster': 7309, 'awaiting': 7310, 'contrast': 7311, 'spice': 7312, 'soon...': 7313, 'hugged': 7314, 'solely': 7315, 'enemies': 7316, 'clear.': 7317, 'links.': 7318, 'weed': 7319, 'occasion': 7320, 'me....': 7321, 'spilled': 7322, 'challenged': 7323, 'posts,': 7324, '\"we\\'re': 7325, 'substance': 7326, 'shuttle': 7327, 'form,': 7328, 'yards': 7329, 'fools': 7330, 'sections': 7331, 'november.': 7332, 'handsome': 7333, 'consciousness': 7334, 'morning!': 7335, 'bites': 7336, 'bitchy': 7337, 'proving': 7338, 'sarcastic': 7339, 'closed.': 7340, 'dammit.': 7341, 'magazine.': 7342, 'study,': 7343, 'curl': 7344, 'improving': 7345, 'spoon': 7346, 'debut': 7347, 'easiest': 7348, 'spirits': 7349, 'honest.': 7350, 'satisfy': 7351, 'away...': 7352, 'sweetest': 7353, 'prescription': 7354, 'necessary.': 7355, 'pumpkin': 7356, 'refers': 7357, 'goods': 7358, 'listen.': 7359, 'estate': 7360, 'formula': 7361, 'pose': 7362, 'example:': 7363, '(there': 7364, 'purchasing': 7365, 'recap': 7366, 'report,': 7367, 'beta': 7368, 'victims': 7369, 'record.': 7370, 'me).': 7371, 'heated': 7372, 'tour.': 7373, 'chest.': 7374, 'top,': 7375, 'matters.': 7376, 'eachother': 7377, 'transition': 7378, 'corner,': 7379, 'responses': 7380, 'lol)': 7381, 'championship': 7382, 'effort.': 7383, 'arms.': 7384, 'packet': 7385, 'preferred': 7386, 'sells': 7387, 'observation': 7388, 'kings': 7389, \"tonight's\": 7390, 'speech,': 7391, 'reputation': 7392, 'factors': 7393, 'dive': 7394, 'footage': 7395, 'animation': 7396, 'casting': 7397, 'spicy': 7398, 'elderly': 7399, 'invest': 7400, 'suitable': 7401, 'masses': 7402, 'inspire': 7403, 'languages': 7404, 'clients': 7405, 'energy,': 7406, 'boom': 7407, 'misery': 7408, 'his.': 7409, 'sites.': 7410, 'dell': 7411, 'it!!!': 7412, 'profit': 7413, 'niya': 7414, 'towel': 7415, 'section,': 7416, 'terrified': 7417, 'notebook': 7418, 'wedding,': 7419, 'yearbook': 7420, 'too..': 7421, 'semester,': 7422, 'nature,': 7423, 'organic': 7424, 'naturally,': 7425, 'essence': 7426, 'filming': 7427, 'participating': 7428, 'everybody.': 7429, 'ass!': 7430, 'gaining': 7431, 'stance': 7432, 'buffy': 7433, 'mall,': 7434, 'karaoke': 7435, 'twice,': 7436, 'properly.': 7437, 'ticket.': 7438, 'player,': 7439, 'finale': 7440, 'disappointed.': 7441, 'gym,': 7442, 'vicious': 7443, 'personality.': 7444, 'balanced': 7445, 'machine,': 7446, 'siya': 7447, 'hey.': 7448, 'psycho': 7449, 'boost': 7450, 'gays': 7451, 'partners': 7452, 'posting.': 7453, 'wacky': 7454, 'skies': 7455, 'tender': 7456, 'waiter': 7457, 'hopped': 7458, 'spends': 7459, 'screams': 7460, 'broken.': 7461, 'concerts': 7462, 'smash': 7463, 'notice.': 7464, 'arthur': 7465, 'teddy': 7466, 'ages.': 7467, 'translate': 7468, 'away?': 7469, 'edward': 7470, 'claire': 7471, 'needs.': 7472, 'kent': 7473, 'differently': 7474, 'dena': 7475, 'sweating': 7476, 'arms,': 7477, 'draws': 7478, 'eagle': 7479, 'dolls': 7480, 'medication': 7481, 'networks': 7482, 'bread,': 7483, 'setup': 7484, 'wage': 7485, 'teeth.': 7486, 'existence.': 7487, 'need,': 7488, 'swallow': 7489, 'fave': 7490, '(now': 7491, 'noticing': 7492, 'morning...': 7493, 'complaint': 7494, 'remarkable': 7495, 'knit': 7496, 'luck!': 7497, 'legs,': 7498, 'nicest': 7499, 'meow!': 7500, 'daughter,': 7501, 'make-up': 7502, 'occurs': 7503, 'smarter': 7504, 'todays': 7505, 'rated': 7506, 'lifting': 7507, 'drown': 7508, 'try,': 7509, 'kindly': 7510, 'from?': 7511, 'ongoing': 7512, 'passenger': 7513, 'tension': 7514, 'christianity': 7515, 'library,': 7516, 'green.': 7517, 'cream,': 7518, 'creek': 7519, 'protein': 7520, 'tom,': 7521, 'pages.': 7522, 'photograph': 7523, 'grin': 7524, 'prisoners': 7525, 'cartoons': 7526, 'him:': 7527, 'shouted': 7528, 'bouncing': 7529, 'stay.': 7530, '1:30': 7531, 'investigation': 7532, 'clubs': 7533, 'marriage,': 7534, 'goofy': 7535, 'cans': 7536, 'today..': 7537, 'with?': 7538, 'yup.': 7539, 'thanks!': 7540, 'murray': 7541, 'learn.': 7542, 'chess': 7543, 'breakdown': 7544, 'calories': 7545, 'hitler': 7546, 'tourist': 7547, 'ducks': 7548, 'serial': 7549, 'crying,': 7550, 'way)': 7551, 'planted': 7552, 'paperwork': 7553, 'declare': 7554, 'mercury': 7555, 'teens': 7556, 'summary': 7557, 'assessment': 7558, 'wired': 7559, 'wanda': 7560, 'switching': 7561, 'hall,': 7562, 'insist': 7563, 'scary,': 7564, 't.v.': 7565, 'changed,': 7566, 'offend': 7567, 'consumer': 7568, 'pencil': 7569, 'reverse': 7570, 'sign.': 7571, 'assignments': 7572, 'nation.': 7573, 'viewers': 7574, 'somehow.': 7575, 'phrases': 7576, 'offer.': 7577, 'conduct': 7578, 'he...': 7579, 'warn': 7580, 'legs.': 7581, 'colleges': 7582, 'raises': 7583, 'xrating:': 7584, 'handing': 7585, 'hometown': 7586, 'wrong?': 7587, 'partying': 7588, 'whine': 7589, '(some': 7590, 'missing.': 7591, 'pipe': 7592, 'irritating': 7593, 'minnesota': 7594, \"grandma's\": 7595, 'restaurants': 7596, 'tragedy': 7597, 'strings': 7598, 'house!': 7599, 'everyday,': 7600, 'friendship.': 7601, 'whisper': 7602, 'dogs.': 7603, 'dessert': 7604, 'cranky': 7605, 'know)': 7606, 'large,': 7607, 'tink': 7608, 'lent': 7609, 'when,': 7610, 'snow,': 7611, 'ready,': 7612, 'meanwhile': 7613, 'beck': 7614, 'nikon': 7615, 'creator': 7616, 'chicken.': 7617, 'cars,': 7618, 'tree,': 7619, 'wolf': 7620, 'holidays.': 7621, 'taste.': 7622, 'dragging': 7623, 'fabric': 7624, 'eventually.': 7625, 'flick': 7626, 'sexually': 7627, 'striking': 7628, 'details,': 7629, 'archive': 7630, 'tabby': 7631, 'notes,': 7632, 'king,': 7633, 'endure': 7634, 'meal.': 7635, 'homemade': 7636, 'trains': 7637, '18th': 7638, 'wounds': 7639, 'establish': 7640, 'chewing': 7641, 'membership': 7642, 'weights': 7643, 'cutest': 7644, 'manual': 7645, 'font': 7646, 'smelling': 7647, 'driving.': 7648, 'moments.': 7649, 'competitive': 7650, 'orchestra': 7651, 'extensive': 7652, 'consistently': 7653, 'applies': 7654, 'school?': 7655, 'farewell': 7656, 'parallel': 7657, 'prisoner': 7658, 'polish': 7659, 'memegen': 7660, 'shoved': 7661, 'niece': 7662, 'precisely': 7663, 'intent': 7664, 'mcdonalds': 7665, 'incredible.': 7666, 'lincoln': 7667, 'revealing': 7668, 'joy,': 7669, 'chicken,': 7670, 'instance': 7671, 'bitch,': 7672, 'diverse': 7673, 'cara-': 7674, 'again..': 7675, 'cope': 7676, 'clinic': 7677, 'sit.': 7678, 'citizen': 7679, 'camp,': 7680, 'confirm': 7681, 'blah!...': 7682, 'pimp': 7683, 'expected,': 7684, 'red.': 7685, 'infinite': 7686, 'blend': 7687, 'sucky': 7688, 'dads': 7689, 'saint': 7690, 'contemplating': 7691, 'blood,': 7692, 'hehe...': 7693, 'anyways...': 7694, 'about...': 7695, 'imaginary': 7696, 'down!': 7697, 'salvation': 7698, 'numbers.': 7699, 'unfair': 7700, 'impressed.': 7701, 'diet.': 7702, 'following:': 7703, 'running,': 7704, 'alcoholic': 7705, 'morgan': 7706, 'matthews': 7707, 'interaction': 7708, 'predict': 7709, 'rabbit': 7710, 'groups.': 7711, 'ruling': 7712, 'frog': 7713, 'market.': 7714, 'emphasis': 7715, 'self,': 7716, 'masters': 7717, 'suggesting': 7718, 'satisfaction': 7719, 'cult': 7720, 'novels': 7721, 'michael,': 7722, 'finance': 7723, 'jeans,': 7724, 'cindy': 7725, 'heels': 7726, 'me!!!': 7727, 'looking.': 7728, 'rank': 7729, 'hottest': 7730, 'leaving.': 7731, 'course)': 7732, 'manga': 7733, 'optimistic': 7734, 'spark': 7735, 'colleagues': 7736, 'restaurant.': 7737, 'shud': 7738, 'dave,': 7739, 'lesser': 7740, 'homecoming': 7741, \"friggin'\": 7742, 'seperate': 7743, 'errors': 7744, 'emma': 7745, 'young.': 7746, 'roles': 7747, 'studying.': 7748, 'banks': 7749, 'icon': 7750, 'out\"': 7751, 'buys': 7752, 'unto': 7753, 'vacation,': 7754, 'night:': 7755, 'girlfriends': 7756, 'reliable': 7757, 'charm': 7758, 'discount': 7759, 'lust': 7760, 'deadline': 7761, 'iced': 7762, 'concerned.': 7763, \"sarah's\": 7764, \"baby's\": 7765, 'costumes': 7766, 'planes': 7767, 'pm]:': 7768, 'bloom': 7769, 'ball,': 7770, 'huh.': 7771, 'delayed': 7772, 'shifts': 7773, 'classmates': 7774, 'review.': 7775, 'stumble': 7776, 'ahead,': 7777, 'cheat': 7778, '19th': 7779, 'noone': 7780, 'parts.': 7781, 'infection': 7782, 'bleed': 7783, 'wreck': 7784, 'california,': 7785, 'industrial': 7786, 'deadly': 7787, 'tara': 7788, 'buffalo': 7789, 'heard,': 7790, 'lonely.': 7791, 'tunnel': 7792, 'laundry.': 7793, 'broadband': 7794, 'vain': 7795, 'protected': 7796, 'recovering': 7797, 'troubled': 7798, 'carrie': 7799, 'landing': 7800, 'fails': 7801, 'no-one': 7802, 'low.': 7803, 'opted': 7804, '(probably': 7805, 'angle': 7806, 'nickname': 7807, 'extension': 7808, 'more?': 7809, 'sound,': 7810, 'aged': 7811, 'visible': 7812, 'exciting,': 7813, 'competition.': 7814, 'clown': 7815, 'privacy': 7816, 'confirmation': 7817, 'time\"': 7818, 'repeating': 7819, 'ninja': 7820, 'bag,': 7821, 'floors': 7822, 'thanked': 7823, 'cool...': 7824, 'l.a.': 7825, 'many.': 7826, 'asia': 7827, 'directory': 7828, 'depressed,': 7829, 'position,': 7830, 'parties.': 7831, 'naomi': 7832, 'smashed': 7833, 'displayed': 7834, 'indy': 7835, 'fitness': 7836, 'river.': 7837, 'most,': 7838, 'stages': 7839, 'operations': 7840, 'eating.': 7841, 'strictly': 7842, 'less,': 7843, 'remarks': 7844, 'poking': 7845, 'shipping': 7846, 'shrimp': 7847, 'lime': 7848, 'experts': 7849, \"cousin's\": 7850, 'businesses': 7851, 'colorado': 7852, 'ears.': 7853, 'yesterday!': 7854, 'explorer': 7855, 'travis': 7856, 'perspective.': 7857, 'mister': 7858, 'gibson': 7859, 'grounded': 7860, 'myself...': 7861, 'widely': 7862, 'worried.': 7863, 'injury': 7864, 'attraction': 7865, 'expensive.': 7866, 'slammed': 7867, 'affects': 7868, 'russia': 7869, 'stress.': 7870, 'goal:': 7871, 'verbal': 7872, 'something!': 7873, 'angie': 7874, 'ripping': 7875, 'recognition': 7876, 'retail': 7877, 'humor.': 7878, 'submitted': 7879, 'observe': 7880, 'containing': 7881, 'promotion': 7882, 'offices': 7883, 'interact': 7884, 'training.': 7885, 'listens': 7886, 'inspiring': 7887, 'paths': 7888, 'weight,': 7889, 'penalty': 7890, 'guitar.': 7891, 'daughters': 7892, 'rhythm': 7893, 'annoys': 7894, 'threats': 7895, 'annoying,': 7896, 'hotel,': 7897, 'sung': 7898, 'aggressive': 7899, 'doubt,': 7900, 'derek': 7901, 'barbie': 7902, 'escaped': 7903, 'biting': 7904, 'puzzle': 7905, 'hilton': 7906, 'knowledge,': 7907, 'molly': 7908, 'winding': 7909, 'goodbye.': 7910, 'address,': 7911, 'lookin': 7912, 'farther': 7913, 'isang': 7914, 'actively': 7915, 'day\"': 7916, 'pole': 7917, 'libraries': 7918, 'continually': 7919, 'worthwhile': 7920, 'aimed': 7921, 'katie,': 7922, 'born.': 7923, 'outer': 7924, 'round.': 7925, 'cash.': 7926, 'subject,': 7927, 'cheating': 7928, 'hill.': 7929, 'lick': 7930, 'schools.': 7931, 'survival': 7932, 'only.': 7933, 'pay.': 7934, 'producers': 7935, 'achievement': 7936, '(unless': 7937, '\"did': 7938, 'projects.': 7939, 'lest': 7940, 'balcony': 7941, 'divaindisguise14:': 7942, 'traveled': 7943, 'translated': 7944, 'rights,': 7945, 'above,': 7946, 'awfully': 7947, 'kidding,': 7948, 'drill': 7949, 'feminist': 7950, 'popularity': 7951, 'rich,': 7952, 'outlet': 7953, 'bookstore': 7954, \"lil'\": 7955, 'dark.': 7956, 'olds': 7957, 'dashboard': 7958, 'media.': 7959, 'dread': 7960, 'concepts': 7961, 'sonic': 7962, 'meaningless': 7963, 'board,': 7964, \"goin'\": 7965, 'heat.': 7966, 'dreaded': 7967, 'lands': 7968, 'june.': 7969, 'flavor': 7970, 'characters.': 7971, 'scenario': 7972, 'champion': 7973, 'deaths': 7974, 'bash': 7975, 'pounding': 7976, 'dork': 7977, 'windy': 7978, 'belongs': 7979, 'stroke': 7980, 'jumps': 7981, 'producing': 7982, 'webpage': 7983, 'nude': 7984, 'behalf': 7985, 'maggie': 7986, 'rights.': 7987, 'dash': 7988, '-arv': 7989, 'distinct': 7990, 'neighborhood.': 7991, 'enterprise': 7992, 'werent': 7993, 'proud.': 7994, 'humorous': 7995, 'obnoxious': 7996, 'mark.': 7997, 'acquired': 7998, 'newspapers': 7999, 'aliens': 8000, 'craft': 8001, 'thee': 8002, 'void': 8003, 'colors.': 8004, 'monsters': 8005, 'freeze': 8006, 'mich': 8007, 'faded': 8008, 'ride,': 8009, 'sucker': 8010, 'trees,': 8011, 'worthless': 8012, 'boobs': 8013, 'spirit.': 8014, 'happiest': 8015, 'abstract': 8016, 'alter': 8017, 'slot': 8018, 'compromise': 8019, 'next?': 8020, 'controlled': 8021, 'poverty': 8022, 'breath.': 8023, 'hawaii': 8024, 'consistent': 8025, '(thank': 8026, 'full.': 8027, 'hasnt': 8028, 'demo': 8029, 'envy': 8030, \"company's\": 8031, 'tommorow': 8032, 'rivers': 8033, 'ooh,': 8034, 'tamil': 8035, 'lend': 8036, 'mindless': 8037, 'unemployment': 8038, 'regional': 8039, 'objective': 8040, 'name)': 8041, 'scientist': 8042, 'dying.': 8043, 'stopped.': 8044, 'vol.': 8045, 'straight,': 8046, 'irritated': 8047, 'animated': 8048, 'patriot': 8049, 'match.': 8050, 'principle': 8051, 'goal.': 8052, 'excerpt': 8053, \"country's\": 8054, 'mice': 8055, 'sexy.': 8056, 'hangs': 8057, '\"can': 8058, 'scarf': 8059, 'becky': 8060, 'chairman': 8061, 'drowning': 8062, 'homer': 8063, 'hopeless': 8064, 'logged': 8065, 'even.': 8066, 'shades': 8067, 'jelly': 8068, 'done!': 8069, 'friday!': 8070, 'powered': 8071, 'already!': 8072, 'trees.': 8073, 'judges': 8074, 'showers': 8075, 'friends...': 8076, 'disappointing': 8077, 'gamespot': 8078, 'french,': 8079, 'faith,': 8080, 'court.': 8081, 'akong': 8082, 'how,': 8083, 'welfare': 8084, 'delight': 8085, 'steam': 8086, 'bombs': 8087, 'witnessed': 8088, 'ryan,': 8089, 'immediately.': 8090, 'layers': 8091, 'exposure': 8092, 'drinks,': 8093, 'terrific': 8094, 'sen.': 8095, 'lobby': 8096, 'hike': 8097, 'toby': 8098, 'britain': 8099, 'happen?': 8100, 'told,': 8101, 'services.': 8102, 'reform': 8103, 'rugby': 8104, 'hate.': 8105, 'traffic.': 8106, 'dress.': 8107, 'train.': 8108, 'round,': 8109, 'brooklyn': 8110, 'has,': 8111, 'fish,': 8112, 'institution': 8113, 'depths': 8114, 'midnight.': 8115, 'finals.': 8116, 'chair,': 8117, 'luxury': 8118, 'september.': 8119, 'answers.': 8120, 'have?': 8121, \"won't.\": 8122, 'disappointment': 8123, 'meet.': 8124, 'poor,': 8125, 'speeding': 8126, 'recover': 8127, 'suck,': 8128, 'addressed': 8129, 'carlos': 8130, 'interest.': 8131, '11:30': 8132, 'permit': 8133, '13th': 8134, 'dwell': 8135, 'defending': 8136, 'goodness.': 8137, 'enthusiasm': 8138, 'with:': 8139, 'rumor': 8140, 'lesson.': 8141, 'broadway': 8142, 'ahhh': 8143, 'needle': 8144, 'represented': 8145, 'husband.': 8146, 'fight,': 8147, 'massage': 8148, 'whatnot.': 8149, 'requests': 8150, 'havnt': 8151, 'destiny': 8152, 'holly': 8153, 'march.': 8154, 'etc.,': 8155, 'moon.': 8156, 'nah,': 8157, 'sale.': 8158, 'weirdest': 8159, 'week?': 8160, 'electrical': 8161, 'impress': 8162, 'esta': 8163, 'curled': 8164, 'historic': 8165, 'kudos': 8166, 'index': 8167, 'handled': 8168, 'allergic': 8169, 'lucky.': 8170, 'web.': 8171, 'planned.': 8172, 'shape.': 8173, 'protecting': 8174, 'contrary': 8175, 'addition,': 8176, 'surprisingly,': 8177, 'cries': 8178, 'flood': 8179, 'only,': 8180, 'deep,': 8181, 'controversial': 8182, 'disturbed': 8183, 'identified': 8184, 'tennessee': 8185, 'hears': 8186, 'sane': 8187, 'research.': 8188, 'university,': 8189, 'tricks': 8190, 'special,': 8191, 'downtown.': 8192, 'yours,': 8193, 'confuse': 8194, 'copied': 8195, 'workshop': 8196, 'hear,': 8197, 'removing': 8198, 'blond': 8199, 'europe.': 8200, 'dancing,': 8201, 'celebrities': 8202, 'destination': 8203, 'composed': 8204, 'tests.': 8205, 'support,': 8206, 'lawrence': 8207, 'slut': 8208, 'plans,': 8209, 'highschool': 8210, 'me?\"': 8211, 'osama': 8212, 'much?': 8213, '(no,': 8214, 'supervisor': 8215, 'purdy': 8216, 'tears,': 8217, 'emotions.': 8218, 'nose,': 8219, 'patients': 8220, 'tommorrow': 8221, 'hiring': 8222, 'stitch': 8223, 'cares.': 8224, 'dogs,': 8225, 'introducing': 8226, 'fixed.': 8227, 'listing': 8228, 'amazing!': 8229, 'destroying': 8230, 'glowing': 8231, 'achieved': 8232, 'laura,': 8233, 'screen,': 8234, 'wheaton': 8235, 'publicly': 8236, 'imagining': 8237, 'importantly': 8238, 'homeland': 8239, 'think...': 8240, 'prep': 8241, 'interface': 8242, 'rains': 8243, 'knight': 8244, 'purposes': 8245, \"c'est\": 8246, 'sins': 8247, 'sydney': 8248, 'rule.': 8249, 'industry.': 8250, 'vote,': 8251, 'demanded': 8252, \"teacher's\": 8253, 'clearing': 8254, 'things!': 8255, 'anyhow.': 8256, 'noises': 8257, 'priest': 8258, 'hood': 8259, 'longer,': 8260, 'poetry,': 8261, 'life\"': 8262, 'priorities': 8263, 'unbelievably': 8264, 'foul': 8265, 'reveals': 8266, 'price.': 8267, 'oprah': 8268, \"dean's\": 8269, 'inc.': 8270, 'lady,': 8271, 'killers': 8272, 'warren': 8273, 'research,': 8274, 'contributed': 8275, \"something's\": 8276, 'much...': 8277, 'geez,': 8278, 'hunter': 8279, 'flirting': 8280, 'sanity': 8281, 'rest,': 8282, 'not...': 8283, 'technology,': 8284, 'network.': 8285, 'qualify': 8286, 'nintendo': 8287, 'erica': 8288, 'buy.': 8289, 'mandy': 8290, 'forums': 8291, 'laundry,': 8292, 'usually,': 8293, 'blocking': 8294, 'sake,': 8295, 'trusted': 8296, 'soviet': 8297, 'printing': 8298, 'sporting': 8299, 'horoscope': 8300, 'birthdays': 8301, 'queens': 8302, 'regardless,': 8303, \"st.marg's.\": 8304, 'desires': 8305, 'religion.': 8306, 'substitute': 8307, 'hafta': 8308, 'train,': 8309, 'silly,': 8310, 'teachers.': 8311, '\"let\\'s': 8312, 'hack': 8313, 'legitimate': 8314, 'wakes': 8315, 'cut.': 8316, 'noticed,': 8317, 'king.': 8318, 'smallest': 8319, 'disco': 8320, 'destined': 8321, 'adjust': 8322, \"mike's\": 8323, 'nazi': 8324, 'decorated': 8325, 'slack': 8326, 'faint': 8327, 'surgery.': 8328, 'complaints': 8329, 'roommates': 8330, 'argh!': 8331, 'melody': 8332, 'hat.': 8333, 'blur': 8334, 'public,': 8335, 'fucker': 8336, 'of...': 8337, 'vegetarian': 8338, 'hallway': 8339, 'demon': 8340, 'milk,': 8341, 'just...': 8342, 'hammer': 8343, 'star.': 8344, 'looking,': 8345, 'comparing': 8346, 'behaviour': 8347, 'scattered': 8348, 'buffet': 8349, 'won,': 8350, 'volleyball': 8351, 'canada,': 8352, 'bugging': 8353, 'slapped': 8354, 'cows': 8355, 'lakers': 8356, 'never,': 8357, 'sleeping,': 8358, 'clips': 8359, 'managers': 8360, 'wisconsin': 8361, '2:30': 8362, 'follows:': 8363, \"parent's\": 8364, 'bearing': 8365, 'anyway!': 8366, 'grief': 8367, 'slightest': 8368, 'upgrade': 8369, 'speak,': 8370, 'swept': 8371, 'surreal': 8372, 'snowing': 8373, 'trap': 8374, 'cabinet': 8375, \"family's\": 8376, 'lol...': 8377, 'smart.': 8378, 'writer.': 8379, 'priority': 8380, 'smarterchild:': 8381, 'arrogant': 8382, 'crucial': 8383, 'appearing': 8384, 'link,': 8385, 'institutions': 8386, 'slower': 8387, 'dorm': 8388, 'headline': 8389, 'scandal': 8390, 'dreading': 8391, 'dizzy': 8392, 'extraordinary': 8393, 'measures': 8394, 'sock': 8395, 'life:': 8396, 'abilities': 8397, 'collection.': 8398, 'inappropriate': 8399, 'adventure.': 8400, 'while...': 8401, '2002,': 8402, 'topic,': 8403, 'laughed.': 8404, 'maintaining': 8405, 'relax.': 8406, 'rubbing': 8407, 'microwave': 8408, 'cloudy': 8409, 'photoshop': 8410, 'senses': 8411, 'talaga': 8412, 'option.': 8413, 'mega': 8414, 'science.': 8415, 'kris': 8416, 'retired': 8417, 'lose.': 8418, 'techniques': 8419, 'instruments': 8420, 'geography': 8421, 'maker': 8422, 'chamber': 8423, 'strategies': 8424, 'skills,': 8425, 'design.': 8426, 'mint': 8427, 'naughty': 8428, 'stretching': 8429, 'argued': 8430, 'expense': 8431, 'quiz.': 8432, 'butt.': 8433, 'years?': 8434, \"doctor's\": 8435, 'girly': 8436, 'adopt': 8437, 'movie!': 8438, 'elite': 8439, 'mail,': 8440, 'whispered': 8441, 'templates': 8442, 'damn!': 8443, 'circus': 8444, 'statistics': 8445, 'fit.': 8446, 'muse': 8447, 'homosexuality': 8448, 'responsibilities': 8449, 'exploring': 8450, 'margaret': 8451, 'mozilla': 8452, 'signature': 8453, 'raging': 8454, 'performances': 8455, 'there)': 8456, 'rave': 8457, 'goodness,': 8458, 'deficit': 8459, 'musicians': 8460, 'exclusive': 8461, 'stored': 8462, 'finest': 8463, 'jenna': 8464, 'examine': 8465, 'exchanged': 8466, 'shame.': 8467, 'significantly': 8468, 'vomit': 8469, 'mildly': 8470, 'mashed': 8471, 'title,': 8472, 'excessive': 8473, 'day..': 8474, '(ok,': 8475, 'comment,': 8476, 'realizes': 8477, 'winners': 8478, 'fills': 8479, 'activities.': 8480, 'cases,': 8481, 'childish': 8482, 'revelation': 8483, 'neck.': 8484, 'folder': 8485, 'weakness': 8486, 'convincing': 8487, 'punched': 8488, 'tolerate': 8489, 'milk.': 8490, \"chris's\": 8491, 'smashing': 8492, 'statement.': 8493, 'naive': 8494, 'pounds.': 8495, 'slowly,': 8496, 'shelves': 8497, 'dutch': 8498, 'how.': 8499, 'bad...': 8500, 'singles': 8501, 'piles': 8502, 'policy.': 8503, 'sept.': 8504, 'supper': 8505, 'suspected': 8506, 'time..': 8507, 'saves': 8508, 'styles': 8509, 'warm.': 8510, 'come!': 8511, \"70's\": 8512, 'joys': 8513, 'succeed': 8514, 'batch': 8515, 'invites': 8516, 'knew,': 8517, 'deaf': 8518, 'crazy!': 8519, 'follow.': 8520, 'student.': 8521, 'joe,': 8522, 'trucks': 8523, 'hospital,': 8524, 'velvet': 8525, 'peer': 8526, 'right...': 8527, 'letter.': 8528, 'melted': 8529, 'off?': 8530, 'technologies': 8531, 'sheep': 8532, 'couch,': 8533, 'thirteen': 8534, 'socially': 8535, 'person!': 8536, 'motor': 8537, 'quitting': 8538, 'mixing': 8539, 'manner.': 8540, 'ladies,': 8541, 'director,': 8542, 'ages,': 8543, \"must've\": 8544, 'passed.': 8545, 'watching.': 8546, 'listen,': 8547, 'poke': 8548, \"who'd\": 8549, 'satellite': 8550, 'bachelor': 8551, 'requirements': 8552, 'tomatoes': 8553, 'rock!': 8554, 'chocolate.': 8555, 'cake,': 8556, 'cardboard': 8557, 'bathing': 8558, 'abit': 8559, 'point?': 8560, 'belated': 8561, 'hate,': 8562, 'ellen': 8563, 'coincidence.': 8564, 'reloaded': 8565, 'hangover': 8566, 'blaming': 8567, 'discussion.': 8568, 'color,': 8569, 'too)': 8570, 'relay': 8571, '(very': 8572, 'oppose': 8573, 'dancing.': 8574, 'poison': 8575, 'crowds': 8576, 'nonsense': 8577, 'bobo': 8578, 'breath,': 8579, 'letter,': 8580, 'bells': 8581, 'interview,': 8582, 'freshly': 8583, 'travels': 8584, 'encountered': 8585, 'pussy': 8586, 'sweet!': 8587, 'marie': 8588, 'photographer': 8589, 'vacuum': 8590, 'island,': 8591, 'wine,': 8592, '(also': 8593, 'apart,': 8594, 'arrived,': 8595, 'cliff': 8596, 'notes.': 8597, 'linkin': 8598, 'where,': 8599, 'puerto': 8600, 'assistance': 8601, 'skin,': 8602, 'tackle': 8603, 'hopeful': 8604, 'arrives': 8605, 'hmm..': 8606, 'beginning,': 8607, 'that).': 8608, 'playing,': 8609, 'competing': 8610, 'spanish.': 8611, 'russell': 8612, 'overtime': 8613, 'cds.': 8614, 'prof': 8615, 'parental': 8616, 'showered': 8617, 'hurts,': 8618, 'corporations': 8619, 'stars.': 8620, 'lara': 8621, 'fashion.': 8622, 'reactions': 8623, 'fork': 8624, '\"look': 8625, 'net.': 8626, 'pets': 8627, 'kansas': 8628, 'depp': 8629, 'words:': 8630, 'defeated': 8631, 'impossible.': 8632, 'read:': 8633, 'interviewing': 8634, 'bedroom.': 8635, 'insanity': 8636, 'fans.': 8637, 'brutal': 8638, 'dearest': 8639, '17th': 8640, '15th': 8641, 'friends!': 8642, 'wind,': 8643, 'brighter': 8644, 'akin': 8645, 'readers,': 8646, 'whip': 8647, 'salsa': 8648, 'meh.': 8649, 'indulge': 8650, 'field,': 8651, 'outline': 8652, 'sandwiches': 8653, 'roast': 8654, 'tires': 8655, \"joe's\": 8656, 'melissa,': 8657, 'did!': 8658, 'fulfill': 8659, 'calgary': 8660, 'trivia': 8661, 'drama,': 8662, 'today-': 8663, \"isn't.\": 8664, 'windows.': 8665, 'math.': 8666, \"boy's\": 8667, 'doctor.': 8668, 'hilarious!': 8669, 'paradise': 8670, 'simultaneously': 8671, 'happened?': 8672, 'alabama': 8673, 'path.': 8674, 'infected': 8675, 'failure.': 8676, 'deposit': 8677, 'naming': 8678, 'curry': 8679, 'reflects': 8680, 'health.': 8681, 'workout.': 8682, 'correctly': 8683, 'lump': 8684, 'snake': 8685, 'partial': 8686, 'tests,': 8687, 'releasing': 8688, 'each.': 8689, 'gots': 8690, 'size.': 8691, 'job?': 8692, 'rows': 8693, 'long-term': 8694, 'settling': 8695, 'stressing': 8696, 'walls.': 8697, 'shakes': 8698, 'spike': 8699, 'harvard': 8700, 'surprises': 8701, 'party!': 8702, 'speaking,': 8703, 'functions': 8704, 'batteries': 8705, 'going?': 8706, 'reasoning': 8707, 'efficient': 8708, \"matt's\": 8709, 'cease': 8710, 'safer': 8711, 'hello!': 8712, 'stray': 8713, 'bread.': 8714, 'small.': 8715, 'modem': 8716, 'autumn': 8717, 'feast': 8718, 'final.': 8719, 'boss,': 8720, 'account,': 8721, 'seems.': 8722, 'gulf': 8723, 'lady.': 8724, '1:00': 8725, 'living,': 8726, 'lasting': 8727, 'weekend...': 8728, 'strokes': 8729, 'beleive': 8730, 'person?': 8731, '2000.': 8732, '10:00': 8733, 'kerry.': 8734, 'shines': 8735, 'beautifully': 8736, 'stretched': 8737, 'blogger,': 8738, 'crap!': 8739, 'horrible,': 8740, 'savings': 8741, 'schools,': 8742, 'nicely.': 8743, 'sufficient': 8744, 'nights,': 8745, 'math,': 8746, 'affection': 8747, 'advice.': 8748, 'o.c.': 8749, 'dass': 8750, 'christian,': 8751, 'delightful': 8752, 'noon.': 8753, 'thomas,': 8754, 'proceeds': 8755, 'ikea': 8756, 'diner': 8757, 'dana': 8758, 'stars,': 8759, 'actions.': 8760, 'mistakes.': 8761, 'greetings': 8762, 'zombie': 8763, 'horses': 8764, 'bosses': 8765, 'questioned': 8766, 'situations.': 8767, 'bastards': 8768, 'material.': 8769, 'dental': 8770, 'lesbians': 8771, '\"thank': 8772, 'recieve': 8773, 'rack': 8774, 'student,': 8775, 'accounting': 8776, 'alam': 8777, 'finally.': 8778, 'sq(u)ee!': 8779, 'cherish': 8780, 'harbor': 8781, 'legislation': 8782, 'friends?': 8783, 'douglas': 8784, 'heat,': 8785, 'flashing': 8786, 'artificial': 8787, 'assault': 8788, 'vanity': 8789, 'completely.': 8790, 'chair.': 8791, 'corey': 8792, 'papa': 8793, 'haunted': 8794, 'estimated': 8795, 'sliding': 8796, 'vivid': 8797, 'dairy': 8798, 'few.': 8799, 'thumbs': 8800, 'since,': 8801, 'forehead': 8802, '\"big': 8803, 'buses': 8804, '(our': 8805, \"sam's\": 8806, 'communist': 8807, 'info.': 8808, 'down...': 8809, 'lawyers': 8810, 'used.': 8811, 'gift.': 8812, 'list:': 8813, 'giggle': 8814, 'landlord': 8815, 'devices': 8816, 'pizza,': 8817, 'pumped': 8818, 'knowledge.': 8819, 'surprised.': 8820, 'fingers.': 8821, 'oregon': 8822, 'football.': 8823, 'suit.': 8824, 'samurai': 8825, 'played.': 8826, 'searches': 8827, 'd.c.': 8828, 'knew.': 8829, 'further,': 8830, 'refusing': 8831, 'clear,': 8832, 'taken.': 8833, 'chart': 8834, 'compliment': 8835, 'linking': 8836, 'espn': 8837, 'animals.': 8838, 'day)': 8839, '\"love': 8840, 'harvey': 8841, 'explosion': 8842, 'jour:': 8843, 'consideration': 8844, \"dave's\": 8845, 'professors': 8846, 'tomorrow?': 8847, 'vancouver': 8848, 'satisfying': 8849, 'term.': 8850, 'shampoo': 8851, 'days?': 8852, 'prob': 8853, 'fading': 8854, 'button.': 8855, 'salary': 8856, 'ourselves,': 8857, 'convert': 8858, \"who've\": 8859, 'placing': 8860, 'dynamic': 8861, 'sincerely': 8862, 'jayel': 8863, 'umbrella': 8864, 'waist': 8865, 'that.\"': 8866, 'laptop.': 8867, 'finished,': 8868, 'theories': 8869, 'finish.': 8870, 'david,': 8871, 'yet!': 8872, 'monty': 8873, 'acentos': 8874, 'part-time': 8875, 'swinging': 8876, 'ward': 8877, 'india,': 8878, 'discovering': 8879, \"(i've\": 8880, 'leon': 8881, 'fragile': 8882, 'crimes': 8883, 'enable': 8884, 'explode': 8885, 'pieces.': 8886, 'image.': 8887, 'community,': 8888, 'humid': 8889, 'verge': 8890, 'countries.': 8891, 'inclined': 8892, 'april.': 8893, 'lyrics.': 8894, 'swiss': 8895, 'soak': 8896, 'forty': 8897, 'sake.': 8898, 'pleasure.': 8899, 'confess': 8900, 'spokesman': 8901, 'michigan.': 8902, 'occupation': 8903, '8:00': 8904, 'tracking': 8905, 'connection.': 8906, 'prone': 8907, '\"let': 8908, 'type,': 8909, 'characters,': 8910, 'wictory': 8911, 'obtain': 8912, 'wheat': 8913, 'corners': 8914, 'someplace': 8915, 'ditch': 8916, 'french.': 8917, 'blogging,': 8918, 'reject': 8919, 'england,': 8920, 'warped': 8921, 'sana': 8922, 'filthy': 8923, 'these,': 8924, 'inevitably': 8925, 'miles:': 8926, 'pregnant.': 8927, 'rob,': 8928, 'parties,': 8929, 'nonetheless,': 8930, 'brilliant.': 8931, 'response.': 8932, 'miles.': 8933, 'campus.': 8934, 'adam,': 8935, 'analyze': 8936, 'indians': 8937, 'anderson': 8938, 'sweep': 8939, 'affectionately...': 8940, 'tactics': 8941, 'grounds': 8942, 'dictionary': 8943, 'appreciated.': 8944, 'colorful': 8945, 'video,': 8946, 'theory,': 8947, 'painfully': 8948, 'fest': 8949, 'rope': 8950, 'contest.': 8951, 'are!': 8952, 'darren': 8953, 'instrument': 8954, 'magazine,': 8955, 'frames': 8956, 'trio': 8957, '(actually': 8958, 'mumbai': 8959, 'mayer': 8960, '\"fuck': 8961, 'slowly.': 8962, 'replacing': 8963, 'worm': 8964, 'cocktail': 8965, 'star,': 8966, 'sponsored': 8967, 'tissue': 8968, 'relationships,': 8969, 'severely': 8970, 'shave': 8971, 'taller': 8972, 'summers': 8973, 'teachers,': 8974, '(lol)': 8975, 'curriculum': 8976, 'impending': 8977, 'controversy': 8978, '(most': 8979, 'island.': 8980, 'quality.': 8981, 'apologized': 8982, 'entertained': 8983, 'july,': 8984, 'solar': 8985, 'myself!': 8986, 'members.': 8987, 'nyc.': 8988, 'sack': 8989, 'envelope': 8990, 'media,': 8991, 'curb': 8992, 'surround': 8993, 'nathan:': 8994, 'seated': 8995, \"postcount('\": 8996, 'forth.': 8997, 'separated': 8998, 'guitar,': 8999, 'help!': 9000, 'karl': 9001, 'touches': 9002, 'engaging': 9003, 'drinking,': 9004, 'mentions': 9005, 'lamp': 9006, 'weekends.': 9007, 'abandon': 9008, 'disgusting.': 9009, 'leaned': 9010, 'unbelievable': 9011, 'countdown': 9012, 'imagine.': 9013, 'transportation': 9014, 'rogers': 9015, 'donnie': 9016, 'brandy': 9017, 'unconscious': 9018, 'siblings': 9019, 'photos,': 9020, 'nagging': 9021, 'shattered': 9022, 'initiative': 9023, 'minded': 9024, 'alley': 9025, 'researchers': 9026, 'catherine': 9027, 'third,': 9028, 'bands.': 9029, 'tracy': 9030, 'joshua': 9031, '2002.': 9032, 'qaeda': 9033, 'cardio': 9034, 'hairy': 9035, 'gunna': 9036, 'fold': 9037, 'distract': 9038, 'sophia:': 9039, 'love\"': 9040, 'packages': 9041, 'return,': 9042, 'brain,': 9043, 'genius.': 9044, 'sir,': 9045, 'friend:': 9046, '25th': 9047, 'now....': 9048, 'her..': 9049, 'halls': 9050, '(only': 9051, 'ultra': 9052, 'obsessive': 9053, 'todo': 9054, 'visions': 9055, 'alexander': 9056, 'jose': 9057, 'happy?': 9058, 'storm.': 9059, 'calls,': 9060, 'cultures': 9061, 'mode.': 9062, 'windows,': 9063, 'waved': 9064, 'drinking.': 9065, 'advisor': 9066, 'bills.': 9067, '(his': 9068, 'florida.': 9069, 'challenge.': 9070, 'babysitting': 9071, 'sophomore': 9072, 'coworkers': 9073, '\"have': 9074, 'missed.': 9075, 'predicted': 9076, 'technique': 9077, 'brass': 9078, 'truck.': 9079, 'itching': 9080, 'sharpton': 9081, 'excess': 9082, 'administration.': 9083, 'you!\"': 9084, 'anal': 9085, 'them:': 9086, 'custom': 9087, 'tucked': 9088, 'foxnews.com': 9089, 'spaces': 9090, 'version.': 9091, 'trevor': 9092, 'hah.': 9093, 'ethnic': 9094, 'leftover': 9095, 'us...': 9096, 'learning.': 9097, 'spring.': 9098, 'dear,': 9099, 'asshole.': 9100, 'races': 9101, 'france,': 9102, 'cause.': 9103, 'contacted': 9104, 'premiere': 9105, 'upset,': 9106, 'consume': 9107, 'channel.': 9108, '2001,': 9109, 'advice,': 9110, 'driving,': 9111, 'episode.': 9112, 'yes...': 9113, 'imagine,': 9114, 'tho.': 9115, 'that..': 9116, 'vulnerable': 9117, 'raped': 9118, 'beauty,': 9119, 'plaza': 9120, 'loneliness': 9121, 'motorcycle': 9122, 'emerging': 9123, 'substantial': 9124, 'care?': 9125, 'please!': 9126, 'cock': 9127, 'interactive': 9128, 'opportunity.': 9129, 'suprised': 9130, 'anything...': 9131, 'cookies.': 9132, 'loser.': 9133, 'worldwide': 9134, 'tore': 9135, 'ruins': 9136, 'freely': 9137, 'bills,': 9138, 'occured': 9139, 'separation': 9140, 'corporation': 9141, 'it),': 9142, 'compelled': 9143, 'advocate': 9144, 'disease.': 9145, 'franklin': 9146, 'danielle': 9147, 'processing': 9148, 'poetic': 9149, 'batman': 9150, 'newsletter': 9151, 'alison': 9152, 'chelsea': 9153, 'illusion': 9154, 'candy.': 9155, 'disgusted': 9156, 'midnight,': 9157, 'nation,': 9158, 'decisions.': 9159, 'compassion': 9160, 'lastly,': 9161, 'electoral': 9162, 'this)': 9163, 'reflected': 9164, 'stoned': 9165, 'plates': 9166, 'comforting': 9167, 'anything?': 9168, 'nashville': 9169, 'curly': 9170, 'post!': 9171, 'furthermore,': 9172, 'cats.': 9173, 'stinky': 9174, '16th': 9175, 'dripping': 9176, 'resolved': 9177, 'sliced': 9178, '(both': 9179, 'communities': 9180, 'strangest': 9181, 'uneventful': 9182, 'discipline': 9183, 'conditioning': 9184, 'interesting...': 9185, 'fountain': 9186, 'safe,': 9187, 'restless': 9188, 'betsy': 9189, 'developers': 9190, 'rules,': 9191, 'solved': 9192, \"wasn't.\": 9193, 'feared': 9194, 'salmon': 9195, 'employed': 9196, 'hotmail': 9197, 'catches': 9198, \"jackson's\": 9199, 'john.': 9200, 'sinking': 9201, 'tripped': 9202, 'bald': 9203, 'stalking': 9204, 'scoring': 9205, 'pursuing': 9206, 'exams.': 9207, 'idiotic': 9208, 'comprehend': 9209, 'quit.': 9210, 'row,': 9211, 'whatsoever.': 9212, 'storms': 9213, 'daughter.': 9214, \"car's\": 9215, 'outstanding': 9216, 'ambiance:': 9217, 'booze': 9218, 'code.': 9219, 'dire': 9220, 'wesley': 9221, 'elbow': 9222, 'anger,': 9223, 'convey': 9224, 'tall,': 9225, 'downright': 9226, 'drained': 9227, 'sam,': 9228, 'mortgage': 9229, 'happening,': 9230, 'fashioned': 9231, 'stem': 9232, 'producer': 9233, 'connie': 9234, 'lonely,': 9235, 'sleep...': 9236, 'bombing': 9237, 'posed': 9238, 'appealing': 9239, 'football,': 9240, 'attendance': 9241, 'donuts': 9242, 'dial': 9243, 'sneaking': 9244, 'ends.': 9245, 'mickey': 9246, 'crowd,': 9247, 'ethics': 9248, 'portable': 9249, 'tofu': 9250, 'lazy,': 9251, 'gusto': 9252, 'years!': 9253, 'chilling': 9254, 'lie,': 9255, 'brown.': 9256, 'hanks': 9257, 'south.': 9258, 'theirs': 9259, 'count.': 9260, '(okay,': 9261, 'absent': 9262, 'absurd': 9263, 'sundays': 9264, 'cara,': 9265, 'aussie': 9266, '(her': 9267, 'degree.': 9268, 'hostel': 9269, 'that:': 9270, 'automatic': 9271, 'mailed': 9272, 'daylight': 9273, 'ironically,': 9274, 'yah,': 9275, 'juice.': 9276, 'phoned': 9277, 'softly': 9278, 'interrupted': 9279, 'pigs': 9280, 'softball': 9281, \"administration's\": 9282, 'sector': 9283, 'e-mail.': 9284, 'journal,': 9285, 'deer': 9286, 'towels': 9287, 'time).': 9288, 'immature': 9289, 'elsewhere': 9290, 'purpose,': 9291, '9:00': 9292, 'comes,': 9293, 'really...': 9294, 'guaranteed': 9295, 'bucket': 9296, 'scoop': 9297, 'gradually': 9298, 'rotten': 9299, 'battles': 9300, 'heaven,': 9301, 'postal': 9302, 'tropical': 9303, 'tivo': 9304, 'tattoo': 9305, 'audition': 9306, 'brown,': 9307, 'preach': 9308, 'funk': 9309, 'probably.': 9310, 'cube': 9311, 'donkey': 9312, 'assorted': 9313, 'wine.': 9314, 'perceived': 9315, '\"there\\'s': 9316, 'boarding': 9317, 'insecure': 9318, 'portland': 9319, 'babysit': 9320, 'distribution': 9321, 'balls.': 9322, 'scent': 9323, 'organizing': 9324, 'forget,': 9325, \"frickin'\": 9326, 'teeth,': 9327, 'honda': 9328, 'interpretation': 9329, 'moon,': 9330, 'stephanie': 9331, 'definetly': 9332, 'london.': 9333, 'arm.': 9334, 'complete.': 9335, '*cough*': 9336, 'helpless': 9337, 'governments': 9338, 'minimal': 9339, 'relating': 9340, 'stab': 9341, 'mortal': 9342, 'springs': 9343, 'better?': 9344, 'low,': 9345, 'andrew,': 9346, 'pretended': 9347, 'cancer,': 9348, 'realm': 9349, 'degrees.': 9350, 'montreal': 9351, 'election,': 9352, 'lab.': 9353, 'marriages': 9354, 'wind.': 9355, 'ugly.': 9356, 'unaware': 9357, 'gifted': 9358, 'romeo': 9359, 'roel': 9360, 'blamed': 9361, 'suspicious': 9362, 'levels.': 9363, 'willie': 9364, 'hooray': 9365, 'walls,': 9366, 'direction,': 9367, 'pulp': 9368, 'unemployed': 9369, 'neon': 9370, 'civilian': 9371, 'tonight...': 9372, 'programs.': 9373, 'begining': 9374, 'quarters': 9375, 'oklahoma': 9376, 'unnecessary': 9377, 'agents': 9378, 'justice.': 9379, 'complain.': 9380, 'headlines': 9381, 'establishment': 9382, 'directors': 9383, 'jokes.': 9384, 'sincere': 9385, 'sorting': 9386, '7:00': 9387, 'mikey': 9388, 'exotic': 9389, 'responding': 9390, 'testament': 9391, 'maid': 9392, 'confusing.': 9393, 'sooooooo': 9394, 'atlantic': 9395, 'stanley': 9396, 'towns': 9397, 'comfy': 9398, 'voter': 9399, 'tourists': 9400, 'fans,': 9401, 'confused,': 9402, 'swings': 9403, 'auntie': 9404, 'pala': 9405, 'that?\"': 9406, 'approve': 9407, \"rockin'\": 9408, 'usage': 9409, 'librarian': 9410, 'internship': 9411, 'frodo': 9412, 'cubicle': 9413, 'comfortable.': 9414, 'crab': 9415, 'aber': 9416, 'convenient': 9417, 'assist': 9418, 'gigs': 9419, 'slices': 9420, 'semi': 9421, 'gentleman': 9422, 'suburban': 9423, 'awarded': 9424, 'manda': 9425, 'tough.': 9426, 'rejection': 9427, 'swore': 9428, 'vanessa': 9429, 'temptation': 9430, 'significance': 9431, 'waiting,': 9432, \"brian's\": 9433, 'procedure': 9434, 'positively': 9435, 'boss.': 9436, 'horrid': 9437, 'pathetic.': 9438, 'dahil': 9439, 'sickness': 9440, 'chop': 9441, '3:30': 9442, 'now.\"': 9443, 'servers': 9444, 'tossing': 9445, 'august,': 9446, 'sixteen': 9447, 'shaved': 9448, 'lucky,': 9449, 'whoa,': 9450, 'philadelphia': 9451, 'free!': 9452, 'saturday:': 9453, 'labeled': 9454, 'ate,': 9455, 'wonder.': 9456, 'xanga': 9457, 'indicate': 9458, 'transferred': 9459, 'dems': 9460, 'involvement': 9461, 'presenting': 9462, 'lone': 9463, '12:30': 9464, 'exploded': 9465, 'assholes': 9466, 'stacey': 9467, \"adam's\": 9468, 'strongest': 9469, 'cheap.': 9470, 'tape.': 9471, 'rebel': 9472, 'bastard.': 9473, 'commander': 9474, 'snapped': 9475, 'ago...': 9476, 'soup.': 9477, 'serious,': 9478, 'denial': 9479, 'painful.': 9480, 'filmed': 9481, 'defensive': 9482, 'hooray!': 9483, 'hole.': 9484, 'parker': 9485, 'thrill': 9486, 'judgment': 9487, 'alcohol,': 9488, 'singing.': 9489, 'graduation.': 9490, 'nifty': 9491, 'sipping': 9492, 'cheek': 9493, 'spirit,': 9494, 'enjoyable.': 9495, 'moody': 9496, 'nothin': 9497, 'anyhoo': 9498, '(via': 9499, 'volunteered': 9500, 'females': 9501, 'chris.': 9502, 'baptist': 9503, 'novel.': 9504, 'same-sex': 9505, 'suppose,': 9506, 'generated': 9507, 'readers.': 9508, 'settings': 9509, 'ginger': 9510, 'marten:': 9511, 'generic': 9512, 'groups,': 9513, 'lol!!': 9514, 'take,': 9515, 'links,': 9516, 'memo': 9517, 'lights,': 9518, 'coworker': 9519, 'swollen': 9520, 'freezer': 9521, 'amidst': 9522, 'kittens': 9523, 'hit.': 9524, 'sucked,': 9525, 'thingy.': 9526, 'forward.': 9527, 'preferably': 9528, 'session.': 9529, 'subsequent': 9530, 'revised': 9531, 'am...': 9532, 'kenny': 9533, 'snacks': 9534, 'themes': 9535, 'rats': 9536, 'jealous.': 9537, 'vince': 9538, 'welcome.': 9539, 'males': 9540, 'design,': 9541, 'century.': 9542, 'noah': 9543, 'ricky': 9544, 'written,': 9545, 'particular,': 9546, 'stiff': 9547, 'namely': 9548, 'sally': 9549, 'james,': 9550, 'prominent': 9551, 'enthusiastic': 9552, 'herself,': 9553, 'fluffy': 9554, 'psychology': 9555, '2:00': 9556, 'lottery': 9557, 'affecting': 9558, \"didn't,\": 9559, 'goth': 9560, 'shipped': 9561, 'scratching': 9562, 'streets,': 9563, 'fluid': 9564, 'broken,': 9565, 'kelly,': 9566, 'salon': 9567, 'notice,': 9568, 'haunt': 9569, 'chains': 9570, 'horizon': 9571, 'amazes': 9572, 'skull': 9573, 'waits': 9574, 'loathe': 9575, 'foot.': 9576, 'soaking': 9577, 'shakespeare': 9578, 'sleeps': 9579, 'should,': 9580, 'passion.': 9581, 'cleaner': 9582, 'leg.': 9583, 'jacob': 9584, 'noise.': 9585, 'corps': 9586, 'civic': 9587, 'billions': 9588, '30th': 9589, 'glasses.': 9590, 'ear.': 9591, 'track,': 9592, '(insert': 9593, 'heritage': 9594, \"there'll\": 9595, 'distance.': 9596, 'worst.': 9597, 'caleb': 9598, 'lucas': 9599, 'turtle': 9600, 'wealthy': 9601, 'delivering': 9602, 'puke': 9603, 'meaning.': 9604, 'cara': 9605, '\"god': 9606, 'cursing': 9607, 'altogether': 9608, 'detail.': 9609, 'holiday,': 9610, 'heaps': 9611, 'fly.': 9612, 'powell': 9613, 'unrelated': 9614, 'silently': 9615, 'singing,': 9616, \"'what\": 9617, 'shelly': 9618, 'pleasantly': 9619, 'moron': 9620, 'researching': 9621, 'respected': 9622, 'spot,': 9623, 'giddy': 9624, 'admission': 9625, 'designs': 9626, 'lingering': 9627, 'please?': 9628, 'pine': 9629, 'omg,': 9630, 'agencies': 9631, 'moving,': 9632, 'landscape': 9633, '2000,': 9634, 'theater.': 9635, 'edit:': 9636, 'normally,': 9637, 'canceled': 9638, 'huge.': 9639, 'okay?': 9640, 'ritual': 9641, 'operate': 9642, 'else...': 9643, 'place?': 9644, 'force.': 9645, 'charging': 9646, 'things?': 9647, 'sarah:': 9648, 'tide': 9649, 'giants': 9650, 'nelson': 9651, 'oil,': 9652, 'illness': 9653, 'smoke.': 9654, 'apology': 9655, 'preaching': 9656, 'container': 9657, 'bias': 9658, 'house...': 9659, 'novel,': 9660, 'savage': 9661, 'alas': 9662, 'concluded': 9663, 'ranks': 9664, 'pple': 9665, 'decades': 9666, 'honey,': 9667, 'intelligent,': 9668, 'resolutions': 9669, 'better...': 9670, 'vibe': 9671, 'alcohol.': 9672, 'horn': 9673, 'huge,': 9674, 'mine!': 9675, 'sting': 9676, 'moreover,': 9677, 'accompany': 9678, 'development.': 9679, 'pockets': 9680, 'mardi': 9681, 'ugly,': 9682, 'comprehensive': 9683, 'ill.': 9684, 'rings:': 9685, 'matched': 9686, 'surrender': 9687, 'block.': 9688, 'lovely,': 9689, 'generate': 9690, 'dances': 9691, 'sensible': 9692, 'economics': 9693, 'palace': 9694, 'delicate': 9695, 'centered': 9696, 'bury': 9697, '12th': 9698, 'it.)': 9699, 'supportive': 9700, 'june,': 9701, 'europe,': 9702, 'security.': 9703, 'cats,': 9704, 'banging': 9705, 'mix.': 9706, 'eventful': 9707, 'mundane': 9708, 'anymore...': 9709, 'interior': 9710, '14th': 9711, 'trials': 9712, 'papers,': 9713, 'qualities': 9714, 'residence': 9715, 'agrees': 9716, 'course!': 9717, 'forward,': 9718, 'score.': 9719, 'bad?': 9720, 'departure': 9721, 'movement.': 9722, 'horny': 9723, 'assumption': 9724, 'croydon': 9725, 'ugh!': 9726, 'liek': 9727, 'expensive,': 9728, 'statement,': 9729, \"paul's\": 9730, 'effect,': 9731, 'poem.': 9732, 'story:': 9733, \"y'all.\": 9734, 'suspended': 9735, 'ants': 9736, 'designing': 9737, 'ranch': 9738, 'whole,': 9739, 'vocabulary': 9740, 'bethany': 9741, 'giggling': 9742, 'activity.': 9743, 'terminal': 9744, 'prompted': 9745, 'garden.': 9746, 'cheap,': 9747, 'woof!': 9748, 'specifically,': 9749, 'petition': 9750, 'full-time': 9751, 'disgust': 9752, 'bands,': 9753, 'bartender': 9754, 'uploaded': 9755, 'long!': 9756, 'passport': 9757, 'gloves': 9758, 'yeah..': 9759, 'bball': 9760, 'moods': 9761, 'taping': 9762, 'passengers': 9763, 'tedious': 9764, 'sing.': 9765, 'compliments': 9766, 'shoulder.': 9767, 'blockbuster': 9768, 'feminine': 9769, 'balloon': 9770, 'somethings': 9771, 'tho,': 9772, 'cleveland': 9773, '\"come': 9774, 'options.': 9775, 'politely': 9776, 'krispy': 9777, 'computers.': 9778, 'talents': 9779, 'boiling': 9780, 'television,': 9781, 'mmm,': 9782, 'reviewed': 9783, 'thanksgiving.': 9784, 'congressional': 9785, 'touch,': 9786, 'weighs': 9787, 'installing': 9788, 'flirt': 9789, 'tight.': 9790, 'land,': 9791, 'floppy': 9792, 'architecture': 9793, 'moments,': 9794, 'anywho': 9795, 'productive.': 9796, 'journals': 9797, 'secular': 9798, 'anymore?': 9799, 'commute': 9800, 'intensity': 9801, 'bedroom,': 9802, 'farmers': 9803, 'turn.': 9804, 'healthy.': 9805, 'lenny': 9806, 'devote': 9807, '\"new': 9808, 'sequence': 9809, 'i...': 9810, 'opposing': 9811, 'dignity': 9812, 'freedom,': 9813, 'memory,': 9814, 'yells': 9815, 'girl!': 9816, 'home?': 9817, 'everything!': 9818, '\"now': 9819, 'cds,': 9820, 'here.\"': 9821, 'ladder': 9822, 'stall': 9823, 'outrageous': 9824, 'spaghetti': 9825, 'concerned,': 9826, 'hat,': 9827, 'laser': 9828, 'those,': 9829, 'exam,': 9830, 'generations': 9831, 'grabbing': 9832, 'directions.': 9833, 'leaving,': 9834, 'hideous': 9835, 'market,': 9836, 'sans': 9837, 'convention.': 9838, 'bowl,': 9839, 'stubborn': 9840, 'superman': 9841, 'jeans.': 9842, 'movements': 9843, 'beauty.': 9844, 'protective': 9845, 'hugging': 9846, 'console': 9847, 'standards.': 9848, \"clinton's\": 9849, 'patent': 9850, 'pitt': 9851, 'lesson,': 9852, 'conscience': 9853, 'am]:': 9854, '23rd': 9855, 'releases': 9856, 'integrity': 9857, 'input': 9858, 'behind,': 9859, 'gladly': 9860, 'luggage': 9861, 'paycheck': 9862, 'pitcher': 9863, 'readily': 9864, 'ontario': 9865, 'over...': 9866, 'juicy': 9867, 'debate.': 9868, 'loved.': 9869, 'goes:': 9870, 'squad': 9871, 'volunteers': 9872, 'existed': 9873, 'swearing': 9874, 'depression.': 9875, 'guitarist': 9876, 'scar': 9877, 'hiking': 9878, 'outing': 9879, 'river,': 9880, 'sentence.': 9881, 'soil': 9882, \"neighbor's\": 9883, 'renting': 9884, \"child's\": 9885, 'failed.': 9886, 'nathan,': 9887, 'sidebar': 9888, 'convo': 9889, 'sucks!': 9890, 'understanding.': 9891, 'sabi': 9892, 'anywhere,': 9893, 'exercises': 9894, 'training,': 9895, 'tried.': 9896, 'import': 9897, 'engines': 9898, 'olivia': 9899, 'frustrating.': 9900, \"'bout\": 9901, 'premise': 9902, 'relief.': 9903, 'struggled': 9904, 'seconds,': 9905, 'funnier': 9906, 'advised': 9907, 'allies': 9908, 'forming': 9909, 'yea.': 9910, 'clerk': 9911, 'ranked': 9912, 'whole.': 9913, 'cover.': 9914, 'before?': 9915, 'portrait': 9916, 'founded': 9917, 'seat,': 9918, 'laptop,': 9919, 'publicity': 9920, 'territory': 9921, 'fail.': 9922, 'shoots': 9923, 'collapse': 9924, 'scope': 9925, 'gaze': 9926, 'december.': 9927, 'animals,': 9928, 'drift': 9929, 'gallon': 9930, 'routine.': 9931, 'manager,': 9932, 'sequel': 9933, 'arcade': 9934, 'rice,': 9935, 'trainer': 9936, 'frightening': 9937, 'msnbc': 9938, 'friend?': 9939, 'companion': 9940, 'filing': 9941, 'everyones': 9942, 'diva': 9943, 'parks': 9944, 'markets': 9945, \"uncle's\": 9946, 'hype': 9947, 'firing': 9948, 'neat.': 9949, 'occasions': 9950, 'florida,': 9951, 'messages.': 9952, 'butterflies': 9953, 'weary': 9954, 'jsut': 9955, 'nite.': 9956, 'approaches': 9957, 'dirk': 9958, 'dyed': 9959, 'flaws': 9960, 'powerpoint': 9961, 'curiosity': 9962, 'combo': 9963, 'hot!': 9964, 'brian,': 9965, 'glass.': 9966, 'japan.': 9967, 'columbus': 9968, 'backpack': 9969, 'mondays': 9970, 'rocks,': 9971, 'brunch': 9972, 'rubbish': 9973, 'electronics': 9974, 'babe': 9975, 'labels': 9976, '6:00': 9977, 'airplane': 9978, 'spring,': 9979, \"nothing's\": 9980, 'scanner': 9981, 'quoting': 9982, 'thoughts:': 9983, 'levengals': 9984, 'boil': 9985, 'stripped': 9986, 'visit,': 9987, \"kid's\": 9988, 'gentlemen,': 9989, '11:00': 9990, 'broke.': 9991, 'snuck': 9992, 'just,': 9993, 'liver': 9994, 'networking': 9995, 'grace,': 9996, 'earning': 9997, 'punished': 9998, 'correctly,': 9999}\n",
      "['that', 'have', 'with', 'this', 'just']\n"
     ]
    }
   ],
   "source": [
    "word_dict = dict_from_data(words_data)\n",
    "print(word_dict)\n",
    "train_data = [key for key in word_dict.keys()]\n",
    "print(train_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are gonna look a little how our words are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length word of the vocabulary: 6.5097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'count words'),\n",
       " Text(0.5, 0, '# letters per word'),\n",
       " Text(0.5, 1.0, 'Dictionary Histogram')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths_vocab = [len(w) for w in train_data]\n",
    "mean_vocab = np.mean(lengths_vocab)\n",
    "print('Mean length word of the vocabulary: {}'.format(mean_vocab))\n",
    "\n",
    "plt.hist(lengths_vocab)\n",
    "plt.gca().set(title='Dictionary Histogram', ylabel='count words', xlabel='# letters per word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph contrast with the distribution of the length on the english words, which have a mean of 8.23 characters (as commented in http://www.ravi.io/language-word-lengths). But we have to take into acount that we didn’t take as sample all the english dictionary, as the distribution of the english words lengths doesn’t take into account the frequency, which is normally higher for shorter words, being more easily represented in smaller samples.\n",
    "\n",
    "Now we are gonna take a quick peek at the letter distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = []\n",
    "values = []\n",
    "\n",
    "for letter in string.ascii_lowercase:\n",
    "    sum_letter = 0\n",
    "    for word in train_data:\n",
    "        sum_letter += word.count(letter)\n",
    "    \n",
    "    count.append(sum_letter)\n",
    "    values.append(letter)\n",
    "    \n",
    "plt.bar(values, count)\n",
    "plt.gca().set(title='Letter Histogram', ylabel='count letters', xlabel='letter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution is almost identical as the distribution of the letters in the english vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "import random\n",
    "test_data = random.choices(words_data, k=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have checked that the dataset we are going to use consist of the 5000 most common words in the blogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 The dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we are going to construct in this notebook we will construct a feature representation which consists in representing each letter of each word as an integer. We will be using the Latin alphabet as a dictionary between letters and integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26} \n",
      "\n",
      "26 {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "letter2int = dict(zip(string.ascii_lowercase, range(1,27)))\n",
    "int2letter = {v: k for k, v in letter2int.items()}\n",
    "\n",
    "print(len(letter2int), letter2int, '\\n')\n",
    "print(len(int2letter), int2letter, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Transform the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word dictionary which allows us to transform the letters appearing in the words into integers, it is time to make use of it and convert our posts words to their integer sequence representation. Since we will be using a recurrent neural network, it will be convenient if the length of each word is the same. To do this, we will use the previously defined max size (the length of the word supercalifragilisticexpialidocious) as the fixed size for our words and then pad short words with the category 'no letter' (which we will label 0) and truncate long words.\n",
    "\n",
    "For now, the punctuations and numbers will be ignored, as it will simplify drastically the task. If there's enough time in the future, it could be possible to implement some kind of punctuation mark holder that allows to save the punctuation of the sentence and put them back after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2integer(letter_dict, word, pad = 34):\n",
    "    padding = 0 #we will pad with zeros\n",
    "    word_padded = []\n",
    "    length = len(word)\n",
    "    \n",
    "    #conversion\n",
    "    for letter_index, letter in enumerate(word):\n",
    "        if letter in letter_dict:\n",
    "            if letter_dict[letter] >= 0:\n",
    "                word_padded.append(letter_dict[letter])\n",
    "        else:\n",
    "            length -= 1\n",
    "    \n",
    "    #padding\n",
    "    if len(word_padded) < pad:\n",
    "        word_padded = (word_padded + pad * [padding])[:pad]\n",
    "            \n",
    "    return word_padded, length\n",
    "\n",
    "def sentence2integer(letter_dict, data, pad = 34):   \n",
    "    result = []\n",
    "    lengths = []\n",
    "    perc = 0        \n",
    "    \n",
    "    for idx_w, word in enumerate(data):\n",
    "        \n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} word = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "        \n",
    "        converted_word, len_word = word2integer(letter_dict, word, pad)\n",
    "        result.append(converted_word)\n",
    "        lengths.append(len_word)\n",
    "        \n",
    "    return result, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10000 word = 0%\n",
      "1000 / 10000 word = 10.0%\n",
      "2000 / 10000 word = 20.0%\n",
      "3001 / 10000 word = 30.0%\n",
      "4000 / 10000 word = 40.0%\n",
      "5000 / 10000 word = 50.0%\n",
      "6000 / 10000 word = 60.0%\n",
      "7000 / 10000 word = 70.0%\n",
      "8000 / 10000 word = 80.0%\n",
      "9000 / 10000 word = 90.0%\n",
      "0 / 20000 word = 0%\n",
      "2000 / 20000 word = 10.0%\n",
      "4000 / 20000 word = 20.0%\n",
      "6001 / 20000 word = 30.0%\n",
      "8000 / 20000 word = 40.0%\n",
      "10000 / 20000 word = 50.0%\n",
      "12000 / 20000 word = 60.0%\n",
      "14000 / 20000 word = 70.0%\n",
      "16000 / 20000 word = 80.0%\n",
      "18000 / 20000 word = 90.0%\n",
      "Previous lengths: 10000-10000 \n",
      "New length: 9445-9445\n"
     ]
    }
   ],
   "source": [
    "train_data_padded, train_data_padded_len = sentence2integer(letter2int, train_data)\n",
    "test_data_p, len_test_p = sentence2integer(letter2int, test_data)\n",
    "\n",
    "# There are some words in training that after the transformation will stay at less than 4 letters, so they will be disposed now.\n",
    "train_data_p = [] \n",
    "len_train_p = []\n",
    "for word, lenw in zip(train_data_padded, train_data_padded_len):\n",
    "    if lenw > 3:\n",
    "            train_data_p.append(word)\n",
    "            len_train_p.append(lenw)\n",
    "            \n",
    "print('Previous lengths: {}-{} \\nNew length: {}-{}'.format(len(train_data_padded), len(train_data_padded_len), \n",
    "                                                           len(train_data_p), len(len_train_p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 9, 20, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 4\n",
      "[20, 8, 5, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 4\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "for ind_w in [2, 8]:\n",
    "    print(train_data_p[ind_w], len_train_p[ind_w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = None\n",
    "blogs_data = None\n",
    "blogs_data_shuffled = None\n",
    "test_data = None\n",
    "train_data = None\n",
    "train_data_padded = None\n",
    "train_data_padded_len = None\n",
    "word = None\n",
    "word1 = None\n",
    "word_dict = None\n",
    "words = None\n",
    "words_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Jumble the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the objective is to get back the words after being jumbled, we have now to jumble this data. We create a function to jumble the words, and then another function to handle all the words in a sentence.\n",
    "\n",
    "Probabilistically, the result of the function could be the same word passed as input, but we will ignore this possible case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jumble_word(word, word_len):\n",
    "    if word_len <= 2:\n",
    "        word_j = word\n",
    "        \n",
    "    else:\n",
    "        sub_word = []\n",
    "        for w in word:\n",
    "            sub_word.append(w)\n",
    "\n",
    "        sub_word = sub_word[1:word_len-1]\n",
    "        shufled_word = shuffle(sub_word)\n",
    "        \n",
    "        word_j = word.copy()\n",
    "        word_j[1:word_len-1] = shufled_word\n",
    "        \n",
    "    return word_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 5, 4, 6, 3, 7, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Check it\n",
    "word = [1,2,3,4,5,6,7,0,0,0,0,0,0,0]\n",
    "len_w = 7\n",
    "print(jumble_word(word, len_w))\n",
    "\n",
    "word = [1,2,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "len_w = 2\n",
    "print(jumble_word(word, len_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jumble_data(data, data_len):\n",
    "    jumbled_data = []\n",
    "    \n",
    "    idx_w = 0\n",
    "    perc = 0\n",
    "    for word, w_len in zip(data, data_len):\n",
    "        jumbled_sentence = []\n",
    "\n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} words = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "\n",
    "        jumbled_word = jumble_word(word, w_len)\n",
    "\n",
    "        jumbled_data.append(jumbled_word)\n",
    "        idx_w+=1\n",
    "        \n",
    "    return jumbled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 9445 words = 0%\n",
      "945 / 9445 words = 10.0%\n",
      "1889 / 9445 words = 20.0%\n",
      "2834 / 9445 words = 30.0%\n",
      "3778 / 9445 words = 40.0%\n",
      "4723 / 9445 words = 50.0%\n",
      "5667 / 9445 words = 60.0%\n",
      "6612 / 9445 words = 70.0%\n",
      "7556 / 9445 words = 80.0%\n",
      "8501 / 9445 words = 90.0%\n",
      "0 / 20000 words = 0%\n",
      "2000 / 20000 words = 10.0%\n",
      "4000 / 20000 words = 20.0%\n",
      "6001 / 20000 words = 30.0%\n",
      "8000 / 20000 words = 40.0%\n",
      "10000 / 20000 words = 50.0%\n",
      "12000 / 20000 words = 60.0%\n",
      "14000 / 20000 words = 70.0%\n",
      "16000 / 20000 words = 80.0%\n",
      "18000 / 20000 words = 90.0%\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "jumbled_train = jumble_data(train_data_p, len_train_p)\n",
    "jumbled_test = jumble_data(test_data_p, len_test_p)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: [10, 21, 19, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length: 4\n",
      "Jumbled: [10, 21, 19, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Word: [1, 23, 1, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length: 4\n",
      "Jumbled: [1, 1, 23, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Word: [1, 2, 15, 21, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length: 5\n",
      "Jumbled: [1, 2, 21, 15, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Word: [20, 8, 5, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length: 4\n",
      "Jumbled: [20, 8, 5, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "for ind_w in [4, 6]:\n",
    "    print('Word: {}'.format(train_data_p[ind_w]))\n",
    "    print('Length: {}'.format(len_train_p[ind_w]))\n",
    "    print('Jumbled: {}\\n'.format(jumbled_train[ind_w]))\n",
    "    \n",
    "    print('Word: {}'.format(test_data_p[ind_w]))\n",
    "    print('Length: {}'.format(len_test_p[ind_w]))\n",
    "    print('Jumbled: {}\\n'.format(jumbled_test[ind_w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train input: 9445, train output: 9445, lengths train: 9445 \n",
      "test input: 20000, test output: 20000, lengths test: 20000\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Check sizes coincide\n",
    "print('train input: {}, train output: {}, lengths train: {} \\ntest input: {}, test output: {}, lengths test: {}'.format(\n",
    "    len(jumbled_train), len(train_data_p), len(len_train_p),\n",
    "    len(jumbled_test), len(test_data_p), len(len_test_p)))\n",
    "\n",
    "# Check the same numbers are used in both input and output on same sentence number \n",
    "for word1, word2 in zip(jumbled_train, train_data_p):\n",
    "    sum_num=0\n",
    "    \n",
    "    if len(word1) != len(word2):\n",
    "        print('errror of size in training!')\n",
    "\n",
    "    for indx in range(len(word1)):\n",
    "        sum_num += word1[indx]-word2[indx]\n",
    "    \n",
    "    if sum_num != 0:\n",
    "        print('error of numbers in training!')\n",
    "\n",
    "for word1, word2 in zip(jumbled_test, test_data_p):\n",
    "    sum_num=0\n",
    "    \n",
    "    if len(word1) != len(word2):\n",
    "        print('errror of size in test!')\n",
    "\n",
    "    for indx in range(len(word1)):\n",
    "        sum_num += word1[indx]-word2[indx]\n",
    "    \n",
    "    if sum_num != 0:\n",
    "        print('error of numbers in test!')\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1 Uploading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data to file: cache/data/input_train_data.pkl\n",
      "Wrote data to file: cache/data/output_train_data.pkl\n",
      "Wrote data to file: cache/data/length_train_data.pkl\n",
      "Wrote data to file: cache/data/input_test_data.pkl\n",
      "Wrote data to file: cache/data/output_test_data.pkl\n",
      "Wrote data to file: cache/data/length_test_data.pkl\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(data_dir): # Make sure that the folder exists, if not create it\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "# Uploading train and test files separately, we check it doesn't upload an empty file\n",
    "data_files = [jumbled_train, train_data_p, len_train_p,\n",
    "              jumbled_test, test_data_p, len_test_p]\n",
    "cache_files = ['input_train_data.pkl', 'output_train_data.pkl', 'length_train_data.pkl',\n",
    "               'input_test_data.pkl', 'output_test_data.pkl', 'length_test_data.pkl']\n",
    "\n",
    "for data_file, cache_file in zip(data_files, cache_files):\n",
    "    cache_data = None\n",
    "    cache_data = dict(data_file=data_file)\n",
    "    file_dir = os.path.join(data_dir, cache_file)\n",
    "\n",
    "    with open(file_dir, \"wb\") as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "        \n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        print(\"Wrote data to file:\", file_dir)\n",
    "    else:\n",
    "        print('Wrote empty file on file', file_dir)\n",
    "        \n",
    "print('Done')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are gonna clean some more variables as we will download them in the next sections or not gonna use them more\n",
    "len_data_p = None\n",
    "words_data_p = None\n",
    "jumbled_data = None\n",
    "trainX = None\n",
    "trainY = None\n",
    "train_len = None\n",
    "testX = None\n",
    "testY = None\n",
    "test_len = None\n",
    "word2 = None\n",
    "word1 = None\n",
    "word = None\n",
    "data_file = None\n",
    "data_files = None\n",
    "files = None\n",
    "cache_files = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2 Loading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cache_file, data_dir=data_dir):\n",
    "    cache_data = None\n",
    "    file_dir = os.path.join(data_dir, cache_file)\n",
    "\n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read data from file:\", file_dir)\n",
    "        except:\n",
    "            print('Problem reading the file', file_dir)\n",
    "    else:\n",
    "        print('File empty')\n",
    "\n",
    "    if cache_data is None:\n",
    "        print('Didnt read anything')\n",
    "        resulting_files = []\n",
    "    else:\n",
    "        resulting_files = (cache_data['data_file'])\n",
    "        \n",
    "    return resulting_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already done all the previous steps in the past, you should have the training and test split files in the cache/data folder. We will load them and the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from file: cache/data/input_train_data.pkl\n",
      "Read data from file: cache/data/input_test_data.pkl\n",
      "Read data from file: cache/data/output_train_data.pkl\n",
      "Read data from file: cache/data/output_test_data.pkl\n",
      "Read data from file: cache/data/length_train_data.pkl\n",
      "Read data from file: cache/data/length_test_data.pkl\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "cache_files = ['input_train_data.pkl', 'input_test_data.pkl',\n",
    "               'output_train_data.pkl', 'output_test_data.pkl',\n",
    "               'length_train_data.pkl', 'length_test_data.pkl']\n",
    "\n",
    "resulting_files = []\n",
    "for cache_file in cache_files:\n",
    "    resulting_files.append(load_data(cache_file, data_dir))\n",
    "        \n",
    "print('Done')\n",
    "\n",
    "trainX, testX, trainY, testY, train_len, test_len = resulting_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word trainX -> [12, 9, 11, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word trainY -> [12, 9, 11, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word train_len -> 4\n",
      "word testX -> [20, 5, 18, 8, 5, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word testY -> [20, 8, 5, 18, 5, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word test_len -> 6\n"
     ]
    }
   ],
   "source": [
    "# Check they have been loaded correctly\n",
    "ind = 5\n",
    "\n",
    "print('word trainX ->', trainX[ind])\n",
    "print('word trainY ->', trainY[ind])\n",
    "print('word train_len ->', str(train_len[ind]))\n",
    "\n",
    "print('word testX ->', testX[ind])\n",
    "print('word testY ->', testY[ind])\n",
    "print('word test_len ->', str(test_len[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Upload data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to upload the training dataset to S3 in order for our training code to access it. For now we will save both training and test locally and we will upload to S3 later on.\n",
    "\n",
    "It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form output[34], length, input[34] where input[34] represents the word, which is a sequence of 34 integers the letters in the words.\n",
    "\n",
    "We will save the training CSV and both dictionaries on the model directory, to upload later together in s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_dir): # Make sure that the folder exists if not, create it\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "pd.concat([pd.DataFrame(trainY), pd.DataFrame(train_len), pd.DataFrame(trainX)], axis=1) \\\n",
    "        .to_csv(os.path.join(model_dir, 'train.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([pd.DataFrame(testY), pd.DataFrame(test_len), pd.DataFrame(testX)], axis=1) \\\n",
    "        .to_csv(os.path.join(cache_dir, 'test.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, when we construct an endpoint which processes a submitted review we will need to make use of the dictionaries which we have created. As such, we will save them to a file now for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, 'letter2int_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(letter2int, f)\n",
    "with open(os.path.join(model_dir, 'int2letter_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(int2letter, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to load the dictionaries for any case from the files, it can be done using the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from file: letter2int_dict.pkl\n",
      "Read data from file: int2letter_dict.pkl\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "def load_dictionary(dict_file, dict_dir = cache_dir):\n",
    "    cache_data = None\n",
    "    file_dir = os.path.join(dict_dir, dict_file)\n",
    "\n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read data from file:\", dict_file)\n",
    "        except:\n",
    "            print('Problem reading file', dict_file)\n",
    "    else:\n",
    "        print('File empty')\n",
    "\n",
    "    if cache_data is None:\n",
    "        print('Didnt read anything')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return cache_data\n",
    "\n",
    "# For example\n",
    "letter2int = load_dictionary('letter2int_dict.pkl', model_dir)\n",
    "int2letter = load_dictionary('int2letter_dict.pkl', model_dir)\n",
    "\n",
    "print(letter2int)\n",
    "print(int2letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = None\n",
    "testX = None\n",
    "trainY = None\n",
    "testY = None\n",
    "train_len = None\n",
    "test_len = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 To S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to upload the training data and the dictionaries to the SageMaker default S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-670005714529/sagemaker/capstoneProject\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/capstoneProject'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# upload training data to S3\n",
    "train_dir = os.path.join(cache_dir, 'train.csv')\n",
    "input_data = sagemaker_session.upload_data(path=model_dir, bucket=bucket, key_prefix=prefix)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you've uploaded the data, by printing the contents of the default bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-pytorch-2020-08-10-10-25-18-500/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-10-49-25-390/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-11-43-00-806/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-12-29-45-086/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-12-42-16-743/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-12-56-05-270/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-13-45-49-644/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-15-05-41-149/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-15-51-51-449/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-17-02-14-291/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-17-23-07-386/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-17-39-38-750/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-18-11-25-123/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-18-33-23-479/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-18-50-06-663/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-19-26-54-540/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-20-11-46-316/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-10-20-50-25-519/sourcedir.tar.gz\n",
      "sagemaker/capstoneProject/int2letter_dict.pkl\n",
      "sagemaker/capstoneProject/letter2int_dict.pkl\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-10-25-18-500/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-10-25-18-500/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-12-42-16-743/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-12-42-16-743/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-12-56-05-270/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-12-56-05-270/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-13-45-49-644/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-13-45-49-644/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-15-05-41-149/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-15-05-41-149/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-17-02-14-291/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-17-02-14-291/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-17-23-07-386/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-17-23-07-386/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-17-39-38-750/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-17-39-38-750/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-18-11-25-123/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-18-11-25-123/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-18-33-23-479/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-18-33-23-479/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-18-50-06-663/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-18-50-06-663/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-19-26-54-540/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-19-26-54-540/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-20-11-46-316/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-10-20-11-46-316/output/model.tar.gz\n",
      "sagemaker/capstoneProject/train.csv\n"
     ]
    }
   ],
   "source": [
    "# iterate through S3 objects and print contents\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "     print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by implementing our own neural network in PyTorch along with a training script. The necessary files for them are in the source folder, being them: train.py, model.py, predict.py and util.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mEncoder\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_dim, hid_dim):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(Encoder, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.hid_dim = hid_dim\r\n",
      "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(input_dim, hid_dim, batch_first = \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, src):\r\n",
      "        outputs, (hidden, cell) = \u001b[36mself\u001b[39;49;00m.lstm(src)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m hidden, cell\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDecoder\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_dim, output_dim, hid_dim):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(Decoder, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.hid_dim = hid_dim\r\n",
      "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(output_dim, hid_dim, batch_first = \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.output_dim = output_dim\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc = nn.Linear(hid_dim, output_dim)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, src, hidden, cell):\r\n",
      "        trg, (hidden, cell) = \u001b[36mself\u001b[39;49;00m.lstm(src, (hidden, cell))\r\n",
      "        \u001b[37m# Reshaping the outputs such that it can be fit into the fully connected layer\u001b[39;49;00m\r\n",
      "        batch_size = np.shape(trg)[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        len_size = np.shape(trg)[\u001b[34m1\u001b[39;49;00m]\r\n",
      "\r\n",
      "        out = trg.contiguous().view(-\u001b[34m1\u001b[39;49;00m, \u001b[36mself\u001b[39;49;00m.hid_dim)\r\n",
      "        out = \u001b[36mself\u001b[39;49;00m.fc(out)\r\n",
      "        out = out.view(batch_size, len_size, -\u001b[34m1\u001b[39;49;00m)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m out\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mWordOrderer\u001b[39;49;00m(nn.Module):\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, encoder, decoder):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(WordOrderer, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.encoder = encoder\r\n",
      "        \u001b[36mself\u001b[39;49;00m.decoder = decoder\r\n",
      "        \u001b[34massert\u001b[39;49;00m encoder.hid_dim == decoder.hid_dim, \u001b[33m\"\u001b[39;49;00m\u001b[33mEncoder and Decoder don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt have the same dimensions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.letter2int_dict = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.int2letter_dict = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, jumbled):\r\n",
      "        hidden, cell = \u001b[36mself\u001b[39;49;00m.encoder(jumbled)\r\n",
      "\r\n",
      "        adapted = jumbled[:, \u001b[34m1\u001b[39;49;00m:, :\u001b[36mself\u001b[39;49;00m.decoder.output_dim] \u001b[37m#as we will not pass the first element to the Decoder\u001b[39;49;00m\r\n",
      "        \u001b[37m# which was the length of the word, and so the vocabulary size gets reduced to 27\u001b[39;49;00m\r\n",
      "        word = \u001b[36mself\u001b[39;49;00m.decoder(adapted, hidden, cell)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m word\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the implementation we can observe that there are is only one parameter that we may wish to tweak to improve the performance of our model. This is the hidden dimension.\n",
    "\n",
    "We didn't do it before because it increases the memory that would have been used for the CSV files, but the letters in each word passed to the model as input will have to be encoded on one-hot vectors. This can be done with the function defined two cells below (one_hot_encode).\n",
    "\n",
    "As the input we will pass the length of the word plus the word jumbled, meaning that the one-hot encoding has to be done taking into consideration these lengths too. As the maximum length will be of 34, which is bigger than the length of the letters dictionary (which is 27 considering the 0), the input dimension of the elements will be of 34, whereas the output dimension will be of 27 as we won't need the size anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will try to implement the training function to check its correct functioning. By doing this way, we avoid the larger time it takes if we would do it directly with the Pytorch Model as the loading time it takes to train every time is considerable, and we probably would have to call it several times, trying to fix errors that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (1.6.0)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from torch) (1.18.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float64)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size read from csv -> X: (9445, 35), Y: (9445, 34), len: (9445,)\n",
      "Input shape: (9445, 35, 34) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
      "Torch X: torch.Size([9445, 35, 34]) shape, torch.float32 type\n",
      "Torch Y: torch.Size([9445, 34]) shape, torch.int64 type\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Get the arrays from csv\n",
    "train_sample = pd.read_csv(os.path.join(model_dir, 'train.csv'), header=None, names=None)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = train_sample[train_sample.columns[0:34]]\n",
    "train_sample_len = train_sample[train_sample.columns[34]]\n",
    "train_sample_X = train_sample[train_sample.columns[34:69]] #this includes the word length as the first element\n",
    "print('Size read from csv -> X: {}, Y: {}, len: {}'.format(train_sample_X.shape, train_sample_y.shape, train_sample_len.shape))\n",
    "\n",
    "X_np = train_sample_X.to_numpy(copy=True)\n",
    "Y_np = train_sample_y.to_numpy(copy=True)\n",
    "len_np = train_sample_len.to_numpy(copy=True)\n",
    "\n",
    "# Encode the input sentence as one hot vectors\n",
    "dict_size = 34\n",
    "seq_len = 35\n",
    "batch_size = len(train_sample_X)\n",
    "input_seq = one_hot_encode(X_np, dict_size, seq_len, batch_size)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
    "\n",
    "train_torch_x = torch.tensor(input_seq).float().squeeze().clone()\n",
    "train_torch_x = torch.autograd.Variable(train_torch_x)\n",
    "train_torch_len = torch.tensor(len_np).float().squeeze().type(torch.long).clone()\n",
    "train_torch_y = torch.tensor(Y_np).float().squeeze().type(torch.long).clone()\n",
    "print('Torch X: {} shape, {} type\\nTorch Y: {} shape, {} type'.format(train_torch_x.shape, train_torch_x.dtype, \n",
    "                                                                      train_torch_y.shape, train_torch_y.dtype))\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_torch_x, train_torch_y, train_torch_len)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preparation and arguments as if it was the file\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_dim = 34\n",
    "output_dim = 27\n",
    "hid_dim = 128\n",
    "epochs = 5\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    total_length = len(train_loader.dataset)\n",
    "    model.train()\n",
    "    loss_return = []\n",
    "    increased = 0\n",
    "    loss_previous = 0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        batchs_done = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch_X, batch_y, batch_len = batch\n",
    "            len_batch = len(batch_X)\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            out = model(batch_X)\n",
    "            \n",
    "            batch_loss = 0\n",
    "            for result, target, len_word in zip(out, batch_y, batch_len):\n",
    "                loss = loss_fn(result[:len_word, :], target[:len_word])\n",
    "                batch_loss += loss\n",
    "    \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += batch_loss.data.item()        \n",
    "            batchs_done += len_batch\n",
    "#             print('Batch done. {} / {} inputs = {}%'.format(\n",
    "#                 batchs_done, total_length, np.round(batchs_done/total_length*100, decimals = 1)))\n",
    "\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))\n",
    "        loss_return.append(total_loss / len(train_loader))\n",
    "\n",
    "        # early stopping\n",
    "        if total_loss > loss_previous:\n",
    "            increased += 1\n",
    "            print('Increased ({})'.format(increased))\n",
    "        else: \n",
    "            increased = 0\n",
    "\n",
    "        loss_previous = total_loss\n",
    "\n",
    "        if increased >= 3:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "        \n",
    "    return loss_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 381.15864191828547\n",
      "Increased (1)\n",
      "Epoch: 2, Loss: 342.50367221316776\n",
      "Epoch: 3, Loss: 265.5940826003616\n",
      "Epoch: 4, Loss: 236.2553659387537\n",
      "Epoch: 5, Loss: 228.18131977802997\n"
     ]
    }
   ],
   "source": [
    "from source.model import WordOrderer, Decoder, Encoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(input_dim, hid_dim)\n",
    "decoder = Decoder(input_dim, output_dim, hid_dim)\n",
    "\n",
    "model = WordOrderer(encoder, decoder).to(device)\n",
    "\n",
    "# Train the model.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "losses = train(model, train_sample_dl, epochs, optimizer, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can make a plot to observe better\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although slow, we can observe that the model works and seems to improve over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will clean some variables that will not be used again\n",
    "train_sample = None\n",
    "train_sample_y = None\n",
    "train_sample_len = None\n",
    "train_sample_X = None\n",
    "input_seq = None\n",
    "X_np = None\n",
    "Y_np = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build and Train the PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training script works correctly, we will copy adequately on the train.py file.\n",
    "\n",
    "A typical training script:\n",
    "\n",
    "- Loads training data from a specified directory\n",
    "- Parses any training & model hyperparameters (ex. nodes in a neural network, training epochs, etc.)\n",
    "- Instantiates a model of your design, with any specified hyperparams\n",
    "- Trains that model\n",
    "- Finally, saves the model so that it can be hosted/deployed, later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m one_hot_encode\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m WordOrderer, Decoder, Encoder\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mobtain_data\u001b[39;49;00m(data_dir, namefile, batch_s):\n",
      "    \u001b[37m# Load the training data.\u001b[39;49;00m\n",
      "    train_sample = pd.read_csv(os.path.join(data_dir, namefile), header=\u001b[34mNone\u001b[39;49;00m, names=\u001b[34mNone\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoaded csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    train_sample_y = train_sample[train_sample.columns[\u001b[34m0\u001b[39;49;00m:\u001b[34m34\u001b[39;49;00m]]\n",
      "    train_sample_len = train_sample[train_sample.columns[\u001b[34m34\u001b[39;49;00m]]\n",
      "    train_sample_X = train_sample[train_sample.columns[\u001b[34m34\u001b[39;49;00m:\u001b[34m69\u001b[39;49;00m]]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSize read from csv -> X: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, Y: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, len: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_sample_X.shape, train_sample_len.shape, train_sample_y.shape))\n",
      "\n",
      "    X_np = train_sample_X.to_numpy(copy=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    len_np = train_sample_len.to_numpy(copy=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    Y_np = train_sample_y.to_numpy(copy=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTo numpied\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    dict_size = \u001b[34m34\u001b[39;49;00m\n",
      "    seq_len = \u001b[34m35\u001b[39;49;00m\n",
      "    batch_size = \u001b[36mlen\u001b[39;49;00m(train_sample_X)\n",
      "    input_seq = one_hot_encode(X_np, dict_size, seq_len, batch_size)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mOne hot encoded\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    train_torch_x = torch.from_numpy(input_seq).float().squeeze()\n",
      "    train_torch_len = torch.from_numpy(len_np).float().squeeze().type(torch.long)\n",
      "    train_torch_y = torch.from_numpy(Y_np).float().squeeze().type(torch.long)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTorched\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    train_sample_ds = torch.utils.data.TensorDataset(train_torch_x, train_torch_y, train_torch_len)\n",
      "    train_loader = torch.utils.data.DataLoader(train_sample_ds, batch_size=batch_s)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain loaded\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_loader\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(model, train_loader, epochs, optimizer, loss_fn, device):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStart training\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    total_length = \u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\n",
      "    model.train()\n",
      "    loss_return = []\n",
      "    increased = \u001b[34m0\u001b[39;49;00m\n",
      "    loss_previous = \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        batchs_done = \u001b[34m0\u001b[39;49;00m\n",
      "        total_loss = \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mfor\u001b[39;49;00m batch \u001b[35min\u001b[39;49;00m train_loader:\n",
      "            batch_X, batch_y, batch_len = batch\n",
      "            len_batch = \u001b[36mlen\u001b[39;49;00m(batch_X)\n",
      "\n",
      "            batch_X = batch_X.to(device)\n",
      "            batch_y = batch_y.to(device)\n",
      "\n",
      "            out = model(batch_X)\n",
      "\n",
      "            batch_loss = \u001b[34m0\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m result, target, len_word \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(out, batch_y, batch_len):\n",
      "                loss = loss_fn(result[:len_word, :], target[:len_word])\n",
      "                batch_loss += loss\n",
      "\n",
      "            batch_loss.backward()\n",
      "            optimizer.step()\n",
      "\n",
      "            total_loss += batch_loss.data.item()\n",
      "            batchs_done += len_batch\n",
      "\u001b[37m#             print('Batch done. {} / {} inputs = {}%'.format(\u001b[39;49;00m\n",
      "\u001b[37m#                 batchs_done, total_length, np.round(batchs_done/total_length*100, decimals = 1)))\u001b[39;49;00m\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, Loss: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epoch, total_loss / \u001b[36mlen\u001b[39;49;00m(train_loader)))\n",
      "        loss_return.append(total_loss / \u001b[36mlen\u001b[39;49;00m(train_loader))\n",
      "\n",
      "        \u001b[37m# early stopping\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m total_loss > loss_previous:\n",
      "            increased += \u001b[34m1\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mIncreased (\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(increased))\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            increased = \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "        loss_previous = total_loss\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m increased >= \u001b[34m3\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEarly stopping!\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m loss_return\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "    \u001b[37m# SageMaker Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \u001b[37m# Training Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m128\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mB\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 128)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m42\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 42)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[37m# Model Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m27\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33msize of the input dimension (default: 27)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m27\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mOD\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33msize of the output dimension (default: 27)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mHD\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33msize of the hidden dimension (default: 34)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "\n",
      "    torch.manual_seed(args.seed)\n",
      "\n",
      "    \u001b[37m#load dictionaries\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mletter2int_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        letter2int = pickle.load(f)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mint2letter_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        int2letter = pickle.load(f)\n",
      "\n",
      "    train_loader = obtain_data(args.data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.batch_size)\n",
      "\n",
      "    \u001b[37m# Build the model.\u001b[39;49;00m\n",
      "    encoder = Encoder(args.input_dim, args.hidden_dim)\n",
      "    decoder = Decoder(args.input_dim, args.output_dim, args.hidden_dim)\n",
      "    model = WordOrderer(encoder, decoder).to(device)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mint2letter_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.letter2int_dict = pickle.load(f)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mletter2int_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.int2letter_dict = pickle.load(f)\n",
      "\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel loaded with input_dim \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, output_dim \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, hidden_dim \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_dim, args.output_dim, args.hidden_dim))\n",
      "\n",
      "    \u001b[37m# Train the model.\u001b[39;49;00m\n",
      "    loss_function = nn.CrossEntropyLoss()\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mGoing to train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    train(model, train_loader, args.epochs, optimizer, loss_function, device)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrained\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Save the parameters used to construct the model\u001b[39;49;00m\n",
      "    model_info_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = {\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.input_dim,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33moutput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.output_dim,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.hidden_dim,\n",
      "        }\n",
      "        torch.save(model_info, f)\n",
      "\n",
      "\t\u001b[37m# Save the two letter2int_dict\u001b[39;49;00m\n",
      "    letter2int_dict_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mint2letter_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(letter2int_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        pickle.dump(model.letter2int_dict, f)\n",
      "\n",
      "    letter2int_dict_path2 = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mletter2int_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(letter2int_dict_path2, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        pickle.dump(model.int2letter_dict, f)\n",
      "\n",
      "\t\u001b[37m# Save the model parameters\u001b[39;49;00m\n",
      "    model_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        torch.save(model.cpu().state_dict(), f)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Create Pytorch Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point = 'train.py',\n",
    "                    source_dir = 'source',\n",
    "                    role = role,\n",
    "                    framework_version = '1.0',\n",
    "                    train_instance_count = 1,\n",
    "                    train_instance_type = 'ml.c4.xlarge',\n",
    "                    output_path = output_path,\n",
    "                    sagemaker_session = sagemaker_session,\n",
    "                    hyperparameters = {\n",
    "                        'input_dim': 34,\n",
    "                        'output_dim': 27,\n",
    "                        'hidden_dim': 156,\n",
    "                        'epochs': 90,\n",
    "                        'lr': 0.0005,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As early stopping has been implemented, we try with a large number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Train the Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time, take it easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-10 21:45:59 Starting - Starting the training job...\n",
      "2020-08-10 21:46:00 Starting - Launching requested ML instances......\n",
      "2020-08-10 21:47:06 Starting - Preparing the instances for training......\n",
      "2020-08-10 21:48:24 Downloading - Downloading input data\n",
      "2020-08-10 21:48:24 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-08-10 21:48:24,923 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-08-10 21:48:24,926 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-10 21:48:24,938 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-08-10 21:48:24,939 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-08-10 21:48:25,173 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-08-10 21:48:25,173 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-08-10 21:48:25,173 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-08-10 21:48:25,174 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pip (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/bd/b1/56a834acdbe23b486dea16aaf4c27ed28eb292695b90d01dff96c96597de/pip-20.2.1-py2.py3-none-any.whl (1.5MB)\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/f7/2adca20a7fa71b6a32f823bbd83992adeceab1d8bf72992bb7a55c69c19a/pandas-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/22/e7/4b2bdddb99f5f631d8c1de259897c2b7d65dcfcc1e0a6fd17a7f62923500/numpy-1.19.1-cp36-cp36m-manylinux1_x86_64.whl (13.4MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/66/25/ff030e2437265616a1e9b25ccc864e0371a0bc3adb7c5a404fd661c6f4f6/beautifulsoup4-4.9.1-py3-none-any.whl (115kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 2)) (2019.1)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2 (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 2)) (1.12.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-57c_i9ys/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pip, numpy, pandas, soupsieve, beautifulsoup4, train\n",
      "  Found existing installation: pip 18.1\n",
      "    Uninstalling pip-18.1:\n",
      "      Successfully uninstalled pip-18.1\u001b[0m\n",
      "\u001b[34m  Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.16.4\u001b[0m\n",
      "\u001b[34m  Found existing installation: pandas 0.24.2\u001b[0m\n",
      "\u001b[34m    Uninstalling pandas-0.24.2:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled pandas-0.24.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed beautifulsoup4-4.9.1 numpy-1.19.1 pandas-1.1.0 pip-20.2.1 soupsieve-2.0.1 train-1.0.0\u001b[0m\n",
      "\u001b[34m2020-08-10 21:48:38,284 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-10 21:48:38,296 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"input_dim\": 34,\n",
      "        \"hidden_dim\": 156,\n",
      "        \"lr\": 0.0005,\n",
      "        \"epochs\": 90,\n",
      "        \"output_dim\": 27\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-08-10-21-45-58-872\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-670005714529/sagemaker-pytorch-2020-08-10-21-45-58-872/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":90,\"hidden_dim\":156,\"input_dim\":34,\"lr\":0.0005,\"output_dim\":27}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-670005714529/sagemaker-pytorch-2020-08-10-21-45-58-872/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":90,\"hidden_dim\":156,\"input_dim\":34,\"lr\":0.0005,\"output_dim\":27},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2020-08-10-21-45-58-872\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-670005714529/sagemaker-pytorch-2020-08-10-21-45-58-872/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"90\",\"--hidden_dim\",\"156\",\"--input_dim\",\"34\",\"--lr\",\"0.0005\",\"--output_dim\",\"27\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_DIM=34\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=156\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0005\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=90\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIM=27\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 90 --hidden_dim 156 --input_dim 34 --lr 0.0005 --output_dim 27\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mLoaded csv\u001b[0m\n",
      "\u001b[34mSize read from csv -> X: (9445, 35), Y: (9445,), len: (9445, 34)\u001b[0m\n",
      "\u001b[34mTo numpied\u001b[0m\n",
      "\u001b[34mOne hot encoded\u001b[0m\n",
      "\u001b[34mTorched\u001b[0m\n",
      "\u001b[34mTrain loaded\u001b[0m\n",
      "\u001b[34mModels\u001b[0m\n",
      "\u001b[34mModel loaded with input_dim 34, output_dim 27, hidden_dim 156.\u001b[0m\n",
      "\u001b[34mGoing to train\u001b[0m\n",
      "\u001b[34mStart training\u001b[0m\n",
      "\u001b[34mEpoch: 1, Loss: 391.19019998086463\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 2, Loss: 367.3071392162426\u001b[0m\n",
      "\u001b[34mEpoch: 3, Loss: 351.0043000917177\u001b[0m\n",
      "\u001b[34mEpoch: 4, Loss: 328.2880523269241\u001b[0m\n",
      "\u001b[34mEpoch: 5, Loss: 293.44381631387245\u001b[0m\n",
      "\u001b[34mEpoch: 6, Loss: 250.0314949654244\u001b[0m\n",
      "\u001b[34mEpoch: 7, Loss: 228.86084169954867\u001b[0m\n",
      "\u001b[34mEpoch: 8, Loss: 218.8308338474583\u001b[0m\n",
      "\u001b[34mEpoch: 9, Loss: 212.21548709353885\u001b[0m\n",
      "\u001b[34mEpoch: 10, Loss: 206.03216325914536\u001b[0m\n",
      "\u001b[34mEpoch: 11, Loss: 202.06174881393844\u001b[0m\n",
      "\u001b[34mEpoch: 12, Loss: 199.20506740260768\u001b[0m\n",
      "\u001b[34mEpoch: 13, Loss: 197.4320961204735\u001b[0m\n",
      "\u001b[34mEpoch: 14, Loss: 192.24060945253115\u001b[0m\n",
      "\u001b[34mEpoch: 15, Loss: 188.8635680739944\u001b[0m\n",
      "\u001b[34mEpoch: 16, Loss: 187.1367239049963\u001b[0m\n",
      "\u001b[34mEpoch: 17, Loss: 184.40757277205185\u001b[0m\n",
      "\u001b[34mEpoch: 18, Loss: 182.66450046848607\u001b[0m\n",
      "\u001b[34mEpoch: 19, Loss: 180.00628074439797\u001b[0m\n",
      "\u001b[34mEpoch: 20, Loss: 177.9031535999195\u001b[0m\n",
      "\u001b[34mEpoch: 21, Loss: 176.2959712260478\u001b[0m\n",
      "\u001b[34mEpoch: 22, Loss: 174.60495056977143\u001b[0m\n",
      "\u001b[34mEpoch: 23, Loss: 173.35158291378536\u001b[0m\n",
      "\u001b[34mEpoch: 24, Loss: 171.83807197777\u001b[0m\n",
      "\u001b[34mEpoch: 25, Loss: 171.06624479551573\u001b[0m\n",
      "\u001b[34mEpoch: 26, Loss: 169.4278742815997\u001b[0m\n",
      "\u001b[34mEpoch: 27, Loss: 168.1062504536397\u001b[0m\n",
      "\u001b[34mEpoch: 28, Loss: 167.1012879448968\u001b[0m\n",
      "\u001b[34mEpoch: 29, Loss: 166.20942780778213\u001b[0m\n",
      "\u001b[34mEpoch: 30, Loss: 165.67949820853568\u001b[0m\n",
      "\u001b[34mEpoch: 31, Loss: 164.44898945576435\u001b[0m\n",
      "\u001b[34mEpoch: 32, Loss: 163.17843885679503\u001b[0m\n",
      "\u001b[34mEpoch: 33, Loss: 162.9302255785143\u001b[0m\n",
      "\u001b[34mEpoch: 34, Loss: 162.19989755991344\u001b[0m\n",
      "\u001b[34mEpoch: 35, Loss: 161.31057481507997\u001b[0m\n",
      "\u001b[34mEpoch: 36, Loss: 160.20142900621568\u001b[0m\n",
      "\u001b[34mEpoch: 37, Loss: 159.97078003754487\u001b[0m\n",
      "\u001b[34mEpoch: 38, Loss: 159.29772186279297\u001b[0m\n",
      "\u001b[34mEpoch: 39, Loss: 158.91272818075643\u001b[0m\n",
      "\u001b[34mEpoch: 40, Loss: 157.10713597890492\u001b[0m\n",
      "\u001b[34mEpoch: 41, Loss: 157.38717847257047\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 42, Loss: 156.70828607920055\u001b[0m\n",
      "\u001b[34mEpoch: 43, Loss: 155.87632473095044\u001b[0m\n",
      "\u001b[34mEpoch: 44, Loss: 155.31444900100297\u001b[0m\n",
      "\u001b[34mEpoch: 45, Loss: 154.61339197932062\u001b[0m\n",
      "\u001b[34mEpoch: 46, Loss: 154.28697637609534\u001b[0m\n",
      "\u001b[34mEpoch: 47, Loss: 154.12406364647117\u001b[0m\n",
      "\u001b[34mEpoch: 48, Loss: 153.5548645225731\u001b[0m\n",
      "\u001b[34mEpoch: 49, Loss: 152.10404246562237\u001b[0m\n",
      "\u001b[34mEpoch: 50, Loss: 153.3762060629355\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 51, Loss: 152.24166725777292\u001b[0m\n",
      "\u001b[34mEpoch: 52, Loss: 152.22168772929425\u001b[0m\n",
      "\u001b[34mEpoch: 53, Loss: 150.84374639150258\u001b[0m\n",
      "\u001b[34mEpoch: 54, Loss: 150.693713729446\u001b[0m\n",
      "\u001b[34mEpoch: 55, Loss: 150.5734156015757\u001b[0m\n",
      "\u001b[34mEpoch: 56, Loss: 151.10125113822318\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 57, Loss: 149.3127481615221\u001b[0m\n",
      "\u001b[34mEpoch: 58, Loss: 149.44425789085594\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 59, Loss: 149.63821947252427\u001b[0m\n",
      "\u001b[34mIncreased (2)\u001b[0m\n",
      "\u001b[34mEpoch: 60, Loss: 149.36582472517685\u001b[0m\n",
      "\u001b[34mEpoch: 61, Loss: 148.02807823387352\u001b[0m\n",
      "\u001b[34mEpoch: 62, Loss: 147.41365948238888\u001b[0m\n",
      "\u001b[34mEpoch: 63, Loss: 147.8927380329854\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 64, Loss: 147.88669245951886\u001b[0m\n",
      "\u001b[34mEpoch: 65, Loss: 147.9845875920476\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 66, Loss: 146.4491496730495\u001b[0m\n",
      "\u001b[34mEpoch: 67, Loss: 147.54704563037768\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 68, Loss: 146.96022528571052\u001b[0m\n",
      "\u001b[34mEpoch: 69, Loss: 147.00804210353542\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 70, Loss: 145.09805978311076\u001b[0m\n",
      "\u001b[34mEpoch: 71, Loss: 146.7947353672337\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 72, Loss: 147.05447356765336\u001b[0m\n",
      "\u001b[34mIncreased (2)\u001b[0m\n",
      "\u001b[34mEpoch: 73, Loss: 147.21031209584828\u001b[0m\n",
      "\u001b[34mIncreased (3)\u001b[0m\n",
      "\u001b[34mEarly stopping!\u001b[0m\n",
      "\u001b[34mTrained\u001b[0m\n",
      "\u001b[34m2020-08-10 22:09:57,217 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-08-10 22:10:08 Uploading - Uploading generated training model\n",
      "2020-08-10 22:10:08 Completed - Training job completed\n",
      "Training seconds: 1324\n",
      "Billable seconds: 1324\n",
      "CPU times: user 3.26 s, sys: 86.8 ms, total: 3.35 s\n",
      "Wall time: 24min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# train the estimator on S3 training data\n",
    "estimator.fit({'train': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Prediction with the trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Getting letters back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function that allows obtaining the words back from the padded version to the shorter integer version, and from this integer version to the string version using the dictionaries. This will be done in the following functions.\n",
    "\n",
    "If the model returns all 0's (in the case it had a bad performance). The word will consist of a dot.\n",
    "\n",
    "Also, this will be useful if we want to show how the words in the sentence have been jumbled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer2word(number_dict, word, len_word):\n",
    "    \n",
    "    word_back = []\n",
    "    for letter_index, letter in enumerate(word):\n",
    "        if letter_index < len_word:\n",
    "            if letter in number_dict:\n",
    "                word_back.append(number_dict[letter])\n",
    "            else:\n",
    "                word_back.append('.')\n",
    "                        \n",
    "    return word_back\n",
    "\n",
    "\n",
    "def integer2sentence(number_dict, data, len_data):\n",
    "    result = []\n",
    "    \n",
    "    perc=0\n",
    "    idx_w = 0\n",
    "    for word, len_w in zip(data, len_data):\n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} words = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "\n",
    "        converted_word = integer2word(number_dict, word, len_w)\n",
    "\n",
    "        result.append(converted_word)\n",
    "        idx_w += 1\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the input training data to check and we will see how the sentence would be seen once jumbled\n",
    "def join_sentence(sentence):\n",
    "    list_words = []\n",
    "    for word in sentence:\n",
    "        w = ''.join(word)\n",
    "        list_words.append(w)\n",
    "\n",
    "    sentence_joined = ' '.join(list_words)\n",
    "    \n",
    "    return sentence_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10 words = 0%\n",
      "1 / 10 words = 10.0%\n",
      "2 / 10 words = 20.0%\n",
      "4 / 10 words = 30.0%\n",
      "5 / 10 words = 40.0%\n",
      "6 / 10 words = 50.0%\n",
      "7 / 10 words = 60.0%\n",
      "8 / 10 words = 70.0%\n",
      "9 / 10 words = 80.0%\n",
      "0 / 10 words = 0%\n",
      "1 / 10 words = 10.0%\n",
      "2 / 10 words = 20.0%\n",
      "4 / 10 words = 30.0%\n",
      "5 / 10 words = 40.0%\n",
      "6 / 10 words = 50.0%\n",
      "7 / 10 words = 60.0%\n",
      "8 / 10 words = 70.0%\n",
      "9 / 10 words = 80.0%\n",
      "taht have wtih tihs just like abuot ulrlnik tehy form \n",
      "<--> \n",
      "that have with this just like about urllink they from\n"
     ]
    }
   ],
   "source": [
    "# We will use the test dataset preparated, and assume that the sentence is the first 10 words of the file.\n",
    "test_sample = pd.read_csv(os.path.join(model_dir, 'train.csv'), header=None, names=None, nrows=10)\n",
    "test_sample_x = test_sample[test_sample.columns[34:69]].to_numpy(copy=True)\n",
    "test_sample_len = test_sample[test_sample.columns[34]].to_numpy(copy=True)\n",
    "test_sample_y = test_sample[test_sample.columns[0:34]].to_numpy(copy=True)\n",
    "\n",
    "# Check one sentence, we will consider 5 consecutive words from the training data as a sentence\n",
    "input_words = integer2sentence(int2letter, test_sample_x[:,1:], test_sample_len)\n",
    "output_words = integer2sentence(int2letter, test_sample_y, test_sample_len)\n",
    "\n",
    "print('{} \\n<--> \\n{}'.format(join_sentence(input_words), join_sentence(output_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Predict Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same as we did with the training script to check it works first. The input will be the word already transformed into the integers array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict the sentence as a string, we will transform the sentence into its integer jumbled form with the prepare_predict function. And after the prediction, we will have to transform them back with the read_prediction function, as they will expect to receive the same type of data they sended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_predict(sentence, dictionary):\n",
    "    input_data_words = post_to_words(sentence)\n",
    "    \n",
    "    print('Converting data')\n",
    "    integer_sentence, len_sentence = sentence2integer(dictionary, input_data_words)\n",
    "    print('Jumbling data')\n",
    "    jumbled_sentence = jumble_data(integer_sentence, len_sentence)\n",
    "    \n",
    "    #words with the length previously\n",
    "    for word, leng in zip(jumbled_sentence, len_sentence):\n",
    "        word.insert(0, leng)\n",
    "\n",
    "    return integer_sentence, jumbled_sentence, len_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prediction(prediction, lengths, dictionary):\n",
    "    return join_sentence(integer2sentence(dictionary, prediction, lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are less than 4 letters, will be returned as such without passing through the model, as no transformation is applied.\n",
    "\n",
    "The function will check if the input is a string array or an integer array, and in the case of the string, it will apply the functions necessary to have the adequate type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_input, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    integer_sentence = [] \n",
    "    for word in data_input:\n",
    "        word_batch = [word]\n",
    "\n",
    "        if len(word) > 3:\n",
    "            dict_size = 34\n",
    "            seq_len = 35\n",
    "            batch_size =1\n",
    "            test_seq = one_hot_encode(word_batch, dict_size, seq_len, batch_size)\n",
    "            \n",
    "            data = torch.from_numpy(test_seq).float().squeeze().to(device)\n",
    "            # Have the torch as a batch of size 1\n",
    "            data_batch = data.view(1, np.shape(data)[0], np.shape(data)[1])\n",
    "            # Make sure to put the model into evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model.forward(data_batch)\n",
    "                \n",
    "                word_integer = []\n",
    "                for letter in output[0]: #as there's only 1 batch\n",
    "                    letter_numpy = letter.numpy()\n",
    "                    max_value_ind = np.argmax(letter_numpy, axis=0)\n",
    "                    word_integer.append(max_value_ind)\n",
    "                \n",
    "        else:\n",
    "            word_integer = word_batch.copy()\n",
    "            \n",
    "        integer_sentence.append(word_integer)\n",
    "        \n",
    "    return integer_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10 words = 0%\n",
      "1 / 10 words = 10.0%\n",
      "2 / 10 words = 20.0%\n",
      "4 / 10 words = 30.0%\n",
      "5 / 10 words = 40.0%\n",
      "6 / 10 words = 50.0%\n",
      "7 / 10 words = 60.0%\n",
      "8 / 10 words = 70.0%\n",
      "9 / 10 words = 80.0%\n",
      "(1st word) Result  -> Ground Truth:\n",
      "[20, 1, 1, 20, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "->\n",
      "[20  8  1 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "Resulting sentence: taat have weth tees jugt leke abuot ulrllik teey form\n"
     ]
    }
   ],
   "source": [
    "int_result = predict(test_sample_x, model)\n",
    "str_result = read_prediction(int_result, test_sample_len, int2letter)\n",
    "print('(1st word) Result  -> Ground Truth:')\n",
    "print(int_result[0])\n",
    "print('->')\n",
    "print(test_sample_y[0])\n",
    "print('\\nResulting sentence: {}'.format(str_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to work correctly (ignoring the model performance). So on the next step now we will try it with a sentence string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data\n",
      "0 / 8 word = 0%\n",
      "1 / 8 word = 10.0%\n",
      "2 / 8 word = 20.0%\n",
      "3 / 8 word = 30.0%\n",
      "4 / 8 word = 40.0%\n",
      "5 / 8 word = 50.0%\n",
      "6 / 8 word = 60.0%\n",
      "7 / 8 word = 70.0%\n",
      "Jumbling data\n",
      "0 / 8 words = 0%\n",
      "1 / 8 words = 10.0%\n",
      "2 / 8 words = 20.0%\n",
      "3 / 8 words = 30.0%\n",
      "4 / 8 words = 40.0%\n",
      "5 / 8 words = 50.0%\n",
      "6 / 8 words = 60.0%\n",
      "7 / 8 words = 70.0%\n",
      "0 / 8 words = 0%\n",
      "1 / 8 words = 10.0%\n",
      "2 / 8 words = 20.0%\n",
      "3 / 8 words = 30.0%\n",
      "4 / 8 words = 40.0%\n",
      "5 / 8 words = 50.0%\n",
      "6 / 8 words = 60.0%\n",
      "7 / 8 words = 70.0%\n",
      "\n",
      "Resulting array: [[1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [19, 20, 4, 9, 25, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [6, 18, 1, 13, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [3, 1, 13, 7, 18, 2, 9, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [8, 1, 19, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [19, 1, 20, 20, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [20, 8, 1, 20, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [5, 5, 5, 5, 20, 20, 18, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]]\n",
      "\n",
      "Resulting string: a stdiy fram camgrbiae has satted that eeeettra\n",
      "\n",
      "Length original sentence was 8, length of returned sentence is 8\n"
     ]
    }
   ],
   "source": [
    "s = 'A study from Cambridge, has stated that etcetera'\n",
    "\n",
    "original_s_int, jumbled_s_int, data_len = prepare_predict(s, letter2int)\n",
    "output_array = predict(jumbled_s_int, model)\n",
    "output_string = read_prediction(output_array, data_len, int2letter)\n",
    "\n",
    "print('\\nResulting array:', output_array)\n",
    "print('\\nResulting string:', output_string)\n",
    "print('\\nLength original sentence was {}, length of returned sentence is {}'.format(len(s.split(' ')), \n",
    "                                                                                    len(output_string.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can observe that it works fine, the result depend on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Predict Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can deploy this custom PyTorch model, we have to take one more step: creating a PyTorchModel. This model is responsible for knowing how to execute a specific predict.py script and it is what we'll deploy to create an endpoint.\n",
    "\n",
    "Also don't forget to copy the predict function into the predict.py file and that there's a utils.py file, which contains the functions necessary to make all the others work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 ms, sys: 6 µs, total: 16.5 ms\n",
      "Wall time: 45.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# importing PyTorchModel\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Create a model from the trained estimator data\n",
    "# And point to the prediction script\n",
    "pmodel = PyTorchModel(model_data = estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version = '1.0',\n",
    "                     source_dir = 'source',\n",
    "                     entry_point = 'predict.py',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Deploy trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "Using already existing model: sagemaker-pytorch-2020-08-10-22-10-36-834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!CPU times: user 258 ms, sys: 12.2 ms, total: 270 ms\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# deploy and create a predictor\n",
    "predictor = pmodel.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is deployed, we can see how it performs when applied to the test data.\n",
    "\n",
    "The provided function below takes in a deployed predictor and the sentence and returns two metrics: the accuracy of words which has been fully correctly reconstructed and the accuracy of letters that have been positioned correctly in the sentence.\n",
    "\n",
    "Also if you want to try it with the predict function written on 4.2, you only have to replace the predictor.predict for a predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "      \n",
    "#Helping function\n",
    "def equalArrays(arr1, arr2):\n",
    "    equal = True\n",
    "    for i1, i2 in zip(arr1, arr2):\n",
    "        if i1!=i2:\n",
    "            equal = False\n",
    "    return equal\n",
    "\n",
    "# code to evaluate the endpoint on test data\n",
    "def evaluate(predictor, sentence, ground_truth, lengths, verbose=True):\n",
    "    \"\"\" Evaluate a model on a test set given the prediction endpoint.\"\"\"\n",
    "    \n",
    "    input_data = pd.DataFrame(sentence)\n",
    "    predict_array = predictor.predict(sentence)\n",
    "#     predict_array = predict(sentence, model)\n",
    "       \n",
    "    # Both sentences have to have the same number of words\n",
    "    if len(sentence) != len(ground_truth):\n",
    "        print('error in the length obtained')\n",
    "        return 0\n",
    "    \n",
    "    correct_w = 0\n",
    "    all_results = []\n",
    "    all_original = []\n",
    "    for original, result, length in zip(ground_truth, predict_array, lengths):\n",
    "        if equalArrays(original[:length], result[:length]) and length!=0:\n",
    "            correct_w += 1\n",
    "        for lo, lr in zip(original[:length], result[:length]):\n",
    "            all_results.append(lr)\n",
    "            all_original.append(lo)\n",
    "                \n",
    "            \n",
    "    accuracy_w = correct_w / len(sentence)\n",
    "#     accuracy_l = correct_l / num_letters\n",
    "    acc_l = accuracy_score(all_original, all_results)\n",
    "    cosine = np.mean(cosine_similarity(predict_array,ground_truth)[0])\n",
    "    cm = confusion_matrix(all_original, all_results)\n",
    "\n",
    "    return accuracy_w, acc_l, cm, cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to read the test.csv file to obtain all the test words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   0   1   2   3   4   5   6   7   8   ...  25  26  27  28  29  30  31  \\\n",
      "0   4   4   5  12  19   5   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "\n",
      "   32  33  34  \n",
      "0   0   0   0  \n",
      "\n",
      "[1 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "evaluate_sample = pd.read_csv(os.path.join(cache_dir, 'test.csv'), header=None, names=None, nrows=1)\n",
    "eval_sample_x = evaluate_sample[evaluate_sample.columns[34:69]].to_numpy(copy=True)\n",
    "eval_sample_len = evaluate_sample[evaluate_sample.columns[34]].to_numpy(copy=True)\n",
    "\n",
    "test_data = pd.concat([pd.DataFrame(eval_sample_len), pd.DataFrame(eval_sample_x)], axis=1)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5, 12, 19,  5,  5,  5,  5,  5,  5,  5,  5,  5,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it gives error if the dataframe is of more than 5000 elements, as it takes to much time to predict, we divide the test set into 4 dataframes of 5000 and predict all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For elements 0-5000, (5000, 69)\n",
      "Word Accuracy: 0.299\n",
      "Letter Accuracy: 0.6254250026842275\n",
      "Mean Cosine Distance: 0.5919297003612474\n",
      "For elements 5000-10000, (5000, 69)\n",
      "Word Accuracy: 0.302\n",
      "Letter Accuracy: 0.6309523809523809\n",
      "Mean Cosine Distance: 0.5361515785231741\n",
      "For elements 10000-15000, (5000, 69)\n",
      "Word Accuracy: 0.3024\n",
      "Letter Accuracy: 0.6309820493372341\n",
      "Mean Cosine Distance: 0.46211771126858187\n",
      "For elements 15000-20000, (5000, 69)\n",
      "Word Accuracy: 0.3058\n",
      "Letter Accuracy: 0.6323941637317617\n",
      "Mean Cosine Distance: 0.45579107400861474\n",
      "\n",
      "Total results:\n",
      "Word Accuracy: 0.3023\n",
      "Letter Accuracy: 0.629938399176401\n",
      "Mean Cosine Distance: 0.5114975160404045\n"
     ]
    }
   ],
   "source": [
    "evaluate_sample = pd.read_csv(os.path.join(cache_dir, 'test.csv'), header=None, names=None)\n",
    "\n",
    "chunk_size = 5000\n",
    "splits = range(1 * chunk_size, (evaluate_sample.shape[0] // chunk_size + 1) * chunk_size, chunk_size)[:-1]\n",
    "\n",
    "t_acc_w = []\n",
    "t_acc_l = []\n",
    "t_cosine = []\n",
    "t_cm = np.zeros((26,26))\n",
    "\n",
    "start = 0\n",
    "end = chunk_size\n",
    "\n",
    "for dff in np.split(evaluate_sample, splits, axis=0):\n",
    "    eval_sample_x = dff[dff.columns[34:69]].to_numpy(copy=True)\n",
    "    eval_sample_len = dff[dff.columns[34]].to_numpy(copy=True)\n",
    "    eval_sample_y = dff[dff.columns[0:34]].to_numpy(copy=True)\n",
    "\n",
    "    acc_w, acc_l, cm, cosine = evaluate(predictor, eval_sample_x, eval_sample_y, eval_sample_len)\n",
    "\n",
    "    print('For elements {}-{}, {}'.format(start, end, dff.shape))\n",
    "    print('Word Accuracy: {}'.format(acc_w))\n",
    "    print('Letter Accuracy: {}'.format(acc_l))\n",
    "    print('Mean Cosine Distance: {}'.format(cosine))\n",
    "    t_acc_w.append(acc_w)\n",
    "    t_acc_l.append(acc_l)\n",
    "    t_cosine.append(cosine)\n",
    "    t_cm += cm\n",
    "    start += chunk_size\n",
    "    end += chunk_size\n",
    "    \n",
    "print('\\nTotal results:')\n",
    "print('Word Accuracy: {}'.format(np.sum(t_acc_w)/len(t_acc_w)))\n",
    "print('Letter Accuracy: {}'.format(np.sum(t_acc_l)/len(t_acc_l)))\n",
    "print('Mean Cosine Distance: {}'.format(np.sum(t_cosine)/len(t_cosine)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that it reconstructs almost 1 out of 3 words right and have an accuracy of 63% for the letters, which is pretty good! In the next section we will observe its results and evaluate them ourselves.\n",
    "\n",
    "But first, remember to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(30.5, 0.5, 'Ground Truth'), Text(0.5, 12.5, 'Predicted')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "ax = sns.heatmap(t_cm)\n",
    "ax.set(xlabel=\"Predicted\", ylabel = \"Ground Truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'c6d02813-d1a7-4f5a-9401-2a097f88313f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c6d02813-d1a7-4f5a-9401-2a097f88313f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Mon, 10 Aug 2020 22:22:53 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Use Model for Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although maybe its performance is not the best, our model is working, so it's time to create some custom inference code so that we can send the model a review which has not been processed and have it determine the sentiment of the review.\n",
    "\n",
    "As we saw above, by default the estimator which we created, when deployed, will use the entry script and directory which we provided when creating the model. However, since we now wish to accept a string as input and our model expects a processed review, we need to write some custom inference code (the predict file). For the rest of the files, we can copy the ones from the source directory to the serve directory and provide them to the SageMaker inference container to deploy a Pytorch model.\n",
    "\n",
    "For setting up everything I will follow the steps explained by Nadim Kawwa at https://github.com/NadimKawwa/rnn_sentiment_analysis/blob/master/SageMaker%20Project.ipynb, as they are well explained, understanable and concise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Deploy for Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the serve directory, the predict function will accept a text input and return a text too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m shuffle\n",
      "\n",
      "\u001b[37m# import model\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m WordOrderer, Encoder, Decoder\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m join_sentence, integer2sentence, one_hot_encode, prepare_predict, read_prediction\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = torch.load(f)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    encoder = Encoder(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    decoder = Decoder(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33moutput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    model = WordOrderer(encoder, decoder).to(device)\n",
      "\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "\n",
      "    \u001b[37m# Load the 2 saved letter2int_dict.\u001b[39;49;00m\n",
      "    letter2int_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mletter2int_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(letter2int_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.letter2int_dict = pickle.load(f)\n",
      "    int2letter_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mint2letter_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(int2letter_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.int2letter_dict = pickle.load(f)\n",
      "\n",
      "    model.to(device).eval()\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m data\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mPredicting class labels for the input data...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m model.letter2int_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no letter2int_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m model.int2letter_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no int2letter_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    original_s_int, jumbled_s_int, data_len = prepare_predict(input_data, model.letter2int_dict)\n",
      "\n",
      "    integer_sentence = []\n",
      "    \u001b[34mfor\u001b[39;49;00m word \u001b[35min\u001b[39;49;00m jumbled_s_int:\n",
      "        word_batch = [word]\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(word) > \u001b[34m3\u001b[39;49;00m:\n",
      "            dict_size = \u001b[34m34\u001b[39;49;00m\n",
      "            seq_len = \u001b[34m35\u001b[39;49;00m\n",
      "            batch_size =\u001b[34m1\u001b[39;49;00m\n",
      "            test_seq = one_hot_encode(word_batch, dict_size, seq_len, batch_size)\n",
      "\n",
      "            data = torch.from_numpy(test_seq).float().squeeze().to(device)\n",
      "            \u001b[37m# Have the torch as a batch of size 1\u001b[39;49;00m\n",
      "            data_batch = data.view(\u001b[34m1\u001b[39;49;00m, np.shape(data)[\u001b[34m0\u001b[39;49;00m], np.shape(data)[\u001b[34m1\u001b[39;49;00m])\n",
      "\n",
      "            \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m\n",
      "            model.eval()\n",
      "            \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "                output = model.forward(data_batch)\n",
      "                word_integer = []\n",
      "\n",
      "                \u001b[34mfor\u001b[39;49;00m letter \u001b[35min\u001b[39;49;00m output[\u001b[34m0\u001b[39;49;00m]: \u001b[37m#as there's only 1 batch\u001b[39;49;00m\n",
      "                    letter_numpy = letter.numpy()\n",
      "                    max_value_ind = np.argmax(letter_numpy, axis=\u001b[34m0\u001b[39;49;00m)\n",
      "                    word_integer.append(max_value_ind)\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            word_integer = word_batch.copy()\n",
      "\n",
      "        integer_sentence.append(word_integer)\n",
      "\n",
      "    output_string = read_prediction(integer_sentence, data_len, model.int2letter_dict)\n",
      "    \u001b[34mreturn\u001b[39;49;00m output_string\n"
     ]
    }
   ],
   "source": [
    "!pygmentize serve/predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-pytorch-2020-08-10-22-22-57-688'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Setting up lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we are going to do is set up a Lambda function. This Lambda function will be executed whenever our public API has data sent to it. When it is executed it will receive the data, perform any sort of processing that is required, send the data (the review) to the SageMaker endpoint we've created and then return the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Create an IAM Role for the Lambda function\n",
    "\n",
    "Since we want the Lambda function to call a SageMaker endpoint, we need to make sure that it has permission to do so. To do this, we will construct a role that we can later give the Lambda function.\n",
    "\n",
    "Using the AWS Console, navigate to the IAM page and click on Roles. Then, click on Create role. Make sure that the AWS service is the type of trusted entity selected and choose Lambda as the service that will use this role, then click Next: Permissions.\n",
    "\n",
    "In the search box type sagemaker and select the check box next to the AmazonSageMakerFullAccess policy. Then, click on Next: Review.\n",
    "\n",
    "Lastly, give this role a name. Make sure you use a name that you will remember later on, for example LambdaSageMakerRole. Then, click on Create role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Create a Lambda function\n",
    "\n",
    "Now it is time to actually create the Lambda function.\n",
    "\n",
    "Using the AWS Console, navigate to the AWS Lambda page and click on Create a function. When you get to the next page, make sure that Author from scratch is selected. Now, name your Lambda function, using a name that you will remember later on. Make sure that the Python 3.6 runtime is selected and then choose the role that you created in the previous part. Then, click on Create Function.\n",
    "\n",
    "On the next page you will see some information about the Lambda function you've just created. If you scroll down you should see an editor in which you can write the code that will be executed when your Lambda function is triggered. In this project, we will use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT_HERE',    # The name of the endpoint we created\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
    "                                       Body = event['body'])                       # The actual review\n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : result\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have copy and pasted the code above into the Lambda code editor, replace the **ENDPOINT NAME HERE** portion with the name of the endpoint that we deployed earlier. You can determine the name of the endpoint using the code cell below.\n",
    "\n",
    "And once you have added the endpoint name to the Lambda function, click on Save. Your Lambda function is now up and running. Next we need to create a way for our web app to execute the Lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Setting Up API Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our Lambda function is set up, it is time to create a new API using API Gateway that will trigger the Lambda function we have just created.\n",
    "\n",
    "Using AWS Console, navigate to Amazon API Gateway and then click on Get started.\n",
    "\n",
    "On the next page, make sure that New API is selected and give the new api a name. Then, click on Create API.\n",
    "\n",
    "Now we have created an API, however it doesn't currently do anything. What we want it to do is to trigger the Lambda function that we created earlier.\n",
    "\n",
    "Select the Actions dropdown menu and click Create Method. A new blank method will be created, select its dropdown menu and select POST, then click on the check mark beside it.\n",
    "\n",
    "For the integration point, make sure that Lambda Function is selected and click on the Use Lambda Proxy integration. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway.\n",
    "\n",
    "Type the name of the Lambda function you created earlier into the Lambda Function text entry box and then click on Save. Click on OK in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function you created.\n",
    "\n",
    "The last step in creating the API Gateway is to select the Actions dropdown and click on Deploy API. You will need to create a new Deployment stage and name it anything you like, for example prod.\n",
    "\n",
    "You have now successfully set up a public API to access your SageMaker model. Make sure to copy or write down the URL provided to invoke your newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text Invoke URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Deploying our web app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a publicly available API, we can start using it in a web app with the static HTML file which will make use of the public api you created earlier.\n",
    "\n",
    "In the website folder there should be a file called index.html. Download the file to your computer and open that file up in a text editor of your choice. You should replace the value of the **action = ...** line on the code with the url that you wrote down in the last step and then save the file.\n",
    "\n",
    "Now, if you open index.html on your local computer, your browser will behave as a local web server and you can use the provided site to interact with your SageMaker model.\n",
    "\n",
    "If you'd like to go further, you can host this html file anywhere you'd like, for example using github or hosting a static site on Amazon's S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Web results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finally deploying the web, we can try it with some sentences and see what it returns:\n",
    "    \n",
    "- this isn't an example sentence -> **this inst an elwalpe stanenne**\n",
    "\n",
    "- Despite the apparences, she stod among the best -> **detiese the acteeeeens she stod acnog the best**\n",
    "\n",
    "- You have to be more careful and read between the lines -> **you have to be more courfel and read betewen the lenns**\n",
    "\n",
    "- Did you know that, along with gorgeous architecture, it's home to the largest tamale? -> **did you know taat annig watt gugslues acerrctrhice ins hume to the laggest tallle*\n",
    "\n",
    "This results coincide with the numerical results obtained earlier, but you can try twisting the model parameters and see if yours obtain better results.\n",
    "\n",
    "**With this we have finally completed this project notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Delet the endpoint & Final cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '55f408ee-b35d-4625-aec6-1cb9ecc664b5',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '55f408ee-b35d-4625-aec6-1cb9ecc664b5',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Mon, 10 Aug 2020 22:38:10 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the predictor endpoint \n",
    "boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to always shut down your endpoint if you are no longer using it. You are charged for the length of time that the endpoint is running so if you forget and leave it on you could end up with an unexpectedly large bill. Also look to clean the S3 bucket, the models, endpoints configurations, lambda functions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
