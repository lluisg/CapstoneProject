{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT: CAN THE COMPUTERS READ IT TOO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cache' # The folder we will use for storing data\n",
    "data_dir = os.path.join(cache_dir, 'data')\n",
    "model_dir = os.path.join(cache_dir, 'model')\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'blogs' in os.listdir('.'):\n",
    "    %rm -rf blogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DURING THE NOTEBOOK YOU WILL FIND VARIOUS NUMBER OF CELLS THAT CONSISTS ONLY TO CLEAN FILES IN ORDER TO HAVE AS MUCH SPACE POSSIBLE AND NOT HAVE PROBLEMS WITH IT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE TRAIN DATA NEEDS TO BE TRANSFORMED AS WE DO, BUT THE TEST DATA DOESNT NEED TO AS THE PREDICT WILL CONVERT FROM THE STRING TO THE ADECUATED FORM FOR THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'blogger.zip' not in os.listdir('.'):\n",
    "    !wget -O blogger.zip http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip\n",
    "else:\n",
    "    print('zip already downloaded')\n",
    "!unzip -Z1 blogger.zip | head -1000 | sed 's| |\\\\ |g' | xargs unzip blogger.zip\n",
    "print('zip unziped')\n",
    "# !unzip blogger.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len([name for name in os.listdir('blogs') if os.path.isfile(name)]))\n",
    "print(len(os.listdir('blogs/')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was pretty big (total of 19320 blogs) with maybe too much files (considering that inside each blog normally there are more than one post) so only a subset of it was unzipped. The subset selected is on the head part of the command, in this case resulted in 999 blogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETE ZIP FILE IF YOU ARE NOT USING ANYMORE TO CLEAR SOME SPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Preparing and Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('blogs', '*.xml')\n",
    "files = glob.glob(path)\n",
    "\n",
    "with open(files[0], encoding='Windows-1252') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, each blog consists of different posts, which we will separate, as we are only interested in posts, and we don't want the dates or other info that could be inside the xml's files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As XML files they consists of different tags. We are only gonna take the **post** ones.\n",
    "    \n",
    "We used the beatiful soup to get the texts, that because with other parsers like ElementTree it resulted in ParseError in some files, as there are some HTML elements (like &nspb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_blogs_data(data_dir='blogs'):\n",
    "    data = []\n",
    "\n",
    "    path = os.path.join(data_dir, '*.xml')\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    for f in files:\n",
    "        blog = open(f)\n",
    "\n",
    "        soup = BeautifulSoup(blog, 'xml')\n",
    "        posts = soup.find_all('post')\n",
    "\n",
    "        for post in posts:\n",
    "            data.append(post.text)\n",
    "    return data\n",
    "\n",
    "blogs_data = read_blogs_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there is some error, something related to the decoding.\n",
    "After searching for some time we find a possible solution to which type are the files encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "blog_dir = os.path.join('blogs', os.listdir('blogs')[0])\n",
    "blog = open(blog_dir, 'rb')\n",
    "enco = chardet.detect(blog.read())['encoding']\n",
    "print(enco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we use this encoding into the parsing of all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_blogs_data(data_dir='blogs', encod = \"ISO-8859-1\" ):\n",
    "    data = []\n",
    "\n",
    "    path = os.path.join(data_dir, '*.xml')\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    for f in files:\n",
    "        blog = open(f, encoding=encod)\n",
    "\n",
    "        soup = BeautifulSoup(blog, 'xml')\n",
    "        posts = soup.find_all('post')\n",
    "\n",
    "        for post in posts:\n",
    "            data.append(post.text)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_data = read_blogs_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of posts\n",
    "print(\"# of posts = {}\".format(len(blogs_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check them\n",
    "print('post50: ', blogs_data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data parsed, which consists of 1199 posts, although there are only 9 blogs used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the posts text in the 'blogs_data' array, so we will apply to it the processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To start, we shuffle all the posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "blogs_data_shuffled = shuffle(blogs_data)\n",
    "print('post50 shuffled: ', blogs_data_shuffled[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply different processing steps:\n",
    "    - remove the \\n, \\t, whitespaces starting and ending\n",
    "    - remove possible html tags and other\n",
    "    - put all in lowercase\n",
    "    - remove accents in some chars\n",
    "    - separate the posts by words with the help of the indices\n",
    "    - get the words array from the indices\n",
    "    - ignore words that have chars not considered integers, letters nor punctuation\n",
    "    - also ignore words which are only integers or punctuation\n",
    "    - limit the posts to 500 words per posts (sentence)\n",
    "    - we ignore the words larger than supercalifragilisticexpialidocious\n",
    "    \n",
    "So finally, a list of words are returned. These words include punctuation, which will have to take into account later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len('supercalifragilisticexpialidocious'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def splitWithIndices(s, c=' '):\n",
    "    p = 0\n",
    "    for k, g in groupby(s, lambda x:x==c):\n",
    "        q = p + sum(1 for i in g)\n",
    "        if not k:\n",
    "            yield p, q # or p, q-1 if you are really sure you want that\n",
    "        p = q\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def post_to_words(post):\n",
    "    LIMIT_WORDS = 500\n",
    "\n",
    "    post = post.lstrip().rstrip()\n",
    "    post = BeautifulSoup(post, \"html.parser\").get_text()\n",
    "    post = post.lower()\n",
    "    \n",
    "    words_accents = []\n",
    "    for word in post:\n",
    "        words_accents.append(strip_accents(word))\n",
    "        \n",
    "    words_ind = splitWithIndices(words_accents)\n",
    "        \n",
    "    words_aux = []\n",
    "    for idx in words_ind:\n",
    "        words_aux.append(post[idx[0]:idx[1]])\n",
    "    \n",
    "    words = []\n",
    "    for idx, word in enumerate(words_aux):\n",
    "        isvalid = True\n",
    "        isdigit = True\n",
    "        ispunct = True\n",
    "\n",
    "        for letter in word:\n",
    "            if not letter.isdigit():\n",
    "                if letter not in string.punctuation:\n",
    "                    if not letter.isalpha():\n",
    "                        isvalid = False\n",
    "            \n",
    "            if not letter.isdigit():\n",
    "                isdigit = False\n",
    "            if letter not in string.punctuation:\n",
    "                ispunct = False\n",
    "                \n",
    "        if isvalid == True and isdigit == False and ispunct == False:\n",
    "            if len(word) <= len('supercalifragilisticexpialidocious') and idx <= LIMIT_WORDS:\n",
    "                words.append(word)\n",
    "        \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it\n",
    "words = post_to_words(blogs_data_shuffled[60])\n",
    "print(blogs_data_shuffled[60])\n",
    "print(words)\n",
    "\n",
    "word1 = '123456 .,'\n",
    "w1 = post_to_words(word1)\n",
    "print('\\n{} -> {}'.format(word1, w1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below applies the post_to_words method for all the post in all the blogs in the dataset indicated. In addition it caches the results. This is because performing this processing step can take a long time. This way if you are unable to complete the notebook in the current session, you can come back without needing to process the data a second time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insted of having have the data as sentences, we will have it as words, as the input of the model will be word by word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we are only using one set of data, as this is the one that later will be divided into training and test, and also will be transformed to have the input and the output (jumbled or not jumbled words respectively).\n",
    "\n",
    "We ignore the sentences which for whenever reason have length 0, meaning all the words has been ignored (following the rules stated in the post2words) and ignore the words with length less than 4 letters, as they will not add anything to the model once jumbled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset resultant, and the one we will use will be the most common 10000 words from the blogs stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def preprocess_data(data, cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each post to words; read from cache if available.\"\"\"\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    file_dir = os.path.join(cache_dir, cache_file)\n",
    "\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", file_dir)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_data = []\n",
    "        perc = 0\n",
    "        \n",
    "        for idx_p, post in enumerate(data):\n",
    "            \n",
    "            if idx_p / len(data) >= perc:\n",
    "                print('{} / {} sentences = {}%'.format(idx_p, len(data), np.round(perc*100, decimals = 1)))\n",
    "                perc = perc+0.1\n",
    "            \n",
    "            words = post_to_words(post)\n",
    "            words_stemmed = [PorterStemmer().stem(w) for w in words] # stem\n",
    "            \n",
    "            words_len = [word for word in words if len(word) > 3] #ignore words with less than 4 letters\n",
    "            if len(words_len) != 0:\n",
    "                words_data += words_len\n",
    "                \n",
    "        \n",
    "#         words_data = [post_to_words(post) for post in data]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_data=words_data)\n",
    "            with open(file_dir, \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", file_dir)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_data = (cache_data['words_data'])\n",
    "    \n",
    "    return words_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all the blogs\n",
    "if not os.path.exists(cache_dir): # Make sure that the folder exists, if not create it\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "words_data = preprocess_data(blogs_data_shuffled, cache_dir)\n",
    "print('The dataset consists of {} words'.format(len(words_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more than 115 milion words in 123.000 sentences! Remember a subset of 999 blogs out of 13.000 were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of number of letters per word is adecuated\n",
    "maxlen_w = 0\n",
    "minlen_w = 1000\n",
    "\n",
    "for word in words_data:\n",
    "    if len(word) > maxlen_w:\n",
    "        maxlen_w = len(word)\n",
    "    if len(word) < minlen_w:\n",
    "        minlen_w = len(word)            \n",
    "            \n",
    "print('max length word: {}'.format(maxlen_w))\n",
    "print('min length word: {}'.format(minlen_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dictionary from the words processed and from it we will extract the most common 5.000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(data, vocab_size = 10000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
    "    #       sentence is a list of words.\n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "\n",
    "    for word in data:\n",
    "        if word in word_count:\n",
    "            word_count[word]+=1\n",
    "        else:\n",
    "            word_count[word]=1\n",
    "    \n",
    "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    #       sorted_words[-1] is the least frequently appearing word.\n",
    "    \n",
    "    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_words = [tupl[0] for tupl in sorted_words]\n",
    "\n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx                           # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(words_data, 5000)\n",
    "print(word_dict)\n",
    "train_data = [key for key in word_dict.keys()]\n",
    "print(train_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "import random\n",
    "test_data = random.choices(words_data, k=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have checked that the dataset we are going to use consists of the 5000 most common words in the blogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 The dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we are going to construct in this notebook we will construct a feature representation which consists in represent each letter of each word as an integer. We will be using the latin alphabet as a dictionary between letters and integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter2int = dict(zip(string.ascii_lowercase, range(1,27)))\n",
    "int2letter = {v: k for k, v in letter2int.items()}\n",
    "\n",
    "print(len(letter2int), letter2int, '\\n')\n",
    "print(len(int2letter), int2letter, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Transform the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word dictionary which allows us to transform the letters appearing in the words into integers, it is time to make use of it and convert our posts words to their integer sequence representation. Since we will be using a recurrent neural network, it will be convenient if the length of each word is the same. To do this, we will use the previous defined max size (the length of the word supercalifragilisticexpialidocious) as the fixed size for our words and then pad short words with the category 'no letter' (which we will label 0) and truncate long words.\n",
    "\n",
    "For now, the punctuations and numbers will be ignored, as it will simplify drastically the task. If there's enough time in the future, it could be possible to implemented some kind of punctuation mark holder that allows to save the punctuation of the sentence and put them back after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(letter_dict, word, pad = 34):\n",
    "    NOLETTER = 0 # We will use 0 to represent the 'no letter' category\n",
    "    \n",
    "    word_padded = []\n",
    "    length = len(word)\n",
    "    \n",
    "    #conversion\n",
    "    for letter_index, letter in enumerate(word):\n",
    "        if letter in letter_dict:\n",
    "            if letter_dict[letter] >= 0:\n",
    "                word_padded.append(letter_dict[letter])\n",
    "        else:\n",
    "            length -= 1\n",
    "    \n",
    "    #padding\n",
    "    if len(word_padded) < pad:\n",
    "        word_padded = (word_padded + pad * [NOLETTER])[:pad]\n",
    "            \n",
    "    return word_padded, length\n",
    "\n",
    "def convert_and_pad_data(letter_dict, data, pad = 34):\n",
    "    \n",
    "    result = []\n",
    "    lengths = []\n",
    "\n",
    "    perc = 0        \n",
    "    \n",
    "    for idx_w, word in enumerate(data):\n",
    "        \n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} word = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "        \n",
    "#         print('word------>', word)\n",
    "        converted_word, len_word = convert_and_pad(letter_dict, word, pad)\n",
    "#         print('word------>', word, leng_word)\n",
    "#         print('word converted--->', converted_word, leng_word)\n",
    "\n",
    "        result.append(converted_word)\n",
    "        lengths.append(len_word)\n",
    "        \n",
    "#     return np.array(result), np.array(lengths)\n",
    "    return result, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we will have separated the training and test datasets. The training will consist of the 5000 wods more common in the blogs read. The test will consists on 1000 words randomly selected within the blogs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_padded, train_data_padded_len = convert_and_pad_data(letter2int, train_data)\n",
    "test_data_p, len_test_p = convert_and_pad_data(letter2int, test_data)\n",
    "\n",
    "# There are some words in training that after the transformation will stay at less than 4 letters, so they will be disposed now.\n",
    "train_data_p = [] \n",
    "len_train_p = []\n",
    "for word, lenw in zip(train_data_padded, train_data_padded_len):\n",
    "    if lenw > 3: #Check again length words\n",
    "            train_data_p.append(word)\n",
    "            len_train_p.append(lenw)\n",
    "            \n",
    "print('Previous lengths: {}-{} \\nNew length: {}-{}'.format(len(train_data_padded), len(train_data_padded_len), \n",
    "                                                           len(train_data_p), len(len_train_p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check\n",
    "for ind_w in [2, 8]:\n",
    "    print(train_data_p[ind_w], len_train_p[ind_w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally our dataset will consist of 90 thousand words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = None\n",
    "blogs_data = None\n",
    "blogs_data_shuffled = None\n",
    "test_data = None\n",
    "train_data = None\n",
    "train_data_padded = None\n",
    "train_data_padded_len = None\n",
    "word = None\n",
    "word1 = None\n",
    "word_dict = None\n",
    "words = None\n",
    "words_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Jumble the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the objective is to get back the words after being jumbled, we have now to jumble this data. We create a function to jumble the words, and then another function to handle all the words in a sentence.\n",
    "\n",
    "Probabilistically, the result of the function could be the same word passed as input, but we will ignore this possible case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jumble_word(word, word_len):\n",
    "    if word_len <= 2:\n",
    "        word_j = word\n",
    "        \n",
    "    else:\n",
    "        sub_word = []\n",
    "        for w in word:\n",
    "            sub_word.append(w)\n",
    "\n",
    "        sub_word = sub_word[1:word_len-1]\n",
    "        shufled_word = shuffle(sub_word)\n",
    "        \n",
    "        word_j = word.copy()\n",
    "        word_j[1:word_len-1] = shufled_word\n",
    "        \n",
    "    return word_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it\n",
    "word = [1,2,3,4,5,6,7,0,0,0,0,0,0,0]\n",
    "len_w = 7\n",
    "print(jumble_word(word, len_w))\n",
    "\n",
    "word = [1,2,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "len_w = 2\n",
    "print(jumble_word(word, len_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jumble_data(data, data_len):\n",
    "    jumbled_data = []\n",
    "    \n",
    "    idx_w = 0\n",
    "    perc = 0\n",
    "    for word, w_len in zip(data, data_len):\n",
    "        jumbled_sentence = []\n",
    "\n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} words = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "\n",
    "        jumbled_word = jumble_word(word, w_len)\n",
    "\n",
    "        jumbled_data.append(jumbled_word)\n",
    "        idx_w+=1\n",
    "        \n",
    "    return jumbled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jumbled_train = jumble_data(train_data_p, len_train_p)\n",
    "jumbled_test = jumble_data(test_data_p, len_test_p)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check\n",
    "for ind_w in [4, 6]:\n",
    "    print('Word: {}'.format(train_data_p[ind_w]))\n",
    "    print('Length: {}'.format(len_train_p[ind_w]))\n",
    "    print('Jumbled: {}\\n'.format(jumbled_train[ind_w]))\n",
    "    \n",
    "    print('Word: {}'.format(test_data_p[ind_w]))\n",
    "    print('Length: {}'.format(len_test_p[ind_w]))\n",
    "    print('Jumbled: {}\\n'.format(jumbled_test[ind_w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check sizes coincide\n",
    "print('train input: {}, train output: {}, lengths train: {} \\ntest input: {}, test output: {}, lengths test: {}'.format(\n",
    "    len(jumbled_train), len(train_data_p), len(len_train_p),\n",
    "    len(jumbled_test), len(test_data_p), len(len_test_p)))\n",
    "\n",
    "# Check the same numbers are used in both input and output on same sentence number \n",
    "for word1, word2 in zip(jumbled_train, train_data_p):\n",
    "    sum_num=0\n",
    "    \n",
    "    if len(word1) != len(word2):\n",
    "        print('errror of size in training!')\n",
    "\n",
    "    for indx in range(len(word1)):\n",
    "        sum_num += word1[indx]-word2[indx]\n",
    "    \n",
    "    if sum_num != 0:\n",
    "        print('error of numbers in training!')\n",
    "\n",
    "for word1, word2 in zip(jumbled_test, test_data_p):\n",
    "    sum_num=0\n",
    "    \n",
    "    if len(word1) != len(word2):\n",
    "        print('errror of size in test!')\n",
    "\n",
    "    for indx in range(len(word1)):\n",
    "        sum_num += word1[indx]-word2[indx]\n",
    "    \n",
    "    if sum_num != 0:\n",
    "        print('error of numbers in test!')\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1 Uploading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir): # Make sure that the folder exists, if not create it\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "# Uploading train and test files separately, we check it doesn't upload an empty file\n",
    "data_files = [jumbled_train, train_data_p, len_train_p,\n",
    "              jumbled_test, test_data_p, len_test_p]\n",
    "cache_files = ['input_train_data.pkl', 'output_train_data.pkl', 'length_train_data.pkl',\n",
    "               'input_test_data.pkl', 'output_test_data.pkl', 'length_test_data.pkl']\n",
    "\n",
    "for data_file, cache_file in zip(data_files, cache_files):\n",
    "    cache_data = None\n",
    "    cache_data = dict(data_file=data_file)\n",
    "    file_dir = os.path.join(data_dir, cache_file)\n",
    "\n",
    "    with open(file_dir, \"wb\") as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "        \n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        print(\"Wrote data to cache file:\", file_dir)\n",
    "    else:\n",
    "        print('Wrote empty file on cache file', file_dir)\n",
    "        \n",
    "print('Done')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are gonna clean some more variables as we will download them in the next sections or not gonna use them more\n",
    "len_data_p = None\n",
    "words_data_p = None\n",
    "jumbled_data = None\n",
    "trainX = None\n",
    "trainY = None\n",
    "train_len = None\n",
    "testX = None\n",
    "testY = None\n",
    "test_len = None\n",
    "word2 = None\n",
    "word1 = None\n",
    "word = None\n",
    "data_file = None\n",
    "data_files = None\n",
    "files = None\n",
    "cache_files = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2 Loading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cache_file, data_dir=data_dir):\n",
    "    cache_data = None\n",
    "    file_dir = os.path.join(data_dir, cache_file)\n",
    "\n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read data from cache file:\", file_dir)\n",
    "        except:\n",
    "            print('Problem reading the file', file_dir)\n",
    "    else:\n",
    "        print('File empty')\n",
    "\n",
    "    if cache_data is None:\n",
    "        print('Didnt read anythin')\n",
    "        resulting_files = []\n",
    "    else:\n",
    "        resulting_files = (cache_data['data_file'])\n",
    "        \n",
    "    return resulting_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already done the all the previous steps in the past, you should have the training and test split files in the cache/data folder.\n",
    "\n",
    "We will load them and the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_files = ['input_train_data.pkl', 'input_test_data.pkl',\n",
    "               'output_train_data.pkl', 'output_test_data.pkl',\n",
    "               'length_train_data.pkl', 'length_test_data.pkl']\n",
    "\n",
    "resulting_files = []\n",
    "for cache_file in cache_files:\n",
    "    resulting_files.append(load_data(cache_file, data_dir))\n",
    "        \n",
    "print('Done')\n",
    "\n",
    "trainX, testX, trainY, testY, train_len, test_len = resulting_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check they have been loaded correctly\n",
    "ind = 5\n",
    "\n",
    "print('word trainX ->', trainX[ind])\n",
    "print('word trainY ->', trainY[ind])\n",
    "print('word train_len ->', str(train_len[ind]))\n",
    "\n",
    "print('word testX ->', testX[ind])\n",
    "print('word testY ->', testY[ind])\n",
    "print('word test_len ->', str(test_len[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Upload data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model realy only needs the words and will try to rearange them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form output[34], length, input[34] where input[34] represens the word, which is a sequence of 34 integers the letters in the words.\n",
    "\n",
    "We will save the training csv and both dictionaries on the model directory, in order to upload later together in s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not os.path.exists(model_dir): # Make sure that the folder exists if not, create it\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "pd.concat([pd.DataFrame(trainY), pd.DataFrame(train_len), pd.DataFrame(trainX)], axis=1) \\\n",
    "        .to_csv(os.path.join(model_dir, 'train.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([pd.DataFrame(testY), pd.DataFrame(test_len), pd.DataFrame(testX)], axis=1) \\\n",
    "        .to_csv(os.path.join(cache_dir, 'test.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on when we construct an endpoint which processes a submitted review we will need to make use of the dictionaries which we have created. As such, we will save them to a file now for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, 'letter2int_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(letter2int, f)\n",
    "with open(os.path.join(model_dir, 'int2letter_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(int2letter, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to load the dictionaries for any case from the files, it can be used the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dictionaries\n",
    "def load_dict(dict_file, dict_dir = cache_dir):\n",
    "    cache_data = None\n",
    "    file_dir = os.path.join(dict_dir, dict_file)\n",
    "\n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read data from cache file:\", dict_file)\n",
    "        except:\n",
    "            print('Problem reading the file', dict_file)\n",
    "    else:\n",
    "        print('File empty')\n",
    "\n",
    "    if cache_data is None:\n",
    "        print('Didnt read anythin')\n",
    "#         resulting_files = []\n",
    "    else:\n",
    "        pass\n",
    "#         resulting_files = (cache_data['data_file'])\n",
    "    \n",
    "    return cache_data\n",
    "\n",
    "# For example\n",
    "letter2int = load_dict('letter2int_dict.pkl', model_dir)\n",
    "int2letter = load_dict('int2letter_dict.pkl', model_dir)\n",
    "\n",
    "print(letter2int)\n",
    "print(int2letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear some more data as they are pretty big variables and we will not use them anymore\n",
    "# as we will read directly from the csv file\n",
    "\n",
    "trainX = None\n",
    "testX = None\n",
    "trainY = None\n",
    "testY = None\n",
    "train_len = None\n",
    "test_len = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 To S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to upload the training data and the dictionaries to the SageMaker default S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/capstoneProject'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# upload training data to S3\n",
    "train_dir = os.path.join(cache_dir, 'train.csv')\n",
    "input_data = sagemaker_session.upload_data(path=model_dir, bucket=bucket, key_prefix=prefix)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you've uploaded the data, by printing the contents of the default bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through S3 objects and print contents\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "     print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by implementing our own neural network in PyTorch along with a training script. The necesary files for them are in the source folder, being them: train.py, model.py, predict.py and util.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize source/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the implementation we can observe that there are two parameters that we may wish to tweak to improve the performance of our model. These are the embedding dimension and the hidden dimension. The size of the vocabulary is the size of the previous vocabularies with the positives values.\n",
    "\n",
    "First we will load a small portion of the training data set to use as a sample to check the correct functioning of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will try to implement the training function to check its correct functioning. By doing this way, we avoid the larger time it takes if we would do it directly with the Pytorch Model as the loading time it takes to train everytime is considerable, and we probably would have to call it several times, trying to fix errors that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't do it before because it increases the memory that would have been used for the csv files, but the letters in each word passed to the model as input will have to be encoded on one-hot vectors. This can be done with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float64)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Get the arrays from csv\n",
    "# Read in only the first 250 rows\n",
    "# train_sample = pd.read_csv(os.path.join(model_dir, 'train.csv'), header=None, names=None, nrows=500)\n",
    "train_sample = pd.read_csv(os.path.join(model_dir, 'train.csv'), header=None, names=None)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = train_sample[train_sample.columns[0:34]]\n",
    "train_sample_len = train_sample[train_sample.columns[34]]\n",
    "train_sample_X = train_sample[train_sample.columns[34:69]] #including the size\n",
    "print('Size read from csv -> X: {}, Y: {}, len: {}'.format(train_sample_X.shape, train_sample_y.shape, train_sample_len.shape))\n",
    "\n",
    "X_np = train_sample_X.to_numpy(copy=True)\n",
    "Y_np = train_sample_y.to_numpy(copy=True)\n",
    "len_np = train_sample_len.to_numpy(copy=True)\n",
    "\n",
    "# Encode the input sentence as one hot vectors\n",
    "# dict_size = len(letter2int)+1 #including the 0\n",
    "dict_size = 34\n",
    "seq_len = 34\n",
    "batch_size = len(train_sample_X)\n",
    "input_seq = one_hot_encode(X_np, dict_size, seq_len, batch_size)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
    "\n",
    "train_torch_x = torch.tensor(input_seq).float().squeeze().clone()\n",
    "train_torch_x = torch.autograd.Variable(train_torch_x)\n",
    "train_torch_len = torch.tensor(len_np).float().squeeze().type(torch.long).clone()\n",
    "# train_torch_x = torch.from_numpy(input_seq).float().squeeze()\n",
    "train_torch_y = torch.tensor(Y_np).float().squeeze().type(torch.long).clone()\n",
    "# train_sample_x = torch.from_numpy(output_seq).float().squeeze()\n",
    "# train_torch_y = torch.from_numpy(Y_np).float().squeeze().type(torch.long)\n",
    "print('Torch X: {} shape, {} type\\nTorch Y: {} shape, {} type'.format(train_torch_x.shape, train_torch_x.dtype, \n",
    "                                                                      train_torch_y.shape, train_torch_y.dtype))\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_torch_x, train_torch_y, train_torch_len)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preparation and arguments as if it was the file\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "input_dim = 34\n",
    "output_dim = 27\n",
    "hid_dim = 128\n",
    "epochs = 20\n",
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    total_length = len(train_loader.dataset)\n",
    "    model.train()\n",
    "    loss_return = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        batchs_done = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch_X, batch_y, batch_len = batch\n",
    "            len_batch = len(batch_X)\n",
    "#             print('input shape X: {}, y:{}'.format(np.shape(batch_X), np.shape(batch_y)))\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            out = model(batch_X)\n",
    "#             print('output shape: ',  np.shape(out))\n",
    "#             print('target shape: ',  np.shape(batch_y))\n",
    "            \n",
    "            batch_loss = 0\n",
    "            for result, target, len_word in zip(out, batch_y, batch_len):\n",
    "#                 print(np.shape(result))\n",
    "#                 print(np.shape(target))\n",
    "#                 print('11', result[:len_word, :])\n",
    "#                 print('22', target[:len_word])\n",
    "#                 print('type input loss X: {}, Y: {}'.format(result.type(), target.type()))\n",
    "#                 print('shape input loss X: {}, Y: {}'.format(np.shape(result), np.shape(target)))\n",
    "                loss = loss_fn(result[:len_word, :], target[:len_word])\n",
    "#                 loss = loss_fn(result, target)\n",
    "#                 loss.backward(retain_graph=True)\n",
    "#                 optimizer.step()\n",
    "                batch_loss += loss\n",
    "    \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += batch_loss.data.item()\n",
    "#             total_loss += batch_loss\n",
    "        \n",
    "            batchs_done += len_batch\n",
    "#             print('Batch done. {} / {} inputs = {}%'.format(\n",
    "#                 batchs_done, total_length, np.round(batchs_done/total_length*100, decimals = 1)))\n",
    "\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))\n",
    "        loss_return.append(total_loss / len(train_loader))\n",
    "        \n",
    "    return loss_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.model import Seq2Seq, Decoder, Encoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(input_dim, hid_dim)\n",
    "decoder = Decoder(input_dim, output_dim, hid_dim)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "# Train the model.\n",
    "# loss_function = nn.NLLLoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "losses = train(model, train_sample_dl, epochs, optimizer, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a plot to observe better\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although slow, we can observe that the model works and seems to improve over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will clean some variables that will not be used again\n",
    "train_sample = None\n",
    "train_sample_y = None\n",
    "train_sample_len = None\n",
    "train_sample_X = None\n",
    "input_seq = None\n",
    "X_np = None\n",
    "Y_np = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build and Train the PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training script works correctly, we will copy adecuately on the train.py file.\n",
    "\n",
    "A typical training script:\n",
    "\n",
    "- Loads training data from a specified directory\n",
    "- Parses any training & model hyperparameters (ex. nodes in a neural network, training epochs, etc.)\n",
    "- Instantiates a model of your design, with any specified hyperparams\n",
    "- Trains that model\n",
    "- Finally, saves the model so that it can be hosted/deployed, later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize source/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Create Pytorch Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point = 'train.py',\n",
    "                    source_dir = 'source',\n",
    "                    role = role,\n",
    "                    framework_version = '1.0',\n",
    "                    train_instance_count = 1,\n",
    "                    train_instance_type = 'ml.c4.xlarge',\n",
    "                    output_path = output_path,\n",
    "                    sagemaker_session = sagemaker_session,\n",
    "                    hyperparameters = {\n",
    "                        'input_dim': 34,\n",
    "                        'output_dim': 27,\n",
    "                        'hidden_dim': 128,\n",
    "                        'epochs': 20,\n",
    "                        'lr': 0.02,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Train the Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time, take it easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# train the estimator on S3 training data\n",
    "estimator.fit({'train': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Prediction with the trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Getting letters back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function that allows to obtain the words back from the padded version to the shorter integer version, and from this integer version to the string version using the dictionaries. This will be done in the following functions.\n",
    "\n",
    "If the model returns all 0's (in the case it had a bad performance). The word will consist of a '.'.\n",
    "\n",
    "Also this will be useful if we want to show how the words in the sentence have been jumbled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_back_and_unpad(number_dict, word, len_word):\n",
    "    \n",
    "    word_back = []\n",
    "    for letter_index, letter in enumerate(word):\n",
    "        if letter_index < len_word:\n",
    "            if letter in number_dict:\n",
    "                word_back.append(number_dict[letter])\n",
    "            else:\n",
    "                word_back.append('.')\n",
    "                        \n",
    "    return word_back\n",
    "\n",
    "\n",
    "def convert_back_data(number_dict, data, len_data):\n",
    "    result = []\n",
    "    \n",
    "    perc=0\n",
    "    idx_w = 0\n",
    "    for word, len_w in zip(data, len_data):\n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} words = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "\n",
    "\n",
    "#         print('word------>', word)\n",
    "        converted_word = convert_back_and_unpad(number_dict, word, len_w)\n",
    "#         print('word converted--->', converted_word, leng_word)\n",
    "\n",
    "        result.append(converted_word)\n",
    "        idx_w += 1\n",
    "        \n",
    "#     return np.array(result), np.array(lengths)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the input training data to check and we will see how the sentence would be seen once jumbled\n",
    "def join_sentence(sentence):\n",
    "    list_words = []\n",
    "    for word in sentence:\n",
    "        w = ''.join(word)\n",
    "        list_words.append(w)\n",
    "\n",
    "    sentence_joined = ' '.join(list_words)\n",
    "    \n",
    "    return sentence_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the test dataset preparated, and assume that the sentence is the first 10 words of the file.\n",
    "test_sample = pd.read_csv(os.path.join(model_dir, 'train.csv'), header=None, names=None, nrows=10)\n",
    "test_sample_x = test_sample[test_sample.columns[35:69]].to_numpy(copy=True)\n",
    "test_sample_len = test_sample[test_sample.columns[34]].to_numpy(copy=True)\n",
    "test_sample_y = test_sample[test_sample.columns[0:34]].to_numpy(copy=True)\n",
    "\n",
    "# Check one sentence, we will consider 5 consecutive words from the training data as a sentence\n",
    "input_words = convert_back_data(int2letter, test_sample_x, test_sample_len)\n",
    "output_words = convert_back_data(int2letter, test_sample_y, test_sample_len)\n",
    "\n",
    "print('{} \\n<--> \\n{}'.format(join_sentence(input_words), join_sentence(output_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Predict Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same as we did with the training script to check it works first. The input will be the word already transformed into integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict the sentence as string, we will transform the sentence into its integer jumbled form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_predict(sentence, dictionary):\n",
    "    input_data_words = post_to_words(sentence)\n",
    "    \n",
    "    print('Converting data')\n",
    "    integer_sentence, len_sentence = convert_and_pad_data(dictionary, input_data_words)\n",
    "    print('Jumbling data')\n",
    "    jumbled_sentence = jumble_data(integer_sentence, len_sentence)\n",
    "        \n",
    "    return integer_sentence, jumbled_sentence, len_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are less than 4 letters, will be returned as such without passing through the model, as no transformation is really applied.\n",
    "\n",
    "The function will check if the input is a string array or an integer array, and in the case of the string, it will apply the functions necesary to have the aduecuated type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_input, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    integer_sentence = [] \n",
    "    for word in data_input:\n",
    "#         print('word: ', word)\n",
    "        word_batch = [word]\n",
    "\n",
    "        if len(word) > 3:\n",
    "            dict_size = 34\n",
    "            seq_len = 34\n",
    "            batch_size =1\n",
    "            test_seq = one_hot_encode(word_batch, dict_size, seq_len, batch_size)\n",
    "            \n",
    "\n",
    "            data = torch.from_numpy(test_seq).float().squeeze().to(device)\n",
    "            # Have the torch as a batch of size 1\n",
    "            data_batch = data.view(1, np.shape(data)[0], np.shape(data)[1])\n",
    "#             print('size: {} -> {}'.format(np.shape(data), np.shape(data_batch)))\n",
    "            # Make sure to put the model into evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model.forward(data_batch)\n",
    "                \n",
    "                word_integer = []\n",
    "                for letter in output[0]: #as there's only 1 batch\n",
    "                    \n",
    "#                     print('letter', np.shape(letter))\n",
    "                    letter_numpy = letter.numpy()\n",
    "#                     print('numpy', type(letter_numpy), np.shape(letter_numpy))\n",
    "                    \n",
    "                    max_value_ind = np.argmax(letter_numpy, axis=0)\n",
    "#                     print(max_value_ind)\n",
    "#                     print(letter_numpy)\n",
    "\n",
    "                    word_integer.append(max_value_ind)\n",
    "                \n",
    "        else:\n",
    "\n",
    "            word_integer = word_batch.copy()\n",
    "            \n",
    "        integer_sentence.append(word_integer)\n",
    "#         print('integer word: ', word_integer) \n",
    "    \n",
    "    return integer_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prediction(prediction, lengths, dictionary):        \n",
    "    return join_sentence(convert_back_data(int2letter, prediction, lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_result = predict(test_sample_x, model)\n",
    "str_result = read_prediction(int_result, test_sample_len, letter2int)\n",
    "print('(1st word) Result  -> Ground Truth:')\n",
    "print(int_result[0])\n",
    "print('->')\n",
    "print(test_sample_y[0])\n",
    "print('\\nResulting sentence: {}'.format(str_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works correctly (ignoring the model performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will try with a sentence string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'A study from Cambridge, has stated that etcetera'\n",
    "\n",
    "original_s_int, jumbled_s_int, data_len = prepare_predict(s, letter2int)\n",
    "output_array = predict(original_s_int, model)\n",
    "output_string = read_prediction(output_array, data_len, letter2int)\n",
    "\n",
    "print('\\nResulting array:', output_array)\n",
    "print('\\nResulting string:', output_string)\n",
    "print('\\nLength original sentence was {}, length of returned sentence is {}'.format(len(s.split(' ')), \n",
    "                                                                                    len(output_string.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that it works fine, the rest depends on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Predict Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models do not automatically come with .predict() functions attached so we have to create a predict.py file. This file is responsible for loading a trained model and applying it to passed in, numpy data. When you created a PyTorch estimator, you specified where the training script, train.py was located.\n",
    "\n",
    "Also there's a utils.py file, which contains the functions necesary to make all the other work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can deploy this custom PyTorch model, we have to take one more step: creating a PyTorchModel. This model is responsible for knowing how to execute a specific predict.py script and it is what we'll deploy to create an endpoint.\n",
    "\n",
    "Also don't forget to copy the predict function into the predict.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# importing PyTorchModel\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Create a model from the trained estimator data\n",
    "# And point to the prediction script\n",
    "pmodel = PyTorchModel(model_data = estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version = '1.0',\n",
    "                     source_dir = 'source',\n",
    "                     entry_point = 'predict.py',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Deploy trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# deploy and create a predictor\n",
    "predictor = pmodel.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is deployed, we can see how it performs when applied to the test data.\n",
    "\n",
    "The provided function below, takes in a deployed predictor and the sentence and returns teo metrics: the accuracy of words which has been completely correctly reconstructed in the sentence, and the accuracy of letters that have been positioned correctly in the sentence.\n",
    "\n",
    "Also if you want to try it with the predict function written in the section 4.2, you only have to replace the predictor.predict for a predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helping function\n",
    "def equalArrays(arr1, arr2):\n",
    "    equal = True\n",
    "    for i1, i2 in zip(arr1, arr2):\n",
    "        if i1!=i2:\n",
    "            equal = False\n",
    "    return equal\n",
    "\n",
    "# code to evaluate the endpoint on test data\n",
    "# returns a variety of model metrics\n",
    "# def sentence_accuracy(predictor, sentence, verbose=True):\n",
    "def evaluate(sentence, ground_truth, lengths, dictionary, model, device, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a test set given the prediction endpoint.  \n",
    "    Return binary classification metrics.\n",
    "    :param predictor: A prediction endpoint\n",
    "    :param test_features: Test features\n",
    "    :param test_labels: Class labels for test data\n",
    "    :param verbose: If True, prints a table of all performance metrics\n",
    "    :return: A dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtain the result of the prediction\n",
    "    predict_array = predict(sentence, model)\n",
    "#     predict_array = predictor.predict(sentence)\n",
    "       \n",
    "    # Both sentences have to have the same number of words\n",
    "    if len(sentence) != len(ground_truth):\n",
    "        print('error in the length obtained')\n",
    "        return 0\n",
    "    \n",
    "    correct_w = 0\n",
    "    correct_l = 0\n",
    "    \n",
    "    for original, result, length in zip(ground_truth, predict_array, lengths):\n",
    "        if equalArrays(original, result):\n",
    "            print('Correct prediction: Original {} -> Result {}'.format(join_sentence(convert_back_and_unpad(int2letter, original, length)), string))\n",
    "            correct_w += 1\n",
    "        for lo, lr in zip(original, result):\n",
    "            if lo == lr:\n",
    "                correct_l += 1\n",
    "                \n",
    "    accuracy_w = correct_w / len(sentence)\n",
    "    accuracy_l = correct_l / np.sum(lengths)\n",
    "    \n",
    "    return accuracy_w, accuracy_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to read the test.csv file to obtain all the test words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_sample = pd.read_csv(os.path.join(cache_dir, 'test.csv'), header=None, names=None)\n",
    "eval_sample_x = evaluate_sample[evaluate_sample.columns[35:69]].to_numpy(copy=True)\n",
    "eval_sample_len = evaluate_sample[evaluate_sample.columns[34]].to_numpy(copy=True)\n",
    "eval_sample_y = evaluate_sample[evaluate_sample.columns[0:34]].to_numpy(copy=True)\n",
    "\n",
    "acc_w, acc_l = evaluate(eval_sample_x, eval_sample_y, eval_sample_len, letter2int, model, device)\n",
    "# acc_w, acc_l = evaluate(eval_sample_x, eval_sample_y, eval_sample_len, letter2int, None, None)\n",
    "print('Word Accuracy: {}'.format(acc_w))\n",
    "print('Letter Accuracy: {}'.format(acc_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Delet the endpoint & Final cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts a predictor endpoint as input\n",
    "# And deletes the endpoint by name\n",
    "def delete_endpoint(predictor):\n",
    "        try:\n",
    "            boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)\n",
    "            print('Deleted {}'.format(predictor.endpoint))\n",
    "        except:\n",
    "            print('Already deleted: {}'.format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the predictor endpoint \n",
    "delete_endpoint(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we have cleaned deleted all our endpoints, S3 bucket, models, and endpoint configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.getsizeof(X_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
