{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT: CAN THE COMPUTERS READ IT TOO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cache' # The folder we will use for storing data\n",
    "data_dir = os.path.join(cache_dir, 'data')\n",
    "model_dir = os.path.join(cache_dir, 'model')\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DURING THE NOTEBOOK YOU WILL FIND VARIOUS NUMBER OF CELLS THAT CONSISTS ONLY TO CLEAN FILES IN ORDER TO HAVE AS MUCH SPACE POSSIBLE AND NOT HAVE PROBLEMS WITH IT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE TRAIN DATA NEEDS TO BE TRANSFORMED AS WE DO, BUT THE TEST DATA DOESNT NEED TO AS THE PREDICT WILL CONVERT FROM THE STRING TO THE ADECUATED FORM FOR THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip already downloaded\n",
      "zip already unziped\n"
     ]
    }
   ],
   "source": [
    "if 'blogger.zip' not in os.listdir('.'):\n",
    "    !wget -O blogger.zip http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip\n",
    "else:\n",
    "    print('zip already downloaded')\n",
    "if 'blogs' not in os.listdir('.'):\n",
    "    !unzip -Z1 blogs.zip | head -10 | sed 's| |\\\\ |g' | xargs unzip blogs.zip\n",
    "else:\n",
    "    print('zip already unziped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# print(len([name for name in os.listdir('blogs') if os.path.isfile(name)]))\n",
    "print(len(os.listdir('blogs/')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was pretty big with, maybe, too much files (considering that inside each blog normally there are more than one post) so only a subset of it was unzipped. The subset selected is on the head part of the command, in this case resulted in 9 blogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETE ZIP FILE IF YOU ARE NOT USING ANYMORE TO CLEAR SOME SPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Preparing and Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Blog>\n",
      "\n",
      "<date>30,July,2004</date>\n",
      "<post>\n",
      "\n",
      "\t \n",
      "       Planning the Marathon   I checked Active.com, and the registration for the 2005 Spirit of St. Louis Marathon doesn't open until 8am August 15.&nbsp; I'm planning to get online at 8:01am and signup.&nbsp;   I like doing triathlons andwill probably sign up for another one next year, but I think that I'm going to make running my primary focus for the next 6 months after the tri, starting with the half-marathon.&nbsp; I think I'm going to find a 10k for October, and maybe join the StL Track Club for their winter race series. \n",
      "    \n",
      "</post>\n",
      "\n",
      "<date>22,July,2004</date>\n",
      "<post>\n",
      "\n",
      "\t \n",
      "       I Must Be Crazy   So, here I am, less than 6 months from my first ever race (2004 Spirit of St. Louis 5K - 34:57), and now I've signed up for the Lewis &amp; Clark Half-Marathon on Sept. 18, 2004. I'm trying to work up to the 2005 Spirit of St. Louis Marathon. Then I want to run an ultra next year, too. I think I can do it...if it's a road ultra, at least. I'm looking forward to increasing my mileage slowly. This is the start of my blog to chronicle my training to reach these goals.  \n",
      "    \n",
      "</post>\n",
      "\n",
      "<date>12,August,2004</date>\n",
      "<post>\n",
      "\n",
      "\t \n",
      "       Running \"Up\" in Forest Park   There is a 6 mile loop (it's actually about 5.7 miles) at Forest Park in St. Louis.  The St. Louis Triathlon Club runs there every Wednesday at 5:30pm, and as a member, I try to get out to run with them.  One way around the park goes up a long hill parallel to Skinker Ave., and the other way goes down.  I run with the club a month ago and went \"down\" in 1:06 and some seconds (5.70mi).  I figure that I paced myself pretty good that time, I went through the first mile really fast, and that had been my longest run of the year.  Yesterday, we ran \"up\", and it turns out the distance up is 5.72, but then again, you are running on the outside lane, so that's the difference.  1:04:08!  Good pacing, strong running, and the uphill was only about a minute and a half slower than the overall pace of the run, so that's great.  I'm sure my down time will improve significantly my next time out.  I feel strong and fast at the end of my 4-6 miles runs now, which means the long runs are doing what they are supposed to do.  My asthma is still holding me back during the second and third mile, but I think I'm going out too fast.  I went under 10:00 pace for the first mile again yesterday.  If I didn't have to take so many walking breaks, I think that I would be able to hold 10:xx pace the whole time.  Also, my left calf was killing me yesterday and this morning until I hit it with some cold water in the shower.  Adjusting to my new shoes (Pearl Izumi Push+) is going to be a pain in the ass.  *\"Up\", \"down\"...this sounds like the Comrades Marathon...only about 10 times shorter* \n",
      "    \n",
      "</post>\n",
      "\n",
      "<date>10,August,2004</date>\n",
      "<post>\n",
      "\n",
      "\t \n",
      "       Too Many Days Off   Yeah, that's right, a 5 day layoff.  All that talk about skipping a day, and wanting to run, but, alas, life changed and it was not meant to be.  I hung out with my wife on Friday because she got a day off, and then Saturday I worked for about 9 hours on her car, and I just didn't have the energy to do 9 miles after that.  But I finally ran today.   4.09mi in 48:19.  I finished really strong, and I tried to push through and run most of it, but I just don't have the legs yet for 4 miles straight running. But that's cool. I'm ready to keep up the work. It's all about improving over months, not weeks. I'm looking to improve my speed for the next year. That's the goal. I know the endurance will go up as I crank the mileage up. I think I'll add some speedwork in the form of fartleks or long intervals (800-1600) to finish out this year, maybe even Yasso 800's, and then put snap in my running next year with quarters.  I'm thinking about getting Daniel's Running Formula.  I've heard it's a very good book. \n",
      "    \n",
      "</post>\n",
      "\n",
      "<date>06,August,2004</date>\n",
      "<post>\n",
      "\n",
      "\t \n",
      "       On Days Not Running...   I don't know how I feel yet about not running and wanting to. I wanted to run yesterday, but I didn't. I figured that I needed to take a day off, because I'd run the two previous days, and then I'm running tonight and the long run (9 miles) on Saturday. I guess I'm okay with that, but it's weird wanting to run and not going. I know that I need to take a day off, and I can't crank up my mileage too fast or I'll hurt myself. That would be bad with only 4 weeks to the triathlon and 6 weeks to the half-marathon. I think I'm starting to get nervous, but I'm not recognizing it yet. I know I can finish both races, and I think I'll hit my time goals (sub-1:20:00 for the tri and under 2:45:00 for the half-marathon -- that's 12:30min/mi pace), so that's not the problem. I'm just not used to this kind of pressure. But it's GOOD pressure, don't get me wrong. Having goals and making plans of this sort is a good thing. And it all leads me toward a healthier, happier life. I kinda like to be a little twisted about all this. It makes me realize how much I want it, and how much I am willing to work for it. \n",
      "    \n",
      "</post>\n",
      "\n",
      "<date>05,August,2004</date>\n",
      "<post>\n",
      "\n",
      "\t \n",
      "       Going nowhere fast...   I hate treadmill running.  I mean, okay, I know I should be thankful, 'cause I can run anytime, and I never would have gotten started without the treadmill at the Y, but man, time sure does drag, except when I'm on a walk break, and then I forget how long I've walked 'cause I'm too busy listening to music or watching the TV's.  Yes, yes, I know, it's my fault.  I don't have to like it, though.  Here's the details:  4.00 miles - 48:46  On to better stuff...I'm really enjoying this whole running thing.  I started to do this to keep my weight dropping and health and yadda, yadda, but I'm really starting to like it.  And I'm looking at getting a second pair of shoes.  Turns out I underpronate a little, so I'm checking out the Pearl Izumi Push+.  And at only 80 bucks, I can't go wrong.   I think I want to race in October... \n",
      "    \n",
      "</post>\n",
      "\n",
      "<date>04,August,2004</date>\n",
      "<post>\n",
      "\n",
      "\t \n",
      "       Old Race Reports   This is a copy of my old race report to the Tri-deads.   RR: My very first race (or, Let the sneezing begin).. .  Sorry this is so late. What can I say, I got married two weeks later. :0)  St. Louis Marathon CEO 5K Run/Walk April 3, 2004 Time: 34:57  Short report: Fun race, got to watch the Women's Olympic Marathon Trials beforehand, perfect day to run (sunny and about 70 degrees), and I got enough dust up my nose to sneeze continuously for three days afterwards.  Long report:  I decided a long time ago that I was going to run a couple of 5K's before my first triathlon to test fitness and get some experience out in a race atmosphere, considering I do most of my training by myself. I wanted something big enough that I wouldn't be alone on the course when they picked up the finish line, but I didn't want to have to travel too far to get there. What better choice than a run in St. Louis, which is about 20 min. from my house (even though I live in Illinois). So I signed up for the 5K, a good starters distance I felt (and just slightly longer than the distance of the run for my tri in Sept.), and because I figured I had too much facial hair to sneak into the 1 mile fun run for the kids. I signed up around Dec. and didn't here much back after that, except that packet pickup was to be the day before the race (turns out I got number 20...talk about signing up early!). So I set about increasing my distance slowly, improving my speed, etc. Then the problems started, as they always do. First, I go and get engaged to the woman I want to spend the rest of my life with...there goes 14 whirlwind days of training out the window as I turned from a die-hard training fool into a soft and lovable never-able-to-go-away-from-my-girl fool. Ah, well, it happens, so I wasn't concerned. Ironically enough, my first day back training was...Valentine's Day...go figure. Next comes the exciting world of calf injuries, but I was able to rest and ice and get back on the treadmill to tough it out. Finally, I get to race day. My first ever. And the day of the Marathon Trials. So I wake up early, put down a Snicker's Marathon bar and start sipping on the Gatorade as I drive to the race site in beautiful Forest Park. If you ever come to St. Louis, this is the place to run! Wide bike/walking paths with wonderful scenery and just minutes from downtown (sorry, have to plug my home...Go Cardinals!!!). Anyway, I caught the last 30 minutes of the race and watched as Deena made the team with a second place finish, along with Colleen De Reuck winning and Jen Rhines taking third place. I then cruised over to the starting line of my race to wait out the last hour before it was time to go. Did I mention that drinking 32 oz. of Gatorade and 18 oz. of water an hour before a race does nothing but make you jog to the bathroom for that remain hour? Good times, let me tell you. So, after about 45 minutes of waiting and peeing and peeing some more, my parents showed up to wish me luck and watch my shuffle around in Forest Park. Oh, did I mention that my longest run before this was 5K...in February!!! I was looking forward to the first 2 miles but after that...well, we were just going to see what happened on the way back in. Finally, everyone started to herd over to the road were the race was to begin. I sprinted to the bathroom one last time (turns out that was the fastest I ran all day), and made my way into the crowd. I headed for the back of the pack (ah, home sweet home) and stood around waiting and joking with a few of the people around me. Suddenly, about 2 minutes to 11am, we hear a bang and everyone takes off running...and then stops. It turns out that the tire on a running stroller had popped, and sounded remarkably like a starter's pistol. Well, we stood around a little more and then everyone started to move in front of us. So we took off running. The first 200 meters was a nice downhill trot (\"short strides, save the knees\") and then onto the back portion of the marathon course, which was flat. Everything went quite smoothly, I was feeling good, and hanging with the guy I was talking to before the race (his name was Bill). We cruised out around a few corners and onto the flat roadways that made up the bulk of the course. The wind was blowing up a lot of dirt that made my eyes sting by the time I got to the water station, which was about halfway through. I can say with some certainity that I did not make it to the first mile markers before the leaders came back through, and we all cheered loudly for them. I was afraid at one point that a few of the thinner ones might be blown off the course from the 10 mph crosswind, but everybody made it out alright. I only had to walk a few times during the race, mostly because I can't drink water from a plastic cup and shuffle at the same time. And I got to spill most of it down the front of my shirt in front of the motorcycle cops watching a group of young, attractive ladies behind me. Yes, yes, I am Mr. Smooth. Lots of fun, nonetheless. It's quite a lot of fun to be able to run with other people. I especially enjoyed picking out the next runner in line and trying to catch them, and then doing it with the next person after I passed them. I finished strong and made sure I shuffled quickly the last 200 meters or so and started to haul @$$ when I saw that I could finish in under 35 minutes. And I did, with a 34:57. WOOHOO! Bill finished just behind me (about 600 meters out we decided to kick it in...yeah, that's what they call it). As soon as I hit the finish line and stopped running the sneezing hit. My nose went off every 15 - 20 seconds for the next four hours. I must have put half of the prairie up my nostrils, because the sneezing didn't subside for three days. I didn't care, through, 'cause I went out and ran and had a great time. The post race bagels from The St. Louis Bread Company hit the spot and I enjoyed myself immensely. I can't wait for the next race.  Have a great day, everybody!!!  Adam\n",
      "    \n",
      "</post>\n",
      "\n",
      "<date>04,August,2004</date>\n",
      "<post>\n",
      "\n",
      "\t \n",
      "       Chasing the dream...or being chased while dreaming?   So I was out for my run last night, a quick 3 miles before my longer run tonight.  I was pushing harder than usual to try and pick up my average pace, and it was already 7:30pm when I started, so I wanted to go home and eat.  Well, I got out to my turnaround point and found a collection of some kind of flying insect buzzing the trail.  I tried to run around them, and suddenly, I got dive bombed.  So, I turned around and burned rubber to get out of there.  Picture this, me, sprinting down the trail, being buzzed by some kind of bee/wasp/hornet/horsefly at my overwhelming 9 min/mile pace.  Yeah, sue me, I have allergies.   When I got home, I thought about this (searching for the positive).  It must have looked pretty damn funny, actually.  Well, I got a good, hard run in, so I can't complain.  I was also able to calm my breathing after the panting from the initial scare and the subsequent attacks and keep it steady.  That's got to mean something positive.  Here's the details:  3.04 miles - 33:20  In the words of REO Speedwagon: Keep pushing on!! \n",
      "    \n",
      "</post>\n",
      "\n",
      "\n",
      "</Blog>\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join('blogs', '*.xml')\n",
    "files = glob.glob(path)\n",
    "\n",
    "with open(files[0]) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, each blog consists of different posts, which we will separate, as we are only interested in posts, and we don't want the dates or other info that could be inside the xml's files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As XML files they consists of different tags. We are only gonna take the **post** ones.\n",
    "    \n",
    "We used the beatiful soup to get the texts, that because with other parsers like ElementTree it resulted in ParseError in some files, as there are some HTML elements (like &nspb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x94 in position 26845: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7af62e0e2a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mblogs_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_blogs_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-7af62e0e2a7e>\u001b[0m in \u001b[0;36mread_blogs_data\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mblog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m        \u001b[0;31m# It's a file-type object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         elif len(markup) <= 256 and (\n\u001b[1;32m    288\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34mb'<'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x94 in position 26845: invalid start byte"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_blogs_data(data_dir='blogs'):\n",
    "    data = []\n",
    "\n",
    "    path = os.path.join(data_dir, '*.xml')\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    for f in files:\n",
    "        blog = open(f)\n",
    "\n",
    "        soup = BeautifulSoup(blog, 'xml')\n",
    "        posts = soup.find_all('post')\n",
    "\n",
    "        for post in posts:\n",
    "            data.append(post.text)\n",
    "    return data\n",
    "\n",
    "blogs_data = read_blogs_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there is some error, something related to the decoding.\n",
    "After searching for some time we find a possible solution to which type are the files encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'ascii', 'confidence': 1.0, 'language': ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "blog_dir = os.path.join('blogs', os.listdir('blogs')[0])\n",
    "blog = open(blog_dir, 'rb')\n",
    "chardet.detect(blog.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we use this encoding into the parsing of all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_blogs_data(data_dir='blogs'):\n",
    "    data = []\n",
    "\n",
    "    path = os.path.join(data_dir, '*.xml')\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    for f in files:\n",
    "        blog = open(f, encoding=\"ISO-8859-1\")\n",
    "\n",
    "        soup = BeautifulSoup(blog, 'xml')\n",
    "        posts = soup.find_all('post')\n",
    "\n",
    "        for post in posts:\n",
    "            data.append(post.text)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_data = read_blogs_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of posts = 1199\n"
     ]
    }
   ],
   "source": [
    "#number of posts\n",
    "print(\"# of posts = {}\".format(len(blogs_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post50:  \n",
      "\n",
      "\t \n",
      "      Wow, what an emotional rollercoaster of a weekend. I had a great day Satuday! Kelly drove me home. It was amazing. I mean, I've known this girl since I was like...18 months! It's just so weird to be in the passenger seat and not have a parent there, so yeah, kathryn's house was super duper fun...I was surprised, like seriously. I had a GREAT time. I felt really bad though since I kind of went last minute, and I tried to call Diane's cell, but I kept getting a busy signal, and, arg, it was bad!  But anyway...yeah, the night was great...until I got home. I had a long talk with Kelly C., and...I've just been a horrible friend to both Kelly and Emily for the past 6 months. I don't want to go into details, because the past is the past, and...I can't change it, but....I apologized and truly meant it.   Then today, I talked to them both individually, and...I definitely got off too easy, but they forgave me! I seriously couldn't believe it. They forgave me within a matter of seconds. I was just so happy and shocked...and...relieved. I didn't lose two of my best friends. Phew! Yeah, I definitely did get off easy, they forgave me way too fast, but...no complaints here. It taught me a huge lesson though. I'm never ever taking my friends for granted again, and I'm definitely going to think of how a friend would feel, rather than how I feel about the situation, because I'm going to feel differently about things than other people are, and I'm going to be really layed back about something that could be really important to a friend, and I have to respect that.   Phew...so, huge rollercoaster weekend that let me end up on the top! I'm feeling great right now, my only worry, would be that Kelly and Emily don't believe how sorry I really am, but...I think they do. :) I have such great friends. I'm really lucky.   My day today was cool. Didn't do anything too special. Kath was gone during government though, so I was sad.  We watched this really sad video in health. It was an Oprah show with Dr. Phil, and it had this girl on it who was raped when she was 13, and now she's bulemic, and she tried to kill herself, and it was just so depressing. Joanne and I sat in the back corner and joked for the first half, but...when the girl came on, we stopped. It was too sad. I wish Kelly sat near me. She's in my health class too. Like, I want Kelly to sit in front of me, and Joanne next to me, or vise versa, and it'd be a perfect world...well, for Health at least.  I actually have friends in my classes this semester. Yeah, last semester I had music theory, but...not much else where I had friends. It's great!   Yeah, anyway, I'm going to go...no homework to do, but, I'm still going to go. I think I may go read. Goodnight!  XOXO  Emily and Kelly...you're the coolest, and I'll never ever do anything to hurt you again. :) Thanks for forgiving me, you're awesome.\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check them\n",
    "print('post50: ', blogs_data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data parsed, which consists of 1199 posts, although there are only 9 blogs used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the posts text in the 'blogs_data' array, so we will apply to it the processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To start, we shuffle all the posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post50 shuffled:  \n",
      "\n",
      "\t \n",
      "      Ohhh great, now Diane thinks I'm mad at her because of Pizza Hut and whatnot... :( It wasn't that I didn't have fun at Pizza Hut, it's that I was just in such a bad mood beforehand...that, I just didn't even really let myself have a good time. Oh well. I'll call her later and apologize. I didn't mean to bring my bad mood down on her, and I really did appreciate that she invited me. That was probably the one good thing about that day. Oh well, I'll talk to her and apologize and hopefully she'll forgive me. \n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "blogs_data_shuffled = shuffle(blogs_data)\n",
    "print('post50 shuffled: ', blogs_data_shuffled[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply different processing steps:\n",
    "    - remove the \\n, \\t, whitespaces starting and ending\n",
    "    - remove possible html tags and other\n",
    "    - put all in lowercase\n",
    "    - remove accents in some chars\n",
    "    - separate the posts by words with the help of the indices\n",
    "    - get the words array from the indices\n",
    "    - ignore words that have chars not considered integers, letters nor punctuation\n",
    "    - also ignore words which are only integers or punctuation\n",
    "    - limit the posts to 500 words per posts (sentence)\n",
    "    - we ignore the words larger than supercalifragilisticexpialidocious\n",
    "    \n",
    "So finally, a list of words are returned. These words include punctuation, which will have to take into account later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len('supercalifragilisticexpialidocious'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def splitWithIndices(s, c=' '):\n",
    "    p = 0\n",
    "    for k, g in groupby(s, lambda x:x==c):\n",
    "        q = p + sum(1 for i in g)\n",
    "        if not k:\n",
    "            yield p, q # or p, q-1 if you are really sure you want that\n",
    "        p = q\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def post_to_words(post):\n",
    "    LIMIT_WORDS = 500\n",
    "\n",
    "    post = post.lstrip().rstrip()\n",
    "    post = BeautifulSoup(post, \"html.parser\").get_text()\n",
    "    post = post.lower()\n",
    "    \n",
    "    words_accents = []\n",
    "    for word in post:\n",
    "        words_accents.append(strip_accents(word))\n",
    "        \n",
    "    words_ind = splitWithIndices(words_accents)\n",
    "        \n",
    "    words_aux = []\n",
    "    for idx in words_ind:\n",
    "        words_aux.append(post[idx[0]:idx[1]])\n",
    "    \n",
    "    words = []\n",
    "    for idx, word in enumerate(words_aux):\n",
    "        isvalid = True\n",
    "        isdigit = True\n",
    "        ispunct = True\n",
    "\n",
    "        for letter in word:\n",
    "            if not letter.isdigit():\n",
    "                if letter not in string.punctuation:\n",
    "                    if not letter.isalpha():\n",
    "                        isvalid = False\n",
    "            \n",
    "            if not letter.isdigit():\n",
    "                isdigit = False\n",
    "            if letter not in string.punctuation:\n",
    "                ispunct = False\n",
    "                \n",
    "        if isvalid == True and isdigit == False and ispunct == False:\n",
    "            if len(word) <= len('supercalifragilisticexpialidocious') and idx <= LIMIT_WORDS:\n",
    "                words.append(word)\n",
    "        \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE DICTIONARY TO CHECK FOR REPEATED WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\t \n",
      "       Digital Photography    urlLink Photoxels  is a nice site on digital photography. There are nice and detailed tutorials on digital photography terminology. Besides this, the site has one-pager format tutorials which give out various tips about taking pictures.  Though the digital camera reviews are not as informative and exhaustive as  urlLink Steve's  or  urlLink DCResource , but overall the site makes an interesting reading and is worth a visit.  My favourite forum site for digital photography is  urlLink DPreview . The site has categorized listings according to the camera models and brands. That makes searching a lot more easier than the usual single forum posting method. I like the ease of usability and the whole concept of the forum site.  \n",
      "     \n",
      "    \n",
      "\n",
      "['digital', 'photography', 'urllink', 'photoxels', 'is', 'a', 'nice', 'site', 'on', 'digital', 'photography.', 'there', 'are', 'nice', 'and', 'detailed', 'tutorials', 'on', 'digital', 'photography', 'terminology.', 'besides', 'this,', 'the', 'site', 'has', 'one-pager', 'format', 'tutorials', 'which', 'give', 'out', 'various', 'tips', 'about', 'taking', 'pictures.', 'though', 'the', 'digital', 'camera', 'reviews', 'are', 'not', 'as', 'informative', 'and', 'exhaustive', 'as', 'urllink', \"steve's\", 'or', 'urllink', 'dcresource', 'but', 'overall', 'the', 'site', 'makes', 'an', 'interesting', 'reading', 'and', 'is', 'worth', 'a', 'visit.', 'my', 'favourite', 'forum', 'site', 'for', 'digital', 'photography', 'is', 'urllink', 'dpreview', 'the', 'site', 'has', 'categorized', 'listings', 'according', 'to', 'the', 'camera', 'models', 'and', 'brands.', 'that', 'makes', 'searching', 'a', 'lot', 'more', 'easier', 'than', 'the', 'usual', 'single', 'forum', 'posting', 'method.', 'i', 'like', 'the', 'ease', 'of', 'usability', 'and', 'the', 'whole', 'concept', 'of', 'the', 'forum', 'site.']\n",
      "\n",
      "123456 ., -> []\n"
     ]
    }
   ],
   "source": [
    "# Check it\n",
    "words = post_to_words(blogs_data_shuffled[60])\n",
    "print(blogs_data_shuffled[60])\n",
    "print(words)\n",
    "\n",
    "word1 = '123456 .,'\n",
    "w1 = post_to_words(word1)\n",
    "print('\\n{} -> {}'.format(word1, w1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below applies the post_to_words method for all the post in all the blogs in the dataset indicated. In addition it caches the results. This is because performing this processing step can take a long time. This way if you are unable to complete the notebook in the current session, you can come back without needing to process the data a second time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insted of having have the data as sentences, we will have it as words, as the input of the model will be word by word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we are only using one set of data, as this is the one that later will be divided into training and test, and also will be transformed to have the input and the output (jumbled or not jumbled words respectively).\n",
    "\n",
    "We ignore the sentences which for whenever reason have length 0, meaning all the words has been ignored (following the rules stated in the post2words) and ignore the words with length less than 4 letters, as they will not add anything to the model once jumbled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each post to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    file_dir = os.path.join(cache_dir, cache_file)\n",
    "\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", file_dir)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_data = []\n",
    "        perc = 0\n",
    "        \n",
    "        for idx_p, post in enumerate(data):\n",
    "            \n",
    "            if idx_p / len(data) >= perc:\n",
    "                print('{} / {} sentences = {}%'.format(idx_p, len(data), np.round(perc*100, decimals = 1)))\n",
    "                perc = perc+0.1\n",
    "            \n",
    "            words = post_to_words(post)\n",
    "            \n",
    "            words_len = [word for word in words if len(word) > 3] #ignore words with less than 4 letters\n",
    "            if len(words_len) != 0:\n",
    "                words_data += words_len\n",
    "                \n",
    "        \n",
    "#         words_data = [post_to_words(post) for post in data]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_data=words_data)\n",
    "            with open(file_dir, \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", file_dir)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_data = (cache_data['words_data'])\n",
    "    \n",
    "    return words_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: cache/preprocessed_data.pkl\n",
      "The dataset consists of 96737 words\n"
     ]
    }
   ],
   "source": [
    "# Process all the blogs\n",
    "if not os.path.exists(cache_dir): # Make sure that the folder exists, if not create it\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "words_data = preprocess_data(blogs_data_shuffled, cache_dir)\n",
    "print('The dataset consists of {} words'.format(len(words_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more than 90mil words! Remember that only 9 blogs were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length word: 34\n",
      "min length word: 4\n"
     ]
    }
   ],
   "source": [
    "# check the length of number of letters per word is adecuated\n",
    "maxlen_w = 0\n",
    "minlen_w = 1000\n",
    "\n",
    "for word in words_data:\n",
    "    if len(word) > maxlen_w:\n",
    "        maxlen_w = len(word)\n",
    "    if len(word) < minlen_w:\n",
    "        minlen_w = len(word)            \n",
    "            \n",
    "print('max length word: {}'.format(maxlen_w))\n",
    "print('min length word: {}'.format(minlen_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have checked that the dataset we are going to use consists of words that have limited lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 The dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we are going to construct in this notebook we will construct a feature representation which consists in represent each letter of each word as an integer. We will be using the latin alphabet as a dictionary between letters and integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26} \n",
      "\n",
      "26 {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "letter2int = dict(zip(string.ascii_lowercase, range(1,27)))\n",
    "int2letter = {v: k for k, v in letter2int.items()}\n",
    "\n",
    "print(len(letter2int), letter2int, '\\n')\n",
    "print(len(int2letter), int2letter, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Transform the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word dictionary which allows us to transform the letters appearing in the words into integers, it is time to make use of it and convert our posts words to their integer sequence representation. Since we will be using a recurrent neural network, it will be convenient if the length of each word is the same. To do this, we will use the previous defined max size (the length of the word supercalifragilisticexpialidocious) as the fixed size for our words and then pad short words with the category 'no letter' (which we will label 0) and truncate long words.\n",
    "\n",
    "For now, the punctuations and numbers will be ignored, as it will simplify drastically the task. If there's enough time in the future, it could be possible to implemented some kind of punctuation mark holder that allows to save the punctuation of the sentence and put them back after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(letter_dict, word, pad = 34):\n",
    "    NOLETTER = 0 # We will use 0 to represent the 'no letter' category\n",
    "    \n",
    "    word_padded = []\n",
    "    length = len(word)\n",
    "    \n",
    "    #conversion\n",
    "    for letter_index, letter in enumerate(word):\n",
    "        if letter in letter_dict:\n",
    "            if letter_dict[letter] >= 0:\n",
    "                word_padded.append(letter_dict[letter])\n",
    "        else:\n",
    "            length -= 1\n",
    "    \n",
    "    #padding\n",
    "    if len(word_padded) < pad:\n",
    "        word_padded = (word_padded + pad * [NOLETTER])[:pad]\n",
    "            \n",
    "    return word_padded, length\n",
    "\n",
    "def convert_and_pad_data(letter_dict, data, pad = 34):\n",
    "    \n",
    "    result = []\n",
    "    lengths = []\n",
    "\n",
    "    perc = 0        \n",
    "    \n",
    "    for idx_w, word in enumerate(data):\n",
    "        \n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} word = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "        \n",
    "#         print('word------>', word)\n",
    "        converted_word, len_word = convert_and_pad(letter_dict, word, pad)\n",
    "#         print('word------>', word, leng_word)\n",
    "#         print('word converted--->', converted_word, leng_word)\n",
    "\n",
    "        result.append(converted_word)\n",
    "        lengths.append(len_word)\n",
    "        \n",
    "#     return np.array(result), np.array(lengths)\n",
    "    return result, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 96737 word = 0%\n",
      "9674 / 96737 word = 10.0%\n",
      "19348 / 96737 word = 20.0%\n",
      "29022 / 96737 word = 30.0%\n",
      "38695 / 96737 word = 40.0%\n",
      "48369 / 96737 word = 50.0%\n",
      "58043 / 96737 word = 60.0%\n",
      "67716 / 96737 word = 70.0%\n",
      "77390 / 96737 word = 80.0%\n",
      "87064 / 96737 word = 90.0%\n",
      "Previous lengths: 96737-96737 \n",
      "New length: 90469-90469\n"
     ]
    }
   ],
   "source": [
    "words_data_padded, words_data_padded_len = convert_and_pad_data(letter2int, words_data)\n",
    "\n",
    "# There are some words that after the transformation will stay at less than 4 letters, so they will be disposed now.\n",
    "words_data_p = [] \n",
    "len_data_p = []\n",
    "for word, lenw in zip(words_data_padded, words_data_padded_len):\n",
    "    if lenw > 3: #Check again length words\n",
    "            words_data_p.append(word)\n",
    "            len_data_p.append(lenw)\n",
    "            \n",
    "print('Previous lengths: {}-{} \\nNew length: {}-{}'.format(len(words_data_padded), len(words_data_padded_len), \n",
    "                                                           len(words_data_p), len(len_data_p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [8, 1, 20, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4 [6, 5, 5, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "for ind_w in [2, 8]:\n",
    "    print(words_data_padded_len[ind_w], words_data_padded[ind_w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally our dataset will consist of 90 thousand words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_data = None\n",
    "blogs_data_shuffled = None\n",
    "words = None\n",
    "words_data = None\n",
    "words_data_padded = None\n",
    "words_data_padded_len = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Jumble the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the objective is to get back the words after being jumbled, we have now to jumble this data. We create a function to jumble the words, and then another function to handle all the words in a sentence.\n",
    "\n",
    "Probabilistically, the result of the function could be the same word passed as input, but we will ignore this possible case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jumble_word(word, word_len):\n",
    "    if word_len <= 2:\n",
    "        word_j = word\n",
    "        \n",
    "    else:\n",
    "        sub_word = []\n",
    "        for w in word:\n",
    "            sub_word.append(w)\n",
    "\n",
    "        sub_word = sub_word[1:word_len-1]\n",
    "        shufled_word = shuffle(sub_word)\n",
    "        \n",
    "        word_j = word.copy()\n",
    "        word_j[1:word_len-1] = shufled_word\n",
    "        \n",
    "    return word_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 6, 3, 2, 4, 7, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Check it\n",
    "word = [1,2,3,4,5,6,7,0,0,0,0,0,0,0]\n",
    "len_w = 7\n",
    "print(jumble_word(word, len_w))\n",
    "\n",
    "word = [1,2,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "len_w = 2\n",
    "print(jumble_word(word, len_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jumble_data(data, data_len):\n",
    "    jumbled_data = []\n",
    "    \n",
    "    idx_w = 0\n",
    "    perc = 0\n",
    "    for word, w_len in zip(data, data_len):\n",
    "        jumbled_sentence = []\n",
    "\n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} words = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "\n",
    "        jumbled_word = jumble_word(word, w_len)\n",
    "\n",
    "        jumbled_data.append(jumbled_word)\n",
    "        idx_w+=1\n",
    "        \n",
    "    return jumbled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 90469 words = 0%\n",
      "9047 / 90469 words = 10.0%\n",
      "18094 / 90469 words = 20.0%\n",
      "27141 / 90469 words = 30.0%\n",
      "36188 / 90469 words = 40.0%\n",
      "45235 / 90469 words = 50.0%\n",
      "54282 / 90469 words = 60.0%\n",
      "63329 / 90469 words = 70.0%\n",
      "72376 / 90469 words = 80.0%\n",
      "81423 / 90469 words = 90.0%\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "jumbled_data = jumble_data(words_data_p, len_data_p)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: [15, 20, 8, 5, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length: 5\n",
      "Jumbled: [15, 5, 8, 20, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Word: [1, 12, 23, 1, 25, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length: 6\n",
      "Jumbled: [1, 12, 25, 23, 1, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "for ind_w in [4, 6]:\n",
    "    print('Word: {}'.format(words_data_p[ind_w]))\n",
    "    print('Length: {}'.format(len_data_p[ind_w]))\n",
    "    print('Jumbled: {}\\n'.format(jumbled_data[ind_w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPROBAR QUE TOTES LES PARAULES ESTA BEN FET LU DE LA LLARGADA (QUE CAP 0 S'HAGI COLAT)\n",
    "\n",
    "tambe sembla que posi 0's entremig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Split training and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data ready, only needs to be separated into training and test sets, as it was shuffled earlier it is not necesary to shufle them again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_size = 0.25\n",
    "rnd_state = 42\n",
    "\n",
    "trainX, testX = train_test_split(jumbled_data, test_size = split_size, random_state = rnd_state)\n",
    "\n",
    "trainY, testY = train_test_split(words_data_p, test_size = split_size, random_state = rnd_state)\n",
    "\n",
    "train_len, test_len = train_test_split(len_data_p, test_size = split_size, random_state = rnd_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 90469, output: 90469\n",
      "train input: 67851, train output: 67851 \n",
      "test input: 22618, test output: 22618\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Check sizes coincide\n",
    "print('input: {}, output: {}\\ntrain input: {}, train output: {} \\ntest input: {}, test output: {}'.format(\n",
    "    len(jumbled_data), len(words_data_p), len(trainX), len(trainY), len(testX), len(testY)))\n",
    "\n",
    "# Check the same numbers are used in both input and output on same sentence number \n",
    "for word1, word2 in zip(trainX, trainY):\n",
    "    sum_num=0\n",
    "    \n",
    "    if len(word1) != len(word2):\n",
    "        print('errror of size!')\n",
    "\n",
    "    for indx in range(len(word1)):\n",
    "        sum_num += word1[indx]-word2[indx]\n",
    "    \n",
    "    if sum_num != 0:\n",
    "        print('error of numbers!')\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1 Uploading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data to cache file: cache/data/input_train_data.pkl\n",
      "Wrote data to cache file: cache/data/input_test_data.pkl\n",
      "Wrote data to cache file: cache/data/output_train_data.pkl\n",
      "Wrote data to cache file: cache/data/output_test_data.pkl\n",
      "Wrote data to cache file: cache/data/length_train_data.pkl\n",
      "Wrote data to cache file: cache/data/length_test_data.pkl\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(data_dir): # Make sure that the folder exists, if not create it\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "# Uploading train and test files separately, we check it doesn't upload an empty file\n",
    "data_files = [trainX, testX, trainY, testY, train_len, test_len]\n",
    "cache_files = ['input_train_data.pkl', 'input_test_data.pkl',\n",
    "               'output_train_data.pkl', 'output_test_data.pkl',\n",
    "               'length_train_data.pkl', 'length_test_data.pkl']\n",
    "\n",
    "for data_file, cache_file in zip(data_files, cache_files):\n",
    "    cache_data = None\n",
    "    cache_data = dict(data_file=data_file)\n",
    "    file_dir = os.path.join(data_dir, cache_file)\n",
    "\n",
    "    with open(file_dir, \"wb\") as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "        \n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        print(\"Wrote data to cache file:\", file_dir)\n",
    "    else:\n",
    "        print('Wrote empty file on cache file', file_dir)\n",
    "        \n",
    "print('Done')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPROBAR QUE EL TEST FILE NO SIGUI L'ACUMULACIO DE LANTERIOR (PER AIXO ES MES GRAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are gonna clean some more variables as we will download them in the next sections or not gonna use them more\n",
    "len_data_p = None\n",
    "words_data_p = None\n",
    "jumbled_data = None\n",
    "trainX = None\n",
    "trainY = None\n",
    "train_len = None\n",
    "testX = None\n",
    "testY = None\n",
    "test_len = None\n",
    "word2 = None\n",
    "word1 = None\n",
    "word = None\n",
    "data_file = None\n",
    "data_files = None\n",
    "files = None\n",
    "cache_files = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2 Loading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cache_file, data_dir=data_dir):\n",
    "    cache_data = None\n",
    "    file_dir = os.path.join(data_dir, cache_file)\n",
    "\n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read data from cache file:\", file_dir)\n",
    "        except:\n",
    "            print('Problem reading the file', file_dir)\n",
    "    else:\n",
    "        print('File empty')\n",
    "\n",
    "    if cache_data is None:\n",
    "        print('Didnt read anythin')\n",
    "        resulting_files = []\n",
    "    else:\n",
    "        resulting_files = (cache_data['data_file'])\n",
    "        \n",
    "    return resulting_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already done the all the previous steps in the past, you should have the training and test split files in the cache/data folder.\n",
    "\n",
    "We will load them and the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from cache file: cache/data/input_train_data.pkl\n",
      "Read data from cache file: cache/data/input_test_data.pkl\n",
      "Read data from cache file: cache/data/output_train_data.pkl\n",
      "Read data from cache file: cache/data/output_test_data.pkl\n",
      "Read data from cache file: cache/data/length_train_data.pkl\n",
      "Read data from cache file: cache/data/length_test_data.pkl\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "cache_files = ['input_train_data.pkl', 'input_test_data.pkl',\n",
    "               'output_train_data.pkl', 'output_test_data.pkl',\n",
    "               'length_train_data.pkl', 'length_test_data.pkl']\n",
    "\n",
    "resulting_files = []\n",
    "for cache_file in cache_files:\n",
    "    resulting_files.append(load_data(cache_file, data_dir))\n",
    "        \n",
    "print('Done')\n",
    "\n",
    "trainX, testX, trainY, testY, train_len, test_len = resulting_files\n",
    "resulting_files = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word trainX -> [19, 15, 18, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word trainY -> [19, 15, 18, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word train_len -> 4\n",
      "word testX -> [20, 5, 15, 8, 18, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word testY -> [20, 8, 5, 15, 18, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word test_len -> 6\n"
     ]
    }
   ],
   "source": [
    "# Check they have been loaded correctly\n",
    "ind = 5\n",
    "\n",
    "print('word trainX ->', trainX[ind])\n",
    "print('word trainY ->', trainY[ind])\n",
    "print('word train_len ->', str(train_len[ind]))\n",
    "\n",
    "print('word testX ->', testX[ind])\n",
    "print('word testY ->', testY[ind])\n",
    "print('word test_len ->', str(test_len[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Upload data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model realy only needs the words and will try to rearange them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form output[34], length, input[34] where input[34] represens the word, which is a sequence of 34 integers the letters in the words.\n",
    "\n",
    "We will save the training csv and both dictionaries on the model directory, in order to upload later together in s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not os.path.exists(model_dir): # Make sure that the folder exists if not, create it\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "pd.concat([pd.DataFrame(trainY), pd.DataFrame(train_len), pd.DataFrame(trainX)], axis=1) \\\n",
    "        .to_csv(os.path.join(model_dir, 'train.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([pd.DataFrame(testY), pd.DataFrame(test_len), pd.DataFrame(testX)], axis=1) \\\n",
    "        .to_csv(os.path.join(model_dir, 'test.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on when we construct an endpoint which processes a submitted review we will need to make use of the dictionaries which we have created. As such, we will save them to a file now for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, 'letter2int_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(letter2int, f)\n",
    "with open(os.path.join(model_dir, 'int2letter_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(int2letter, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to load the dictionaries for any case from the files, it can be used the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from cache file: letter2int_dict.pkl\n",
      "Read data from cache file: int2letter_dict.pkl\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# load_dictionaries\n",
    "def load_dict(dict_file, dict_dir = cache_dir):\n",
    "    cache_data = None\n",
    "    file_dir = os.path.join(dict_dir, dict_file)\n",
    "\n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read data from cache file:\", dict_file)\n",
    "        except:\n",
    "            print('Problem reading the file', dict_file)\n",
    "    else:\n",
    "        print('File empty')\n",
    "\n",
    "    if cache_data is None:\n",
    "        print('Didnt read anythin')\n",
    "#         resulting_files = []\n",
    "    else:\n",
    "        pass\n",
    "#         resulting_files = (cache_data['data_file'])\n",
    "    \n",
    "    return cache_data\n",
    "\n",
    "# For example\n",
    "letter2int = load_dict('letter2int_dict.pkl', model_dir)\n",
    "int2letter = load_dict('int2letter_dict.pkl', model_dir)\n",
    "\n",
    "print(letter2int)\n",
    "print(int2letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear some more data as they are pretty big variables and we will not use them anymore\n",
    "# as we will read directly from the csv file\n",
    "\n",
    "trainX = None\n",
    "testX = None\n",
    "trainY = None\n",
    "testY = None\n",
    "train_len = None\n",
    "test_len = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 To S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to upload the training data and the dictionaries to the SageMaker default S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-670005714529/sagemaker/capstoneProject\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/capstoneProject'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# upload training data to S3\n",
    "train_dir = os.path.join(cache_dir, 'train.csv')\n",
    "input_data = sagemaker_session.upload_data(path=model_dir, bucket=bucket, key_prefix=prefix)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you've uploaded the data, by printing the contents of the default bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker/capstoneProject/int2letter_dict.pkl\n",
      "sagemaker/capstoneProject/letter2int_dict.pkl\n",
      "sagemaker/capstoneProject/test.csv\n",
      "sagemaker/capstoneProject/train.csv\n"
     ]
    }
   ],
   "source": [
    "# iterate through S3 objects and print contents\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "     print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the data have been uploaded as csv's, we will delete the data files saved that will be of no use\n",
    "%rm -rf cache/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by implementing our own neural network in PyTorch along with a training script. The necesary files for them are in the source folder, being them: train.py, model.py, predict.py and util.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize source/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the implementation we can observe that there are two parameters that we may wish to tweak to improve the performance of our model. These are the embedding dimension and the hidden dimension. The size of the vocabulary is the size of the previous vocabularies with the positives values.\n",
    "\n",
    "First we will load a small portion of the training data set to use as a sample to check the correct functioning of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will try to implement the training function to check its correct functioning. By doing this way, we avoid the larger time it takes if we would do it directly with the Pytorch Model as the loading time it takes to train everytime is considerable, and we probably would have to call it several times, trying to fix errors that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't do it before because it increases the memory that would have been used for the csv files, but the letters in each word passed to the model as input will have to be encoded on one-hot vectors. This can be done with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float64)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size read from csv -> X: (1000, 34), Y: (1000,), len: (1000, 34)\n",
      "Input shape: (1000, 34, 27) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
      "Torch X: torch.Size([1000, 34, 27]) shape, torch.float32 type\n",
      "Torch Y: torch.Size([1000, 34]) shape, torch.int64 type\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Get the arrays from csv\n",
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv(os.path.join(model_dir, 'train.csv'), header=None, names=None, nrows=1000)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = train_sample[train_sample.columns[0:34]]\n",
    "train_sample_len = train_sample[train_sample.columns[34]]\n",
    "train_sample_X = train_sample[train_sample.columns[35:69]]\n",
    "print('Size read from csv -> X: {}, Y: {}, len: {}'.format(train_sample_X.shape, train_sample_len.shape, train_sample_y.shape))\n",
    "\n",
    "X_np = train_sample_X.to_numpy(copy=True)\n",
    "Y_np = train_sample_y.to_numpy(copy=True)\n",
    "len_np = train_sample_len.to_numpy(copy=True)\n",
    "\n",
    "# Encode the input sentence as one hot vectors\n",
    "dict_size = len(letter2int)+1 #including the 0\n",
    "seq_len = 34\n",
    "batch_size = len(train_sample_X)\n",
    "input_seq = one_hot_encode(X_np, dict_size, seq_len, batch_size)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
    "\n",
    "train_torch_x = torch.tensor(input_seq, requires_grad=True).float().squeeze().clone()\n",
    "train_torch_len = torch.tensor(len_np).float().squeeze().type(torch.long).clone()\n",
    "# train_torch_x = torch.from_numpy(input_seq).float().squeeze()\n",
    "train_torch_y = torch.tensor(Y_np).float().squeeze().type(torch.long).clone()\n",
    "# train_sample_x = torch.from_numpy(output_seq).float().squeeze()\n",
    "# train_torch_y = torch.from_numpy(Y_np).float().squeeze().type(torch.long)\n",
    "print('Torch X: {} shape, {} type\\nTorch Y: {} shape, {} type'.format(train_torch_x.shape, train_torch_x.dtype, \n",
    "                                                                      train_torch_y.shape, train_torch_y.dtype))\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_torch_x, train_torch_y, train_torch_len)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preparation and arguments as if it was the file\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "input_dim = 27\n",
    "output_dim = 27\n",
    "hid_dim = 128\n",
    "epochs = 50\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    total_length = len(train_loader.dataset)\n",
    "    model.train()\n",
    "    loss_return = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        batchs_done = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch_X, batch_y, batch_len = batch\n",
    "            len_batch = len(batch_X)\n",
    "#             print('input shape X: {}, y:{}'.format(np.shape(batch_X), np.shape(batch_y)))\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            out = model(batch_X)\n",
    "#             print('output shape: ',  np.shape(out))\n",
    "#             print('target shape: ',  np.shape(batch_y))\n",
    "            \n",
    "            batch_loss = 0\n",
    "            for result, target, len_word in zip(out, batch_y, batch_len):\n",
    "#                 print(np.shape(result))\n",
    "#                 print(np.shape(target))\n",
    "#                 print('11', result[:len_word, :])\n",
    "#                 print('22', target[:len_word])\n",
    "#                 print('type input loss X: {}, Y: {}'.format(result.type(), target.type()))\n",
    "#                 print('shape input loss X: {}, Y: {}'.format(np.shape(result), np.shape(target)))\n",
    "                loss = loss_fn(result[:len_word, :], target[:len_word])\n",
    "#                 loss.backward(retain_graph=True)\n",
    "#                 optimizer.step()\n",
    "                batch_loss += loss\n",
    "    \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += batch_loss.data.item()\n",
    "#             total_loss += batch_loss\n",
    "        \n",
    "            batchs_done += len_batch\n",
    "#             print('Batch done. {} / {} inputs = {}%'.format(\n",
    "#                 batchs_done, total_length, np.round(batchs_done/total_length*100, decimals = 1)))\n",
    "\n",
    "        print(\"Epoch: {}, NLLLoss: {}\".format(epoch, total_loss / len(train_loader)))\n",
    "        loss_return.append(total_loss / len(train_loader))\n",
    "        \n",
    "    return loss_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, NLLLoss: 382.1217803955078\n",
      "Epoch: 2, NLLLoss: 354.0459289550781\n",
      "Epoch: 3, NLLLoss: 342.15063095092773\n",
      "Epoch: 4, NLLLoss: 335.0703926086426\n",
      "Epoch: 5, NLLLoss: 302.2839107513428\n",
      "Epoch: 6, NLLLoss: 262.3235149383545\n",
      "Epoch: 7, NLLLoss: 248.55022430419922\n",
      "Epoch: 8, NLLLoss: 223.5515651702881\n",
      "Epoch: 9, NLLLoss: 220.2649097442627\n",
      "Epoch: 10, NLLLoss: 223.76385307312012\n",
      "Epoch: 11, NLLLoss: 210.05339813232422\n",
      "Epoch: 12, NLLLoss: 226.5711669921875\n",
      "Epoch: 13, NLLLoss: 226.18839645385742\n",
      "Epoch: 14, NLLLoss: 210.79018592834473\n",
      "Epoch: 15, NLLLoss: 208.96319198608398\n",
      "Epoch: 16, NLLLoss: 219.85968208312988\n",
      "Epoch: 17, NLLLoss: 217.0606174468994\n",
      "Epoch: 18, NLLLoss: 215.26438331604004\n",
      "Epoch: 19, NLLLoss: 218.4445514678955\n",
      "Epoch: 20, NLLLoss: 222.21908950805664\n",
      "Epoch: 21, NLLLoss: 219.67633438110352\n",
      "Epoch: 22, NLLLoss: 211.43719482421875\n",
      "Epoch: 23, NLLLoss: 208.00495719909668\n",
      "Epoch: 24, NLLLoss: 211.62865257263184\n",
      "Epoch: 25, NLLLoss: 217.07529640197754\n",
      "Epoch: 26, NLLLoss: 213.64613723754883\n",
      "Epoch: 27, NLLLoss: 207.0576515197754\n",
      "Epoch: 28, NLLLoss: 207.45994567871094\n",
      "Epoch: 29, NLLLoss: 209.06119918823242\n",
      "Epoch: 30, NLLLoss: 210.87851905822754\n",
      "Epoch: 31, NLLLoss: 215.35483169555664\n",
      "Epoch: 32, NLLLoss: 219.15528106689453\n",
      "Epoch: 33, NLLLoss: 212.9862117767334\n",
      "Epoch: 34, NLLLoss: 207.71490478515625\n",
      "Epoch: 35, NLLLoss: 206.54946517944336\n",
      "Epoch: 36, NLLLoss: 207.55029296875\n",
      "Epoch: 37, NLLLoss: 212.43864631652832\n",
      "Epoch: 38, NLLLoss: 209.93622016906738\n",
      "Epoch: 39, NLLLoss: 208.3858127593994\n",
      "Epoch: 40, NLLLoss: 207.4163055419922\n",
      "Epoch: 41, NLLLoss: 207.94370079040527\n",
      "Epoch: 42, NLLLoss: 208.9007682800293\n",
      "Epoch: 43, NLLLoss: 206.8628330230713\n",
      "Epoch: 44, NLLLoss: 206.34623336791992\n",
      "Epoch: 45, NLLLoss: 205.76420974731445\n",
      "Epoch: 46, NLLLoss: 204.69025421142578\n",
      "Epoch: 47, NLLLoss: 206.73433685302734\n",
      "Epoch: 48, NLLLoss: 204.65779876708984\n",
      "Epoch: 49, NLLLoss: 204.7090835571289\n",
      "Epoch: 50, NLLLoss: 204.77563095092773\n"
     ]
    }
   ],
   "source": [
    "from source.model import Seq2Seq, Decoder, Encoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(input_dim, hid_dim)\n",
    "decoder = Decoder(input_dim, output_dim, hid_dim)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "# Train the model.\n",
    "# loss_function = nn.NLLLoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "losses = train(model, train_sample_dl, epochs, optimizer, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can make a plot to observe better\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although slow, we can observe that the model works and seems to improve over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will clean some variables that will not be used again\n",
    "train_sample = None\n",
    "train_sample_y = None\n",
    "train_sample_len = None\n",
    "train_sample_X = None\n",
    "input_seq = None\n",
    "X_np = None\n",
    "Y_np = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build and Train the PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training script works correctly, we will copy adecuately on the train.py file.\n",
    "\n",
    "A typical training script:\n",
    "\n",
    "- Loads training data from a specified directory\n",
    "- Parses any training & model hyperparameters (ex. nodes in a neural network, training epochs, etc.)\n",
    "- Instantiates a model of your design, with any specified hyperparams\n",
    "- Trains that model\n",
    "- Finally, saves the model so that it can be hosted/deployed, later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize source/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Create Pytorch Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point = 'train.py',\n",
    "                    source_dir = 'source',\n",
    "                    role = role,\n",
    "                    framework_version = '1.0',\n",
    "                    train_instance_count = 1,\n",
    "                    train_instance_type = 'ml.c4.xlarge',\n",
    "                    output_path = output_path,\n",
    "                    sagemaker_session = sagemaker_session,\n",
    "                    hyperparameters = {\n",
    "                        'input_dim': 27,\n",
    "                        'output_dim': 27,\n",
    "                        'hidden_dim': 128,\n",
    "                        'epochs': 10,\n",
    "                        'lr': 0.01,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Train the Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time, take it easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-03 08:09:29 Starting - Starting the training job...\n",
      "2020-08-03 08:09:31 Starting - Launching requested ML instances......\n",
      "2020-08-03 08:10:35 Starting - Preparing the instances for training...\n",
      "2020-08-03 08:11:29 Downloading - Downloading input data\n",
      "2020-08-03 08:11:29 Training - Downloading the training image...\n",
      "2020-08-03 08:11:43 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-08-03 08:11:44,614 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-08-03 08:11:44,616 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-03 08:11:44,628 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-08-03 08:11:44,629 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-08-03 08:11:44,914 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-08-03 08:11:44,914 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-08-03 08:11:44,915 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-08-03 08:11:44,915 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/f7/2adca20a7fa71b6a32f823bbd83992adeceab1d8bf72992bb7a55c69c19a/pandas-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/22/e7/4b2bdddb99f5f631d8c1de259897c2b7d65dcfcc1e0a6fd17a7f62923500/numpy-1.19.1-cp36-cp36m-manylinux1_x86_64.whl (13.4MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/66/25/ff030e2437265616a1e9b25ccc864e0371a0bc3adb7c5a404fd661c6f4f6/beautifulsoup4-4.9.1-py3-none-any.whl (115kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2019.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.0)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2 (from beautifulsoup4->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 1)) (1.12.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4rwfcger/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, pandas, soupsieve, beautifulsoup4, train\n",
      "  Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.16.4\u001b[0m\n",
      "\u001b[34m  Found existing installation: pandas 0.24.2\n",
      "    Uninstalling pandas-0.24.2:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled pandas-0.24.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed beautifulsoup4-4.9.1 numpy-1.19.1 pandas-1.1.0 soupsieve-2.0.1 train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-08-03 08:11:55,764 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-03 08:11:55,775 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"input_dim\": 27,\n",
      "        \"hidden_dim\": 128,\n",
      "        \"lr\": 0.01,\n",
      "        \"epochs\": 10,\n",
      "        \"output_dim\": 27\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-08-03-08-09-29-023\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-670005714529/sagemaker-pytorch-2020-08-03-08-09-29-023/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":128,\"input_dim\":27,\"lr\":0.01,\"output_dim\":27}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-670005714529/sagemaker-pytorch-2020-08-03-08-09-29-023/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":128,\"input_dim\":27,\"lr\":0.01,\"output_dim\":27},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2020-08-03-08-09-29-023\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-670005714529/sagemaker-pytorch-2020-08-03-08-09-29-023/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"128\",\"--input_dim\",\"27\",\"--lr\",\"0.01\",\"--output_dim\",\"27\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_DIM=27\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=128\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIM=27\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 128 --input_dim 27 --lr 0.01 --output_dim 27\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mLoaded csv\u001b[0m\n",
      "\u001b[34mSize read from csv -> X: (10000, 34), Y: (10000,), len: (10000, 34)\u001b[0m\n",
      "\u001b[34mTo numpied\u001b[0m\n",
      "\u001b[34mOne hot encoded\u001b[0m\n",
      "\u001b[34mTorched\u001b[0m\n",
      "\u001b[34mTrain loaded\u001b[0m\n",
      "\u001b[34mModel loaded with input_dim 27, output_dim 27, hidden_dim 128.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch: 1, NLLLoss: 316.218454771404\u001b[0m\n",
      "\u001b[34mEpoch: 2, NLLLoss: 231.0692465818381\u001b[0m\n",
      "\u001b[34mEpoch: 3, NLLLoss: 228.16348937795132\u001b[0m\n",
      "\u001b[34mEpoch: 4, NLLLoss: 221.26883584034593\u001b[0m\n",
      "\u001b[34mEpoch: 5, NLLLoss: 221.95427208912523\u001b[0m\n",
      "\u001b[34mEpoch: 6, NLLLoss: 224.22091628931744\u001b[0m\n",
      "\u001b[34mEpoch: 7, NLLLoss: 221.80863054492806\u001b[0m\n",
      "\u001b[34mEpoch: 8, NLLLoss: 221.3189378327961\u001b[0m\n",
      "\u001b[34mEpoch: 9, NLLLoss: 226.72853885119474\u001b[0m\n",
      "\u001b[34mEpoch: 10, NLLLoss: 233.3437530179567\u001b[0m\n",
      "\u001b[34m2020-08-03 08:17:34,533 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-08-03 08:17:44 Uploading - Uploading generated training model\n",
      "2020-08-03 08:17:44 Completed - Training job completed\n",
      "Training seconds: 396\n",
      "Billable seconds: 396\n",
      "CPU times: user 1.19 s, sys: 34.4 ms, total: 1.23 s\n",
      "Wall time: 8min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# train the estimator on S3 training data\n",
    "estimator.fit({'train': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Prediction with the trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Getting letters back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function that allows to obtain the words back from the padded version to the shorter integer version, and from this integer version to the string version using the dictionaries. This will be done in the following functions.\n",
    "\n",
    "If the model returns all 0's (in the case it had a bad performance). The word will consist of a '.'.\n",
    "\n",
    "Also this will be useful if we want to show how the words in the sentence have been jumbled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_back_and_unpad(number_dict, word, len_word):\n",
    "    \n",
    "    word_back = []\n",
    "    for letter_index, letter in enumerate(word):\n",
    "        if letter_index < len_word:\n",
    "            if letter in number_dict:\n",
    "                word_back.append(number_dict[letter])\n",
    "            else:\n",
    "                word_back.append('.')\n",
    "                        \n",
    "    return word_back\n",
    "\n",
    "\n",
    "def convert_back_data(number_dict, data, len_data):\n",
    "    result = []\n",
    "    \n",
    "    perc=0\n",
    "    idx_w = 0\n",
    "    for word, len_w in zip(data, len_data):\n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} words = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "\n",
    "\n",
    "#         print('word------>', word)\n",
    "        converted_word = convert_back_and_unpad(number_dict, word, len_w)\n",
    "#         print('word converted--->', converted_word, leng_word)\n",
    "\n",
    "        result.append(converted_word)\n",
    "        idx_w += 1\n",
    "        \n",
    "#     return np.array(result), np.array(lengths)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the input training data to check and we will see how the sentence would be seen once jumbled\n",
    "def join_sentence(sentence):\n",
    "    list_words = []\n",
    "    for word in sentence:\n",
    "        w = ''.join(word)\n",
    "        list_words.append(w)\n",
    "\n",
    "    sentence_joined = ' '.join(list_words)\n",
    "    \n",
    "    return sentence_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10 words = 0%\n",
      "1 / 10 words = 10.0%\n",
      "2 / 10 words = 20.0%\n",
      "4 / 10 words = 30.0%\n",
      "5 / 10 words = 40.0%\n",
      "6 / 10 words = 50.0%\n",
      "7 / 10 words = 60.0%\n",
      "8 / 10 words = 70.0%\n",
      "9 / 10 words = 80.0%\n",
      "0 / 10 words = 0%\n",
      "1 / 10 words = 10.0%\n",
      "2 / 10 words = 20.0%\n",
      "4 / 10 words = 30.0%\n",
      "5 / 10 words = 40.0%\n",
      "6 / 10 words = 50.0%\n",
      "7 / 10 words = 60.0%\n",
      "8 / 10 words = 70.0%\n",
      "9 / 10 words = 80.0%\n",
      "jsut more camrea usept senak sort need ohter bzeleadzd winllig \n",
      "<--> \n",
      "just more camera upset sneak sort need other bedazzled willing\n"
     ]
    }
   ],
   "source": [
    "# We will use the test dataset preparated, and assume that the sentence is the first 10 words of the file.\n",
    "test_sample = pd.read_csv(os.path.join(model_dir, 'train.csv'), header=None, names=None, nrows=10)\n",
    "test_sample_x = test_sample[test_sample.columns[35:69]].to_numpy(copy=True)\n",
    "test_sample_len = test_sample[test_sample.columns[34]].to_numpy(copy=True)\n",
    "test_sample_y = test_sample[test_sample.columns[0:34]].to_numpy(copy=True)\n",
    "\n",
    "# Check one sentence, we will consider 5 consecutive words from the training data as a sentence\n",
    "input_words = convert_back_data(int2letter, test_sample_x, test_sample_len)\n",
    "output_words = convert_back_data(int2letter, test_sample_y, test_sample_len)\n",
    "\n",
    "print('{} \\n<--> \\n{}'.format(join_sentence(input_words), join_sentence(output_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Predict Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same as we did with the training script to check it works first. The input will be the word already transformed into integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict the sentence as string, we will transform the sentence into its integer jumbled form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_predict(sentence, dictionary):\n",
    "    input_data_words = post_to_words(sentence)\n",
    "    \n",
    "    print('Converting data')\n",
    "    integer_sentence, len_sentence = convert_and_pad_data(dictionary, input_data_words)\n",
    "    print('Jumbling data')\n",
    "    jumbled_sentence = jumble_data(integer_sentence, len_sentence)\n",
    "        \n",
    "    return integer_sentence, jumbled_sentence, len_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are less than 4 letters, will be returned as such without passing through the model, as no transformation is really applied.\n",
    "\n",
    "The function will check if the input is a string array or an integer array, and in the case of the string, it will apply the functions necesary to have the aduecuated type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_input, data_len, dictionary, model, device):\n",
    "    \n",
    "    if isinstance(data_input[0], str):\n",
    "        print('Is an string sentence input')\n",
    "        original_s_int, jumbled_s_int, data_len = prepare_predict(s, dictionary)\n",
    "    else:\n",
    "        print('Is an integer sentence input')\n",
    "        jumbled_s_int = data_input.copy()\n",
    "        \n",
    "    integer_sentence = [] \n",
    "    for word in jumbled_s_int:\n",
    "#         print('word: ', word)\n",
    "        word_batch = [word]\n",
    "\n",
    "        if len(word) > 3:\n",
    "            dict_size = 27 #including the 0\n",
    "            seq_len = 34\n",
    "            batch_size =1\n",
    "            test_seq = one_hot_encode(word_batch, dict_size, seq_len, batch_size)\n",
    "            \n",
    "\n",
    "            data = torch.from_numpy(test_seq).float().squeeze().to(device)\n",
    "            # Have the torch as a batch of size 1\n",
    "            data_batch = data.view(1, np.shape(data)[0], np.shape(data)[1])\n",
    "#             print('size: {} -> {}'.format(np.shape(data), np.shape(data_batch)))\n",
    "            # Make sure to put the model into evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model.forward(data_batch)\n",
    "                \n",
    "                word_integer = []\n",
    "                for letter in output[0]: #as there's only 1 batch\n",
    "                    \n",
    "#                     print('letter', np.shape(letter))\n",
    "                    letter_numpy = letter.numpy()\n",
    "#                     print('numpy', type(letter_numpy), np.shape(letter_numpy))\n",
    "                    \n",
    "                    max_value_ind = np.argmax(letter_numpy, axis=0)\n",
    "#                     print(max_value_ind)\n",
    "#                     print(letter_numpy)\n",
    "\n",
    "\n",
    "\n",
    "                    word_integer.append(max_value_ind)\n",
    "                \n",
    "        else:\n",
    "\n",
    "            word_integer = word_batch.copy()\n",
    "            \n",
    "        integer_sentence.append(word_integer)\n",
    "#         print('integer word: ', word_integer) \n",
    "    print('Convert back sentences')\n",
    "    string_sentence = join_sentence(convert_back_data(int2letter, integer_sentence, data_len))\n",
    "    \n",
    "    return integer_sentence, string_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-2bad7d17b3db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mint_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sample_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sample_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletter2int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(1st word) Result  -> Ground Truth:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'->'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "int_result, str_result = predict(test_sample_x, test_sample_len, letter2int, model, device)\n",
    "\n",
    "print('(1st word) Result  -> Ground Truth:')\n",
    "print(int_result[0])\n",
    "print('->')\n",
    "print(test_sample_y[0])\n",
    "print('\\nResulting sentence: {}'.format(str_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works correctly (ignoring the model performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will try with a sentence string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-4ac60415ea90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'A study from Cambridge, has stated that etcetera'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletter2int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nResulting array:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "s = 'A study from Cambridge, has stated that etcetera'\n",
    "\n",
    "output_array, output_string = predict(s, None, letter2int, model, device)\n",
    "\n",
    "print('\\nResulting array:', output_array)\n",
    "print('\\nResulting string:', output_string)\n",
    "print('\\nLength original sentence was {}, length of returned sentence is {}'.format(len(s.split(' ')), \n",
    "                                                                                    len(output_string.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that it works fine, the rest depends on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Predict Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models do not automatically come with .predict() functions attached so we have to create a predict.py file. This file is responsible for loading a trained model and applying it to passed in, numpy data. When you created a PyTorch estimator, you specified where the training script, train.py was located.\n",
    "\n",
    "Also there's a utils.py file, which contains the functions necesary to make all the other work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can deploy this custom PyTorch model, we have to take one more step: creating a PyTorchModel. This model is responsible for knowing how to execute a specific predict.py script and it is what we'll deploy to create an endpoint.\n",
    "\n",
    "Also don't forget to copy the predict function into the predict.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 ms, sys: 3.72 ms, total: 15.6 ms\n",
      "Wall time: 45 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# importing PyTorchModel\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Create a model from the trained estimator data\n",
    "# And point to the prediction script\n",
    "model = PyTorchModel(model_data = estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version = '1.0',\n",
    "                     source_dir = 'source',\n",
    "                     entry_point = 'predict.py',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Deploy trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint sagemaker-pytorch-2020-08-03-09-04-40-684: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait, data_capture_config)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             )\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   2903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2423\u001b[0m         )\n\u001b[1;32m   2424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2425\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   2692\u001b[0m                 ),\n\u001b[1;32m   2693\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"InService\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2694\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2695\u001b[0m             )\n\u001b[1;32m   2696\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint sagemaker-pytorch-2020-08-03-09-04-40-684: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# deploy and create a predictor\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is deployed, we can see how it performs when applied to the test data.\n",
    "\n",
    "The provided function below, takes in a deployed predictor and the sentence and returns teo metrics: the accuracy of words which has been completely correctly reconstructed in the sentence, and the accuracy of letters that have been positioned correctly in the sentence.\n",
    "\n",
    "Also if you want to try it with the predict function written in the section 4.2, you only have to replace the predictor.predict for a predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helping function\n",
    "def equalArrays(arr1, arr2):\n",
    "    equal = True\n",
    "    for i1, i2 in zip(arr1, arr2):\n",
    "        if i1!=i2:\n",
    "            equal = False\n",
    "    return equal\n",
    "\n",
    "# code to evaluate the endpoint on test data\n",
    "# returns a variety of model metrics\n",
    "# def sentence_accuracy(predictor, sentence, verbose=True):\n",
    "def evaluate(sentence, ground_truth, lengths, dictionary, model, device, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a test set given the prediction endpoint.  \n",
    "    Return binary classification metrics.\n",
    "    :param predictor: A prediction endpoint\n",
    "    :param test_features: Test features\n",
    "    :param test_labels: Class labels for test data\n",
    "    :param verbose: If True, prints a table of all performance metrics\n",
    "    :return: A dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtain the result of the prediction\n",
    "#     predict_array, predict_string = predict(sentence, lengths, dictionary, model, device)\n",
    "    predict_array, predict_string = predictor.predict(sentence, model, device)\n",
    "       \n",
    "    # Both sentences have to have the same number of words\n",
    "    if len(sentence) != len(ground_truth):\n",
    "        print('error in the length obtained')\n",
    "        return 0\n",
    "    \n",
    "    correct_w = 0\n",
    "    correct_l = 0\n",
    "    \n",
    "    for original, result, length in zip(ground_truth, predict_array, lengths):\n",
    "        if equalArrays(original, result):\n",
    "            print('Correct prediction: Original {} -> Result {}'.format(join_sentence(convert_back_and_unpad(int2letter, original, length)), string))\n",
    "            correct_w += 1\n",
    "        for lo, lr in zip(original, result):\n",
    "            if lo == lr:\n",
    "                correct_l += 1\n",
    "                \n",
    "    accuracy_w = correct_w / len(sentence)\n",
    "    accuracy_l = correct_l\n",
    "    \n",
    "    return accuracy_w, accuracy_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to read the test.csv file to obtain all the test words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is an integer sentence input\n",
      "Convert back sentences\n",
      "0 / 22618 words = 0%\n",
      "2262 / 22618 words = 10.0%\n",
      "4524 / 22618 words = 20.0%\n",
      "6786 / 22618 words = 30.0%\n",
      "9048 / 22618 words = 40.0%\n",
      "11309 / 22618 words = 50.0%\n",
      "13571 / 22618 words = 60.0%\n",
      "15833 / 22618 words = 70.0%\n",
      "18095 / 22618 words = 80.0%\n",
      "20357 / 22618 words = 90.0%\n",
      "g [ 3 15 12 15 18  1  4 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "p [3, 15, 13, 5, 5, 12, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "l 8\n",
      "Word Accuracy: 0.0\n",
      "Letter Accuracy: 0.49345240922838185\n"
     ]
    }
   ],
   "source": [
    "evaluate_sample = pd.read_csv(os.path.join(model_dir, 'test.csv'), header=None, names=None)\n",
    "eval_sample_x = evaluate_sample[evaluate_sample.columns[35:69]].to_numpy(copy=True)\n",
    "eval_sample_len = evaluate_sample[evaluate_sample.columns[34]].to_numpy(copy=True)\n",
    "eval_sample_y = evaluate_sample[evaluate_sample.columns[0:34]].to_numpy(copy=True)\n",
    "acc_w, acc_l = evaluate(eval_sample_x, eval_sample_y, eval_sample_len, letter2int, model, device)\n",
    "print(eval_sample_len)\n",
    "print(np.sum(eval_sample_len))\n",
    "print('Word Accuracy: {}'.format(acc_w))\n",
    "print('Letter Accuracy: {}'.format(acc_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Delet the endpoint & Final cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts a predictor endpoint as input\n",
    "# And deletes the endpoint by name\n",
    "def delete_endpoint(predictor):\n",
    "        try:\n",
    "            boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)\n",
    "            print('Deleted {}'.format(predictor.endpoint))\n",
    "        except:\n",
    "            print('Already deleted: {}'.format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the predictor endpoint \n",
    "delete_endpoint(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we have cleaned deleted all our endpoints, S3 bucket, models, and endpoint configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.getsizeof(X_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
