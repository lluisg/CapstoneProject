{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT: CAN THE COMPUTERS READ IT TOO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cache' # The folder we will use for storing data\n",
    "data_dir = os.path.join(cache_dir, 'data')\n",
    "model_dir = os.path.join(cache_dir, 'model')\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this notebook you will find some clean cells to clean variables in order to have the maximum space available while processing the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip already downloaded\n",
      "Archive:  blogger.zip\n",
      "replace blogs/1000331.female.37.indUnk.Leo.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n",
      "zip unziped\n"
     ]
    }
   ],
   "source": [
    "if 'blogger.zip' not in os.listdir('.'):\n",
    "    !wget -O blogger.zip http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip\n",
    "else:\n",
    "    print('zip already downloaded')\n",
    "!unzip -Z1 blogger.zip | head -1000 | sed 's| |\\\\ |g' | xargs unzip blogger.zip\n",
    "print('zip unziped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "# print(len([name for name in os.listdir('blogs') if os.path.isfile(name)]))\n",
    "print(len(os.listdir('blogs/')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was pretty big (total of 19320 blogs) with maybe too much files (considering that inside each blog normally there is more than one post) so only a subset of it was unzipped. The subset selected is on the head part of the command, in this case resulted in 999 blogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('blogs', '*.xml')\n",
    "files = glob.glob(path)\n",
    "\n",
    "with open(files[0], encoding='Windows-1252') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, each blog consists of different posts, which we will separate, as we are only interested in posts, and we don't want the dates or other info that could be inside the XML's files.\n",
    "\n",
    "As XML files they consist of different tags. We are only gonna take the **post** ones.\n",
    "    \n",
    "We used the Beautiful soup to get the texts, that because with other parsers like ElementTree it resulted in ParseError in some files, as there are some HTML elements (like &nspb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 119205: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7af62e0e2a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mblogs_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_blogs_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-7af62e0e2a7e>\u001b[0m in \u001b[0;36mread_blogs_data\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mblog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m        \u001b[0;31m# It's a file-type object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         elif len(markup) <= 256 and (\n\u001b[1;32m    288\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34mb'<'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 119205: invalid start byte"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_blogs_data(data_dir='blogs'):\n",
    "    data = []\n",
    "\n",
    "    path = os.path.join(data_dir, '*.xml')\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    for f in files:\n",
    "        blog = open(f)\n",
    "\n",
    "        soup = BeautifulSoup(blog, 'xml')\n",
    "        posts = soup.find_all('post')\n",
    "\n",
    "        for post in posts:\n",
    "            data.append(post.text)\n",
    "    return data\n",
    "\n",
    "blogs_data = read_blogs_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there is some error, something related to the decoding.\n",
    "After searching for some time we find a possible solution to which type are the files encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows-1252\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "blog_dir = os.path.join('blogs', os.listdir('blogs')[0])\n",
    "blog = open(blog_dir, 'rb')\n",
    "enco = chardet.detect(blog.read())['encoding']\n",
    "print(enco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we use this encoding into the parsing of all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_blogs_data(data_dir='blogs', encod = \"ISO-8859-1\" ):\n",
    "    data = []\n",
    "\n",
    "    path = os.path.join(data_dir, '*.xml')\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    for f in files:\n",
    "        blog = open(f, encoding=encod)\n",
    "\n",
    "        soup = BeautifulSoup(blog, 'xml')\n",
    "        posts = soup.find_all('post')\n",
    "\n",
    "        for post in posts:\n",
    "            data.append(post.text)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_data = read_blogs_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of posts = 123612\n"
     ]
    }
   ],
   "source": [
    "#number of posts\n",
    "print(\"# of posts = {}\".format(len(blogs_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post50:  \n",
      "\n",
      "    \n",
      "       \n",
      "      Hey guys... it's been a long time since i've been on here... whoops! Oh well, life kind of gets in the way. But anyways, nothing speical has been going on except I got a 100 on my math test! WOO! You don't now how exciting that is... well... megan does because math for me is biology for megan. Me and megan are two of a kind... or not.  Guys, I think there's something wrong with me. I can't stop sleeping! At first I was like oh it's because i've been staying up late and getting up early in the morning. But for about 3-4 weeks i've been going to bed around 10-10:30 and than I get up around 6. The thing is... I come home and take a 2-3 hour nap! It's crazy... I mean if it wasn't EVERY day I wouldn't be so worried. And than... even when i take naps i'm already tired by 8! I dont' know... maybe i'm going through a growth spurt or something... i hope not!  So, i'm trying to think about what's going on in my life. Alias and charmed have started up again and i'm SOOO happy. I have re-instated my regular sunday nights and Jennifer's couch, eating good food and wonderful ice cream.   Yes  urlLink Megan ... Zero (from holes) IS a cuttie.... woo and that last scene... yowsers... hahah.  Today, we didn't have school which lucky me.... I was sick. I missed Jennifer's party! Ahh, i feel so bad about that. But than my mom dragged me shopping and I was like all blotchy and hideous... ew. I ended up getting a lot of stuff though which i'm happy about! I rock... haha... yeah. Wow, life is okay again... I'd like to thank God for that.   \"The worst moment for the atheist is when he is really thankful and has nobody to thank.\" Dante Gabriel Rossetti (1828 - 1882)  \n",
      "       \n",
      "    \n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check them\n",
    "print('post50: ', blogs_data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data parsed, which consists of 123.612 posts from the 999 blogs used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the posts text in the 'blogs_data' array, so we will apply to it the processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To start, we shuffle all the posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post50 shuffled:  \n",
      "\n",
      "\t \n",
      "      What I plan to sell: 1.  Tower with Monitor and mouse 2.  Possibly my original scanner if we don't think we'll need it 3.  Ethernet hub thingy 4.  Large travel makeup bag 5.  Bathroom things 6.  Mini Perfumes 7.  Red leather jacket 8.  Windbreaker 9.  Palm pilot 10. Sharper Image stereo 11. Red Ring (depends on what it is worth) 12. Mustang (this is still up in the air) 13. Guitar and stand \n",
      "     \n",
      "\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "blogs_data_shuffled = shuffle(blogs_data)\n",
    "print('post50 shuffled: ', blogs_data_shuffled[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply different processing steps:\n",
    "    - remove the \\n, \\t, whitespaces starting and ending\n",
    "    - remove possible HTML tags and other\n",
    "    - put all in lowercase\n",
    "    - remove accents in some chars\n",
    "    - separate the posts by words with the help of the indices\n",
    "    - get the words array from the indices\n",
    "    - ignore words that have chars not considered integers, letters nor punctuation\n",
    "    - also ignore words which are only integers or punctuation\n",
    "    - limit the posts to 500 words per posts (sentence)\n",
    "    - we ignore the words larger than supercalifragilisticexpialidocious\n",
    "    \n",
    "So finally, a list of words are returned. These words include punctuation, which will have to take into account later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len('supercalifragilisticexpialidocious'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def splitWithIndices(s, c=' '):\n",
    "    p = 0\n",
    "    for k, g in groupby(s, lambda x:x==c):\n",
    "        q = p + sum(1 for i in g)\n",
    "        if not k:\n",
    "            yield p, q # or p, q-1 if you are really sure you want that\n",
    "        p = q\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def post_to_words(post):\n",
    "    LIMIT_WORDS = 500\n",
    "\n",
    "    post = post.lstrip().rstrip()\n",
    "    post = BeautifulSoup(post, \"html.parser\").get_text()\n",
    "    post = post.lower()\n",
    "    \n",
    "    words_accents = []\n",
    "    for word in post:\n",
    "        words_accents.append(strip_accents(word))\n",
    "        \n",
    "    words_ind = splitWithIndices(words_accents)\n",
    "        \n",
    "    words_aux = []\n",
    "    for idx in words_ind:\n",
    "        words_aux.append(post[idx[0]:idx[1]])\n",
    "    \n",
    "    words = []\n",
    "    for idx, word in enumerate(words_aux):\n",
    "        isvalid = True\n",
    "        isdigit = True\n",
    "        ispunct = True\n",
    "\n",
    "        for letter in word:\n",
    "            if not letter.isdigit():\n",
    "                if letter not in string.punctuation:\n",
    "                    if not letter.isalpha():\n",
    "                        isvalid = False\n",
    "            \n",
    "            if not letter.isdigit():\n",
    "                isdigit = False\n",
    "            if letter not in string.punctuation:\n",
    "                ispunct = False\n",
    "                \n",
    "        if isvalid == True and isdigit == False and ispunct == False:\n",
    "            if len(word) <= len('supercalifragilisticexpialidocious') and idx <= LIMIT_WORDS:\n",
    "                words.append(word)\n",
    "        \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "       \n",
      "      Thank you to everyone who remembered my birthday, even though I'm not so keen on it all.  You've really all made my day, and hopefully most of you will be at dinner tomorrow.  I hope so.  Thanks again, I love you all.   Current Mood:  Happy\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "['thank', 'you', 'to', 'everyone', 'who', 'remembered', 'my', 'birthday,', 'even', 'though', \"i'm\", 'not', 'so', 'keen', 'on', 'it', 'all.', \"you've\", 'really', 'all', 'made', 'my', 'day,', 'and', 'hopefully', 'most', 'of', 'you', 'will', 'be', 'at', 'dinner', 'tomorrow.', 'i', 'hope', 'so.', 'thanks', 'again,', 'i', 'love', 'you', 'all.', 'current', 'mood:', 'happy']\n",
      "\n",
      "123456 ., -> []\n"
     ]
    }
   ],
   "source": [
    "# Check it\n",
    "words = post_to_words(blogs_data_shuffled[60])\n",
    "print(blogs_data_shuffled[60])\n",
    "print(words)\n",
    "\n",
    "word1 = '123456 .,'\n",
    "w1 = post_to_words(word1)\n",
    "print('\\n{} -> {}'.format(word1, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD A GRAPHIC OF THE LENGTH OF THE WORDS, AND THE APPAREANCES OF THE LETTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having the data as sentences, we will have it as words, as the input of the model will be word by word.\n",
    "\n",
    "The method below applies the post_to_words method for all the sentences in the dataset indicated. In addition, it caches the results. This is because performing this processing step can take a long time. This way if you are unable to complete the notebook in the current session, you can come back without needing to process the data a second time.\n",
    "\n",
    "We ignore the sentences which for whenever reason have length 0, meaning all the words has been ignored (following the rules stated in the post2words) and ignore the words with length less than 4 letters, as they will not add anything to the model once jumbled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def preprocess_blogs(data, cache_dir, cache_file=\"preprocessed_blogs.pkl\"):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    file_dir = os.path.join(cache_dir, cache_file)\n",
    "\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", file_dir)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_data = []\n",
    "        perc = 0\n",
    "        \n",
    "        for idx_p, post in enumerate(data):\n",
    "            \n",
    "            if idx_p / len(data) >= perc:\n",
    "                print('{} / {} sentences = {}%'.format(idx_p, len(data), np.round(perc*100, decimals = 1)))\n",
    "                perc = perc+0.1\n",
    "            \n",
    "            words = post_to_words(post)\n",
    "            words_stemmed = [PorterStemmer().stem(w) for w in words] # stem\n",
    "            \n",
    "            words_len = [word for word in words if len(word) > 3] #ignore words with less than 4 letters\n",
    "            if len(words_len) != 0:\n",
    "                words_data += words_len\n",
    "                        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_data=words_data)\n",
    "            with open(file_dir, \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", file_dir)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_data = (cache_data['words_data'])\n",
    "    \n",
    "    return words_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: cache/preprocessed_blogs.pkl\n",
      "The dataset consists of 11517356 words\n"
     ]
    }
   ],
   "source": [
    "# Process all the blogs\n",
    "if not os.path.exists(cache_dir): # Make sure that the folder exists, if not create it\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "words_data = preprocess_blogs(blogs_data_shuffled, cache_dir)\n",
    "print('The dataset consists of {} words'.format(len(words_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more than 11 milion words in 123.000 sentences. Remember that only a subset of 999 blogs out of 13.000 were used, imagine if all the blogs were used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length word: 34\n",
      "min length word: 4\n",
      "mean length word: 5.920329978512429\n"
     ]
    }
   ],
   "source": [
    "# check the length of number of letters per word is adecuated\n",
    "maxlen_w = 0\n",
    "minlen_w = 1000\n",
    "\n",
    "for word in words_data:\n",
    "    if len(word) > maxlen_w:\n",
    "        maxlen_w = len(word)\n",
    "    if len(word) < minlen_w:\n",
    "        minlen_w = len(word)\n",
    "\n",
    "mean_w = np.mean([len(w) for w in words_data])\n",
    "\n",
    "print('max length word: {}'.format(maxlen_w))\n",
    "print('min length word: {}'.format(minlen_w))\n",
    "print('mean length word: {}'.format(mean_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dictionary from the words processed and from it we will extract the most common 5.000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_from_data(data, vocab_size = 5000):    \n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "\n",
    "    for word in data:\n",
    "        if word in word_count:\n",
    "            word_count[word]+=1\n",
    "        else:\n",
    "            word_count[word]=1\n",
    "            \n",
    "    print('Length of the dictionary: {}'.format(len(word_count)))\n",
    "    \n",
    "    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_words = [tupl[0] for tupl in sorted_words]\n",
    "\n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx                           # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we will have separated the training and test datasets. The training will consist of the 5000 words more common in the blogs read. On the other hand, the test will consist of 1000 words randomly selected within the blogs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dictionary: 645253\n",
      "{'that': 0, 'have': 1, 'with': 2, 'this': 3, 'just': 4, 'like': 5, 'about': 6, 'urllink': 7, 'from': 8, 'they': 9, 'what': 10, 'when': 11, 'will': 12, \"it's\": 13, 'some': 14, 'your': 15, 'really': 16, \"don't\": 17, 'there': 18, 'know': 19, 'think': 20, 'then': 21, 'been': 22, 'would': 23, 'more': 24, 'going': 25, 'time': 26, 'were': 27, 'people': 28, 'good': 29, 'because': 30, 'back': 31, 'only': 32, 'last': 33, 'their': 34, 'much': 35, 'want': 36, 'even': 37, 'which': 38, 'went': 39, 'into': 40, 'after': 41, 'other': 42, \"i've\": 43, 'very': 44, 'over': 45, 'them': 46, 'could': 47, 'still': 48, 'than': 49, 'make': 50, 'little': 51, 'love': 52, 'first': 53, 'things': 54, 'something': 55, 'never': 56, 'being': 57, 'feel': 58, 'well': 59, 'should': 60, 'here': 61, 'where': 62, \"i'll\": 63, 'right': 64, 'need': 65, 'also': 66, 'around': 67, 'work': 68, 'take': 69, \"can't\": 70, \"that's\": 71, \"didn't\": 72, 'those': 73, 'down': 74, 'most': 75, 'thing': 76, 'said': 77, 'today': 78, 'made': 79, 'come': 80, 'these': 81, 'before': 82, 'while': 83, 'life': 84, 'night': 85, 'since': 86, 'always': 87, 'well,': 88, 'next': 89, 'great': 90, 'maybe': 91, 'pretty': 92, 'getting': 93, 'many': 94, 'long': 95, 'home': 96, 'another': 97, 'through': 98, 'find': 99, 'look': 100, 'thought': 101, 'every': 102, 'ever': 103, 'actually': 104, 'someone': 105, 'school': 106, 'sure': 107, 'better': 108, 'tell': 109, 'came': 110, 'doing': 111, 'read': 112, 'hope': 113, 'best': 114, \"you're\": 115, 'until': 116, 'same': 117, 'having': 118, 'give': 119, 'guess': 120, 'nothing': 121, 'everyone': 122, 'such': 123, 'myself': 124, 'might': 125, 'keep': 126, 'does': 127, \"he's\": 128, 'whole': 129, 'probably': 130, 'told': 131, 'took': 132, 'nice': 133, 'friends': 134, 'world': 135, 'anything': 136, 'found': 137, 'trying': 138, 'blog': 139, 'kind': 140, 'days': 141, 'left': 142, 'talk': 143, 'least': 144, 'looking': 145, 'week': 146, 'away': 147, 'year': 148, \"doesn't\": 149, 'house': 150, 'happy': 151, 'call': 152, 'years': 153, 'now.': 154, 'called': 155, 'though': 156, 'started': 157, 'quite': 158, 'everything': 159, 'part': 160, 'enough': 161, 'makes': 162, 'wanted': 163, 'gonna': 164, \"there's\": 165, 'without': 166, 'place': 167, 'stuff': 168, 'start': 169, 'post': 170, 'now,': 171, 'must': 172, 'show': 173, 'time.': 174, 'once': 175, 'done': 176, 'again': 177, 'three': 178, 'dont': 179, 'anyone': 180, 'hard': 181, 'hate': 182, 'both': 183, 'each': 184, 'used': 185, 'person': 186, 'help': 187, 'remember': 188, 'that.': 189, 'believe': 190, 'almost': 191, 'seems': 192, 'thinking': 193, 'able': 194, 'finally': 195, 'talking': 196, 'friend': 197, \"wasn't\": 198, \"haven't\": 199, 'today.': 200, 'morning': 201, 'mean': 202, 'feeling': 203, 'watch': 204, 'coming': 205, 'decided': 206, \"we're\": 207, 'times': 208, 'making': 209, 'already': 210, 'else': 211, 'play': 212, 'live': 213, 'write': 214, 'real': 215, \"won't\": 216, 'name': 217, \"isn't\": 218, 'working': 219, 'day.': 220, 'couple': 221, 'anyway,': 222, 'them.': 223, 'cool': 224, 'you.': 225, 'movie': 226, 'seen': 227, 'during': 228, 'wish': 229, 'head': 230, 'guys': 231, 'hours': 232, 'past': 233, 'reading': 234, 'book': 235, \"she's\": 236, 'high': 237, 'different': 238, 'out.': 239, 'stop': 240, 'that,': 241, 'again.': 242, 'felt': 243, 'between': 244, \"they're\": 245, 'hear': 246, 'well.': 247, 'point': 248, 'leave': 249, 'music': 250, 'miss': 251, 'there.': 252, 'fact': 253, 'mind': 254, 'news': 255, 'tomorrow': 256, 'yesterday': 257, 'half': 258, 'comes': 259, 'care': 260, 'check': 261, 'girl': 262, 'family': 263, 'watching': 264, 'asked': 265, 'free': 266, 'taking': 267, 'looks': 268, 'heard': 269, 'class': 270, 'rather': 271, 'too.': 272, 'money': 273, 'room': 274, 'full': 275, 'you,': 276, 'time,': 277, 'lost': 278, 'wait': 279, 'rest': 280, 'night.': 281, 'song': 282, 'second': 283, 'change': 284, 'today,': 285, 'weekend': 286, 'says': 287, 'interesting': 288, 'sleep': 289, 'him.': 290, 'cause': 291, 'party': 292, 'idea': 293, \"couldn't\": 294, 'goes': 295, 'spent': 296, 'story': 297, 'gave': 298, 'later': 299, 'under': 300, 'kinda': 301, 'seem': 302, 'funny': 303, 'good.': 304, 'sometimes': 305, 'against': 306, 'means': 307, 'saying': 308, 'gets': 309, 'less': 310, 'knew': 311, 'small': 312, 'game': 313, 'stay': 314, 'brought': 315, 'reason': 316, 'life.': 317, 'damn': 318, 'know,': 319, 'phone': 320, 'stupid': 321, 'tried': 322, 'minutes': 323, 'playing': 324, 'turn': 325, 'please': 326, 'yeah,': 327, 'white': 328, 'along': 329, 'fucking': 330, 'looked': 331, 'course': 332, 'sorry': 333, 'writing': 334, 'hour': 335, \"wouldn't\": 336, 'become': 337, 'yes,': 338, 'words': 339, 'black': 340, 'computer': 341, 'early': 342, 'heart': 343, 'watched': 344, 'front': 345, 'open': 346, 'hell': 347, 'thank': 348, \"what's\": 349, 'wonder': 350, 'all.': 351, 'favorite': 352, 'kids': 353, 'thats': 354, 'played': 355, 'face': 356, 'food': 357, 'side': 358, 'here.': 359, 'friday': 360, 'late': 361, 'soon': 362, 'happened': 363, 'american': 364, 'either': 365, 'sitting': 366, 'bush': 367, 'tonight': 368, 'close': 369, 'gone': 370, 'parents': 371, 'together': 372, 'understand': 373, 'sort': 374, 'wrong': 375, 'thanks': 376, 'fun.': 377, 'work.': 378, 'supposed': 379, 'day,': 380, 'site': 381, 'eyes': 382, 'there,': 383, 'again,': 384, 'running': 385, 'move': 386, 'living': 387, 'girls': 388, 'totally': 389, 'instead': 390, 'seeing': 391, 'especially': 392, 'group': 393, 'walk': 394, '\"the': 395, 'weeks': 396, 'word': 397, 'wants': 398, 'bought': 399, 'waiting': 400, 'tired': 401, 'however,': 402, '(and': 403, 'bring': 404, 'summer': 405, 'turned': 406, 'lots': 407, 'wanna': 408, 'though.': 409, 'behind': 410, 'saturday': 411, 'four': 412, 'although': 413, 'night,': 414, 'talked': 415, \"we'll\": 416, 'shit': 417, 'number': 418, 'john': 419, 'usually': 420, 'ready': 421, 'then,': 422, 'spend': 423, 'meet': 424, 'week.': 425, 'matter': 426, 'listening': 427, 'taken': 428, 'worth': 429, 'one.': 430, 'hair': 431, 'knows': 432, 'several': 433, 'drive': 434, 'completely': 435, 'except': 436, 'her.': 437, 'know.': 438, 'band': 439, 'city': 440, 'dinner': 441, 'birthday': 442, 'out,': 443, 'problem': 444, 'kept': 445, 'sunday': 446, 'state': 447, 'all,': 448, 'five': 449, 'sound': 450, 'outside': 451, 'yeah': 452, 'though,': 453, 'email': 454, 'gotta': 455, 'using': 456, 'break': 457, 'short': 458, 'months': 459, 'fall': 460, 'glad': 461, 'pick': 462, 'others': 463, 'home.': 464, \"here's\": 465, 'whatever': 466, 'picture': 467, 'middle': 468, 'people.': 469, 'woke': 470, 'cant': 471, 'water': 472, 'starting': 473, 'hand': 474, 'fuck': 475, 'true': 476, 'across': 477, 'sounds': 478, 'light': 479, 'trip': 480, 'needs': 481, 'huge': 482, 'say,': 483, 'pictures': 484, 'given': 485, 'entire': 486, 'this.': 487, 'hold': 488, 'crazy': 489, 'test': 490, 'enjoy': 491, 'inside': 492, 'thing.': 493, 'ended': 494, 'way,': 495, 'power': 496, \"let's\": 497, 'sense': 498, 'said,': 499, 'list': 500, 'course,': 501, 'till': 502, 'case': 503, 'walked': 504, 'them,': 505, 'send': 506, 'needed': 507, 'beautiful': 508, 'mean,': 509, 'often': 510, 'weird': 511, \"you've\": 512, 'line': 513, 'type': 514, 'this,': 515, 'baby': 516, 'way.': 517, 'like,': 518, 'current': 519, 'finished': 520, 'body': 521, 'woman': 522, 'realize': 523, 'forward': 524, 'sick': 525, 'lunch': 526, 'important': 527, 'exactly': 528, 'didnt': 529, 'deal': 530, 'perhaps': 531, 'figure': 532, 'year.': 533, 'learn': 534, 'english': 535, 'definitely': 536, 'whether': 537, 'telling': 538, 'cannot': 539, 'office': 540, 'hopefully': 541, 'him,': 542, \"you'll\": 543, 'wrote': 544, 'information': 545, 'here,': 546, 'plan': 547, 'women': 548, 'but,': 549, 'young': 550, 'blue': 551, 'ones': 552, 'takes': 553, 'paper': 554, 'online': 555, 'question': 556, 'moment': 557, 'personal': 558, 'walking': 559, 'chance': 560, 'dream': 561, 'cold': 562, 'stand': 563, 'order': 564, 'back.': 565, 'president': 566, 'forget': 567, 'listen': 568, 'stuff.': 569, 'sent': 570, 'busy': 571, \"aren't\": 572, 'church': 573, 'work,': 574, 'realized': 575, 'longer': 576, 'happen': 577, 'worked': 578, 'easy': 579, 'article': 580, 'christmas': 581, 'special': 582, 'you?': 583, 'moving': 584, 'within': 585, 'simply': 586, 'driving': 587, 'missed': 588, 'alone': 589, 'upon': 590, 'rock': 591, 'hang': 592, 'off.': 593, 'link': 594, 'certain': 595, 'college': 596, 'support': 597, 'giving': 598, 'mother': 599, 'days.': 600, 'country': 601, 'catch': 602, 'near': 603, 'kill': 604, 'wonderful': 605, '(the': 606, 'people,': 607, 'written': 608, 'known': 609, 'loved': 610, 'door': 611, 'month': 612, 'public': 613, 'single': 614, 'brother': 615, 'amazing': 616, 'team': 617, 'life,': 618, 'liked': 619, 'cool.': 620, \"we've\": 621, 'morning.': 622, 'seemed': 623, 'leaving': 624, 'much.': 625, 'visit': 626, 'internet': 627, 'human': 628, 'something.': 629, 'film': 630, 'share': 631, 'eating': 632, 'students': 633, 'history': 634, 'school.': 635, 'yourself': 636, 'lives': 637, 'page': 638, 'sister': 639, 'songs': 640, 'hurt': 641, 'meeting': 642, 'mine': 643, 'and,': 644, 'feels': 645, 'town': 646, 'things.': 647, 'books': 648, 'experience': 649, 'tomorrow.': 650, 'wearing': 651, 'company': 652, 'excited': 653, 'drink': 654, 'not.': 655, 'finish': 656, 'poor': 657, 'pain': 658, 'perfect': 659, 'piece': 660, 'weekend.': 661, 'one,': 662, 'myself.': 663, 'monday': 664, 'gotten': 665, 'home,': 666, 'world.': 667, 'major': 668, 'apparently': 669, 'man,': 670, 'shall': 671, 'changed': 672, 'blog.': 673, 'large': 674, 'house.': 675, 'mention': 676, 'bunch': 677, 'hands': 678, 'good,': 679, 'okay,': 680, 'currently': 681, 'government': 682, 'sweet': 683, 'picked': 684, 'thoughts': 685, 'business': 686, 'street': 687, 'forgot': 688, 'learned': 689, 'answer': 690, 'wear': 691, 'away.': 692, 'video': 693, 'study': 694, 'awesome': 695, 'random': 696, 'posted': 697, 'wake': 698, 'plus': 699, 'national': 700, 'moved': 701, 'tonight.': 702, '(which': 703, 'fell': 704, 'death': 705, 'week,': 706, 'dark': 707, 'store': 708, 'works': 709, 'questions': 710, 'dead': 711, 'final': 712, 'anyway': 713, 'chris': 714, 'also,': 715, 'more.': 716, 'fine': 717, 'caught': 718, 'green': 719, 'stopped': 720, 'asking': 721, 'movies': 722, 'morning,': 723, 'truly': 724, 'better.': 725, 'anyway.': 726, 'friends.': 727, 'later.': 728, 'straight': 729, 'deep': 730, 'children': 731, 'right.': 732, 'total': 733, 'school,': 734, 'york': 735, 'project': 736, 'sign': 737, 'save': 738, 'hoping': 739, 'speaking': 740, 'year,': 741, 'her,': 742, 'following': 743, 'form': 744, 'imagine': 745, 'managed': 746, 'dear': 747, 'meant': 748, 'strong': 749, 'complete': 750, 'system': 751, 'political': 752, 'back,': 753, 'level': 754, 'voice': 755, 'stuck': 756, 'space': 757, 'too,': 758, 'super': 759, 'rick': 760, 'putting': 761, 'road': 762, 'shows': 763, 'local': 764, 'according': 765, 'report': 766, 'future': 767, 'noticed': 768, 'bill': 769, 'etc.': 770, 'pass': 771, 'happens': 772, 'absolutely': 773, 'clean': 774, 'lose': 775, 'somewhere': 776, 'social': 777, 'bored': 778, 'fight': 779, 'boys': 780, 'unless': 781, 'anymore.': 782, 'headed': 783, 'strange': 784, 'photo': 785, 'mostly': 786, 'including': 787, 'park': 788, 'lack': 789, 'simple': 790, 'nearly': 791, 'towards': 792, 'iraq': 793, 'cute': 794, 'soon.': 795, 'onto': 796, 'third': 797, 'million': 798, 'passed': 799, 'comments': 800, 'thursday': 801, 'message': 802, 'turns': 803, 'about.': 804, 'down.': 805, 'rain': 806, 'service': 807, 'above': 808, 'hey,': 809, 'possible': 810, 'anyways,': 811, 'fear': 812, 'throw': 813, 'ride': 814, 'okay': 815, 'shopping': 816, 'continue': 817, 'comment': 818, 'friends,': 819, 'basically': 820, 'crap': 821, 'expect': 822, 'speak': 823, 'none': 824, 'seriously': 825, 'yesterday.': 826, 'problems': 827, 'beat': 828, 'dance': 829, 'camera': 830, 'calling': 831, 'return': 832, 'somehow': 833, 'extra': 834, 'thinks': 835, 'gives': 836, 'bad.': 837, 'likely': 838, 'clear': 839, 'united': 840, 'evening': 841, 'great.': 842, 'fast': 843, 'worst': 844, 'serious': 845, 'love,': 846, 'blogger': 847, 'conversation': 848, 'eric': 849, 'standing': 850, 'peace': 851, 'general': 852, 'afraid': 853, 'date': 854, 'really,': 855, 'years.': 856, 'based': 857, 'control': 858, 'interested': 859, 'certainly': 860, 'boring': 861, 'attention': 862, 'weather': 863, 'drove': 864, 'over.': 865, 'recently': 866, 'place.': 867, 'coffee': 868, 'teacher': 869, 'knowing': 870, 'shot': 871, 'worry': 872, 'period': 873, 'note': 874, 'quick': 875, 'god,': 876, 'hanging': 877, 'finding': 878, 'suppose': 879, 'issue': 880, 'afternoon': 881, 'child': 882, 'amount': 883, 'update': 884, 'lovely': 885, 'website': 886, 'step': 887, 'right?': 888, 'lady': 889, 'thing,': 890, 'stayed': 891, 'starts': 892, 'search': 893, 'times.': 894, \"it'll\": 895, 'showed': 896, 'off,': 897, 'media': 898, 'main': 899, \"shouldn't\": 900, 'lead': 901, \"who's\": 902, 'despite': 903, 'yet,': 904, 'figured': 905, 'games': 906, 'himself': 907, 'star': 908, 'blood': 909, 'beginning': 910, 'fire': 911, 'minute': 912, 'posting': 913, 'plans': 914, 'stories': 915, 'earlier': 916, 'planning': 917, 'radio': 918, 'french': 919, 'quote': 920, 'normal': 921, 'relationship': 922, 'feet': 923, 'view': 924, 'george': 925, 'yet.': 926, 'mood': 927, 'recent': 928, 'missing': 929, 'student': 930, 'dreams': 931, 'states': 932, 'chicken': 933, 'with.': 934, 'paid': 935, 'touch': 936, 'fact,': 937, 'david': 938, 'area': 939, 'while.': 940, 'energy': 941, 'smile': 942, 'among': 943, 'paul': 944, 'nice.': 945, 'ago.': 946, 'lol.': 947, 'ago,': 948, 'alot': 949, 'done.': 950, 'named': 951, 'decide': 952, 'laugh': 953, 'wedding': 954, 'yeah.': 955, 'mentioned': 956, 'yesterday,': 957, 'funny.': 958, 'learning': 959, 'slept': 960, 'ways': 961, 'mike': 962, 'themselves': 963, 'became': 964, 'library': 965, 'explain': 966, 'agree': 967, 'nobody': 968, 'album': 969, 'lets': 970, 'quizilla': 971, 'begin': 972, 'brain': 973, 'sleep.': 974, 'michael': 975, 'hung': 976, 'yay!': 977, 'kick': 978, 'right,': 979, 'drinking': 980, 'practice': 981, 'everybody': 982, 'card': 983, 'likes': 984, 'pissed': 985, 'notice': 986, 'father': 987, 'lord': 988, 'places': 989, 'fun,': 990, 'research': 991, 'worse': 992, 'things,': 993, 'broke': 994, 'later,': 995, 'tuesday': 996, 'wind': 997, 'happy.': 998, 'calls': 999, 'jesus': 1000, 'keeping': 1001, 'click': 1002, 'sleeping': 1003, 'say.': 1004, 'anything.': 1005, 'consider': 1006, 'shut': 1007, 'spending': 1008, 'cover': 1009, 'house,': 1010, 'wondering': 1011, 'drop': 1012, 'held': 1013, 'trust': 1014, 'july': 1015, 'doubt': 1016, 'building': 1017, 'actually,': 1018, \"weren't\": 1019, 'days,': 1020, 'usual': 1021, 'extremely': 1022, 'suddenly': 1023, 'enjoyed': 1024, 'clothes': 1025, 'america': 1026, 'possibly': 1027, \"you'd\": 1028, 'round': 1029, 'u.s.': 1030, 'self': 1031, 'version': 1032, 'movie.': 1033, 'however': 1034, 'pull': 1035, 'considering': 1036, \"hasn't\": 1037, \"today's\": 1038, 'filled': 1039, 'character': 1040, 'south': 1041, 'wont': 1042, 'floor': 1043, 'beyond': 1044, 'original': 1045, 'slow': 1046, 'keeps': 1047, 'stick': 1048, 'vote': 1049, 'cell': 1050, 'began': 1051, 'situation': 1052, 'choose': 1053, 'not,': 1054, 'proud': 1055, 'drunk': 1056, 'tells': 1057, 'together.': 1058, 'did.': 1059, 'interview': 1060, 'haha.': 1061, 'added': 1062, 'truth': 1063, 'various': 1064, 'was,': 1065, 'blog,': 1066, 'see,': 1067, '\"you': 1068, 'issues': 1069, 'wednesday': 1070, 'evil': 1071, 'scared': 1072, 'train': 1073, 'daily': 1074, 'grow': 1075, 'classes': 1076, 'loves': 1077, 'lived': 1078, 'down,': 1079, 'comments:': 1080, 'eventually': 1081, 'further': 1082, 'first,': 1083, 'center': 1084, 'color': 1085, 'cheese': 1086, 'pulled': 1087, 'mind.': 1088, 'older': 1089, 'around.': 1090, 'slightly': 1091, 'choice': 1092, 'before.': 1093, 'allowed': 1094, 'itself': 1095, 'whenever': 1096, 'follow': 1097, 'weight': 1098, 'falling': 1099, 'program': 1100, 'think.': 1101, 'letter': 1102, 'points': 1103, 'broken': 1104, 'created': 1105, 'mark': 1106, 'beer': 1107, 'that?': 1108, 'died': 1109, 'trouble': 1110, 'fair': 1111, 'particular': 1112, 'asleep': 1113, 'snow': 1114, 'forever': 1115, 'join': 1116, 'create': 1117, 'feelings': 1118, 'lucky': 1119, 'spring': 1120, 'health': 1121, 'either.': 1122, 'kerry': 1123, 'june': 1124, 'myself,': 1125, 'club': 1126, 'difficult': 1127, 'helped': 1128, 'record': 1129, 'science': 1130, 'holding': 1131, 'welcome': 1132, 'somewhat': 1133, 'job.': 1134, 'admit': 1135, 'present': 1136, 'ideas': 1137, 'married': 1138, 'fish': 1139, 'grade': 1140, 'said.': 1141, 'room.': 1142, 'quickly': 1143, 'killed': 1144, 'window': 1145, 'process': 1146, 'stuff,': 1147, 'it...': 1148, 'soul': 1149, 'james': 1150, 'common': 1151, 'else.': 1152, 'actual': 1153, 'tonight,': 1154, '(not': 1155, 'man.': 1156, 'posts': 1157, 'greg': 1158, 'north': 1159, 'table': 1160, 'world,': 1161, 'involved': 1162, 'whom': 1163, 'tomorrow,': 1164, 'wife': 1165, 'wow,': 1166, 'regular': 1167, 'members': 1168, 'warm': 1169, 'community': 1170, 'see.': 1171, 'class.': 1172, 'links': 1173, 'education': 1174, 'ball': 1175, 'blogging': 1176, 'was.': 1177, 'dave': 1178, 'previous': 1179, 'former': 1180, 'homework': 1181, 'track': 1182, 'years,': 1183, 'season': 1184, 'miles': 1185, 'reality': 1186, 'obviously': 1187, 'photos': 1188, 'university': 1189, 'much,': 1190, 'growing': 1191, 'sucks': 1192, 'what?': 1193, 'security': 1194, 'mail': 1195, 'hello': 1196, 'series': 1197, 'turning': 1198, 'staying': 1199, 'language': 1200, 'buying': 1201, 'matt': 1202, 'wanting': 1203, 'before,': 1204, 'quiet': 1205, 'singing': 1206, 'pray': 1207, 'concert': 1208, 'interest': 1209, 'somebody': 1210, 'luck': 1211, 'tend': 1212, 'cost': 1213, 'chocolate': 1214, \"hadn't\": 1215, 'dropped': 1216, 'born': 1217, 'brown': 1218, 'e-mail': 1219, 'entry': 1220, 'weekend,': 1221, 'wrong.': 1222, 'me...': 1223, 'reach': 1224, 'meaning': 1225, 'whose': 1226, 'bed.': 1227, 'allow': 1228, 'offer': 1229, 'india': 1230, 'day:': 1231, 'march': 1232, 'sing': 1233, 'christian': 1234, 'love.': 1235, 'latest': 1236, 'lot.': 1237, 'beach': 1238, 'taste': 1239, 'action': 1240, 'are.': 1241, 'kiss': 1242, 'easily': 1243, 'copy': 1244, 'surprised': 1245, 'waste': 1246, 'hours.': 1247, 'really.': 1248, 'fill': 1249, 'folks': 1250, 'shirt': 1251, 'sooo': 1252, 'apartment': 1253, 'police': 1254, 'king': 1255, 'head.': 1256, 'reasons': 1257, 'press': 1258, 'something,': 1259, 'willing': 1260, 'chinese': 1261, 'becoming': 1262, 'similar': 1263, 'title': 1264, 'secret': 1265, 'received': 1266, 'blogs': 1267, 'military': 1268, 'holy': 1269, 'quality': 1270, 'arms': 1271, 'greatest': 1272, 'awesome.': 1273, 'focus': 1274, 'angry': 1275, 'arrived': 1276, 'laughing': 1277, 'stage': 1278, 'fairly': 1279, 'hotel': 1280, 'horrible': 1281, 'easier': 1282, 'subject': 1283, 'board': 1284, 'paying': 1285, 'husband': 1286, 'showing': 1287, 'appreciate': 1288, 'person.': 1289, 'boss': 1290, 'lazy': 1291, 'respect': 1292, 'tears': 1293, 'ahead': 1294, 'game.': 1295, 'exciting': 1296, 'shit.': 1297, 'results': 1298, \"they've\": 1299, 'windows': 1300, 'shower': 1301, 'times,': 1302, 'address': 1303, 'doesnt': 1304, 'youth': 1305, 'contact': 1306, 'lines': 1307, 'changes': 1308, 'parking': 1309, 'empty': 1310, 'position': 1311, 'east': 1312, 'land': 1313, 'parts': 1314, 'nights': 1315, 'followed': 1316, 'show.': 1317, 'available': 1318, 'dress': 1319, 'west': 1320, 'losing': 1321, 'review': 1322, 'apart': 1323, 'happening': 1324, 'silly': 1325, 'names': 1326, 'opened': 1327, 'international': 1328, 'bad,': 1329, 'everything.': 1330, 'opportunity': 1331, 'cream': 1332, 'shop': 1333, 'blame': 1334, 'suck': 1335, 'smell': 1336, 'attempt': 1337, 'earth': 1338, 'gift': 1339, 'interesting.': 1340, 'barely': 1341, 'sarah': 1342, 'while,': 1343, 'dunno': 1344, 'ground': 1345, 'holiday': 1346, 'mouth': 1347, 'scary': 1348, 'force': 1349, 'god.': 1350, 'worried': 1351, 'harry': 1352, 'excellent': 1353, 'pink': 1354, 'ability': 1355, 'leaves': 1356, 'knowledge': 1357, 'style': 1358, 'camp': 1359, 'yes.': 1360, 'court': 1361, 'scene': 1362, 'helping': 1363, 'car.': 1364, 'decision': 1365, 'teachers': 1366, 'effort': 1367, 'biggest': 1368, 'teaching': 1369, 'why?': 1370, 'long.': 1371, 'hearing': 1372, 'bitch': 1373, 'tired.': 1374, 'technology': 1375, 'freaking': 1376, 'math': 1377, 'upset': 1378, 'over,': 1379, 'ever.': 1380, 'size': 1381, 'digital': 1382, 'slowly': 1383, 'wall': 1384, 'away,': 1385, 'field': 1386, 'difference': 1387, 'pool': 1388, '\"what': 1389, 'shoes': 1390, 'besides': 1391, 'event': 1392, 'cool,': 1393, 'bright': 1394, 'bit.': 1395, 'nothing.': 1396, 'bottom': 1397, 'hardly': 1398, 'higher': 1399, 'particularly': 1400, 'bloody': 1401, 'april': 1402, 'then.': 1403, 'glass': 1404, 'adam': 1405, 'dirty': 1406, 'havent': 1407, 'anymore': 1408, 'discovered': 1409, 'closer': 1410, 'okay.': 1411, 'marriage': 1412, 'teach': 1413, 'memory': 1414, \"he'd\": 1415, 'football': 1416, 'faith': 1417, 'decent': 1418, 'station': 1419, 'loud': 1420, 'family.': 1421, 'events': 1422, 'brings': 1423, 'still,': 1424, 'twice': 1425, \"he'll\": 1426, 'rich': 1427, 'book.': 1428, 'brian': 1429, 'day!': 1430, 'money.': 1431, 'smart': 1432, \"'cause\": 1433, 'access': 1434, 'end.': 1435, 'safe': 1436, 'considered': 1437, 'sometime': 1438, 'carry': 1439, 'term': 1440, 'news,': 1441, 'swear': 1442, 'section': 1443, 'are,': 1444, 'related': 1445, 'wasnt': 1446, 'indian': 1447, 'post.': 1448, \"they'll\": 1449, 'account': 1450, 'fresh': 1451, 'sorry,': 1452, 'include': 1453, 'anybody': 1454, 'expected': 1455, 'guess.': 1456, 'neither': 1457, 'cleaning': 1458, 'lately': 1459, 'party.': 1460, 'popular': 1461, 'cross': 1462, 'fighting': 1463, 'training': 1464, 'letting': 1465, 'music.': 1466, 'credit': 1467, 'room,': 1468, 'jobs': 1469, 'fingers': 1470, 'jump': 1471, 'count': 1472, 'checked': 1473, 'changing': 1474, 'enjoying': 1475, 'funny,': 1476, 'place,': 1477, 'doctor': 1478, 'positive': 1479, 'weeks.': 1480, 'image': 1481, 'result': 1482, 'details': 1483, \"ain't\": 1484, 'highly': 1485, 'annoying': 1486, 'crying': 1487, 'for.': 1488, 'blah': 1489, 'vacation': 1490, 'member': 1491, 'you!': 1492, 'private': 1493, 'point.': 1494, 'other.': 1495, 'reminds': 1496, 'democratic': 1497, 'incredibly': 1498, 'kicked': 1499, 'avoid': 1500, 'screen': 1501, 'more,': 1502, 'society': 1503, 'pizza': 1504, 'movie,': 1505, 'toward': 1506, 'tour': 1507, 'role': 1508, 'opening': 1509, 'enough,': 1510, 'generally': 1511, 'seven': 1512, '\"i\\'m': 1513, 'corner': 1514, 'response': 1515, 'breakfast': 1516, 'americans': 1517, 'nature': 1518, 'handle': 1519, 'invited': 1520, 'google': 1521, 'planned': 1522, 'moments': 1523, 'mall': 1524, 'network': 1525, 'grand': 1526, 'below': 1527, 'forced': 1528, 'pair': 1529, 'discussion': 1530, 'think,': 1531, 'accept': 1532, 'true.': 1533, 'foreign': 1534, 'release': 1535, 'hand,': 1536, 'happen.': 1537, 'enough.': 1538, 'anything,': 1539, 'surprise': 1540, 'long,': 1541, 'skin': 1542, 'foot': 1543, 'friend,': 1544, \"one's\": 1545, 'provide': 1546, 'pieces': 1547, 'mrs.': 1548, 'pages': 1549, 'roll': 1550, 'throughout': 1551, 'around,': 1552, 'tiny': 1553, 'stood': 1554, 'lesson': 1555, 'guitar': 1556, 'double': 1557, 'story.': 1558, 'fully': 1559, 'sell': 1560, 'book,': 1561, 'poetry': 1562, 'price': 1563, 'campaign': 1564, 'rights': 1565, 'heavy': 1566, 'physical': 1567, \"we'd\": 1568, 'plenty': 1569, 'promise': 1570, 'happened.': 1571, 'software': 1572, 'female': 1573, 'speech': 1574, 'winter': 1575, 'officially': 1576, 'spot': 1577, 'bother': 1578, 'machine': 1579, 'now...': 1580, 'excuse': 1581, 'otherwise': 1582, 'flying': 1583, 'build': 1584, 'dancing': 1585, 'killing': 1586, 'have.': 1587, 'lights': 1588, 'bottle': 1589, 'opinion': 1590, 'fun!': 1591, 'job,': 1592, 'dean': 1593, 'stress': 1594, 'daughter': 1595, 'loving': 1596, 'thus': 1597, 'british': 1598, 'desire': 1599, 'tree': 1600, 'bear': 1601, 'administration': 1602, 'acting': 1603, 'source': 1604, 'describe': 1605, 'travel': 1606, 'chicago': 1607, 'sure,': 1608, 'exam': 1609, 'minutes.': 1610, 'well...': 1611, 'pants': 1612, 'too!': 1613, 'signed': 1614, 'says,': 1615, 'soooo': 1616, 'others.': 1617, 'freedom': 1618, 'average': 1619, 'player': 1620, 'official': 1621, 'reminded': 1622, 'now!': 1623, 'like.': 1624, 'friend.': 1625, 'reached': 1626, 'idea.': 1627, 'ourselves': 1628, 'purpose': 1629, 'kitchen': 1630, 'class,': 1631, 'race': 1632, 'hospital': 1633, 'immediately': 1634, 'site.': 1635, 'friday,': 1636, 'face.': 1637, 'haha': 1638, 'failed': 1639, 'lately.': 1640, 'design': 1641, 'steve': 1642, 'famous': 1643, 'cars': 1644, 'washington': 1645, 'threw': 1646, 'ends': 1647, 'senior': 1648, 'natural': 1649, 'emotional': 1650, 'summer.': 1651, 'rush': 1652, 'hall': 1653, 'bless': 1654, 'memories': 1655, 'bigger': 1656, 'studying': 1657, 'about,': 1658, 'seriously,': 1659, 'attack': 1660, 'push': 1661, 'point,': 1662, 'reports': 1663, 'girlfriend': 1664, 'mood:': 1665, 'guys,': 1666, 'ladies': 1667, 'moment.': 1668, 'skills': 1669, 'covered': 1670, 'notes': 1671, 'moon': 1672, 'sure.': 1673, 'this:': 1674, 'nice,': 1675, 'tough': 1676, 'texas': 1677, 'heading': 1678, 'done,': 1679, 'bringing': 1680, 'crowd': 1681, 'stars': 1682, 'mess': 1683, 'text': 1684, 'episode': 1685, 'everyday': 1686, 'jeff': 1687, 'music,': 1688, 'honestly': 1689, 'dying': 1690, 'end,': 1691, 'heat': 1692, 'guy.': 1693, 'market': 1694, 'constantly': 1695, 'beauty': 1696, 'sucks.': 1697, 'mass': 1698, 'better,': 1699, 'car,': 1700, 'boyfriend': 1701, 'month.': 1702, 'becomes': 1703, 'whats': 1704, 'alone.': 1705, 'bout': 1706, 'hurts': 1707, 'culture': 1708, 'performance': 1709, 'august': 1710, 'person,': 1711, 'checking': 1712, 'mind,': 1713, 'closed': 1714, 'topic': 1715, 'pm):': 1716, 'friday.': 1717, 'offered': 1718, 'entirely': 1719, 'wrong,': 1720, 'jack': 1721, 'sad.': 1722, 'great,': 1723, 'clearly': 1724, 'fine.': 1725, 'example': 1726, '*sigh*': 1727, 'tickets': 1728, 'goal': 1729, 'hide': 1730, 'huh?': 1731, 'finally,': 1732, 'terms': 1733, 'guy,': 1734, 'bathroom': 1735, 'cheap': 1736, '(for': 1737, 'bands': 1738, 'career': 1739, 'unfortunately,': 1740, 'fellow': 1741, 'pics': 1742, 'terrible': 1743, 'male': 1744, 'basic': 1745, 'forgotten': 1746, 'quit': 1747, 'anywhere': 1748, 'ordered': 1749, 'sports': 1750, 'remembered': 1751, 'ring': 1752, 'television': 1753, 'lying': 1754, 'potential': 1755, 'cash': 1756, 'listened': 1757, 'dogs': 1758, 'schedule': 1759, 'traffic': 1760, 'japanese': 1761, 'orange': 1762, 'religious': 1763, 'dollars': 1764, 'department': 1765, 'obvious': 1766, 'content': 1767, 'comfortable': 1768, 'nick': 1769, 'smoke': 1770, 'with,': 1771, 'left.': 1772, 'quiz': 1773, 'drama': 1774, 'express': 1775, 'block': 1776, 'typing': 1777, 'nation': 1778, 'whatever.': 1779, 'election': 1780, 'apple': 1781, 'effect': 1782, 'joined': 1783, 'iraqi': 1784, 'plastic': 1785, 'confused': 1786, 'lies': 1787, 'yellow': 1788, 'course.': 1789, 'magazine': 1790, 'built': 1791, 'normally': 1792, 'central': 1793, 'own.': 1794, 'taught': 1795, 'plays': 1796, 'california': 1797, 'theme': 1798, 'sold': 1799, 'burn': 1800, 'laughed': 1801, 'semester': 1802, 'value': 1803, 'bank': 1804, 'theory': 1805, 'together,': 1806, 'kate': 1807, 'dude': 1808, 'creative': 1809, 'development': 1810, 'ass.': 1811, 'sending': 1812, 'policy': 1813, 'deserve': 1814, 'stomach': 1815, 'ryan': 1816, 'sexual': 1817, 'kevin': 1818, 'eight': 1819, 'breaking': 1820, 'staff': 1821, 'cousin': 1822, 'fucked': 1823, 'screaming': 1824, 'mental': 1825, 'name.': 1826, 'problem.': 1827, 'girl,': 1828, 'setting': 1829, 'desk': 1830, 'code': 1831, 'blow': 1832, 'words,': 1833, 'theres': 1834, 'crazy.': 1835, 'soon,': 1836, 'herself': 1837, 'groups': 1838, 'guys.': 1839, 'brand': 1840, 'care.': 1841, 'majority': 1842, 'manage': 1843, 'chat': 1844, 'reason,': 1845, 'prefer': 1846, 'food.': 1847, 'legs': 1848, 'uncle': 1849, 'runs': 1850, 'harder': 1851, 'cable': 1852, 'did,': 1853, 'isnt': 1854, 'reads': 1855, 'plain': 1856, 'directly': 1857, 'note,': 1858, 'waking': 1859, 'gone.': 1860, 'politics': 1861, 'grew': 1862, \"people's\": 1863, 'lower': 1864, 'bible': 1865, 'journal': 1866, 'family,': 1867, 'remind': 1868, 'restaurant': 1869, 'caused': 1870, 'remain': 1871, '(who': 1872, 'flight': 1873, 'hole': 1874, 'first.': 1875, 'serve': 1876, 'today!': 1877, 'months.': 1878, 'example,': 1879, 'waited': 1880, 'monday.': 1881, 'speed': 1882, 'throwing': 1883, 'wide': 1884, 'lyrics': 1885, 'battle': 1886, 'poem': 1887, 'show,': 1888, 'aware': 1889, 'compared': 1890, 'saturday,': 1891, 'services': 1892, 'september': 1893, 'eyes.': 1894, 'kelly': 1895, 'laura': 1896, 'location:': 1897, 'released': 1898, \"mom's\": 1899, 'heh.': 1900, 'republican': 1901, 'load': 1902, 'soft': 1903, 'wild': 1904, 'numbers': 1905, 'afternoon.': 1906, 'passing': 1907, 'personality': 1908, 'everyone.': 1909, 'seat': 1910, 'pack': 1911, 'tons': 1912, 'schools': 1913, 'mine.': 1914, 'robert': 1915, 'modern': 1916, 'rules': 1917, 'hours,': 1918, 'game,': 1919, 'paint': 1920, 'already.': 1921, 'hell,': 1922, 'why.': 1923, 'shoot': 1924, 'lonely': 1925, 'seconds': 1926, 'dinner.': 1927, 'peter': 1928, 'this?': 1929, 'hehe.': 1930, 'spirit': 1931, 'understanding': 1932, 'weird.': 1933, 'money,': 1934, 'army': 1935, 'dressed': 1936, 'debate': 1937, \"she'll\": 1938, 'far,': 1939, 'girl.': 1940, \"god's\": 1941, 'case,': 1942, 'late.': 1943, 'expecting': 1944, 'leading': 1945, 'picking': 1946, 'channel': 1947, 'thought.': 1948, 'companies': 1949, 'breath': 1950, 'sometimes,': 1951, 'wow.': 1952, 'prove': 1953, 'dumb': 1954, 'aside': 1955, 'downtown': 1956, 'why,': 1957, 'fans': 1958, 'rule': 1959, 'cake': 1960, 'ugly': 1961, 'selling': 1962, 'returned': 1963, 'bed,': 1964, 'indeed': 1965, 'drinks': 1966, 'saturday.': 1967, 'characters': 1968, 'charge': 1969, 'anna': 1970, 'potter': 1971, 'spoke': 1972, 'now?': 1973, 'old.': 1974, 'far.': 1975, 'everytime': 1976, 'lake': 1977, 'score': 1978, 'andrew': 1979, 'loss': 1980, 'pressure': 1981, 'info': 1982, 'river': 1983, 'enter': 1984, 'appears': 1985, 'alright': 1986, 'joke': 1987, 'percent': 1988, 'bowl': 1989, 'diet': 1990, 'treat': 1991, 'song.': 1992, 'sleep,': 1993, 'cards': 1994, 'moment,': 1995, 'nervous': 1996, 'asks': 1997, 'advice': 1998, 'match': 1999, 'continued': 2000, 'responsible': 2001, 'says:': 2002, 'else,': 2003, 'legal': 2004, 'possible.': 2005, 'spread': 2006, 'claim': 2007, 'out!': 2008, 'therefore': 2009, 'drug': 2010, 'download': 2011, 'wash': 2012, 'plane': 2013, 'personally': 2014, 'sunday,': 2015, 'conference': 2016, 'kinds': 2017, 'strength': 2018, 'sunday.': 2019, 'afford': 2020, 'reason.': 2021, 'honest': 2022, 'concept': 2023, 'individual': 2024, 'grown': 2025, 'heads': 2026, 'raise': 2027, 'butt': 2028, 'post,': 2029, 'giant': 2030, 'change.': 2031, 'mainly': 2032, 'sites': 2033, 'celebrate': 2034, 'mary': 2035, 'projects': 2036, 'draw': 2037, 'sounded': 2038, 'grab': 2039, 'pure': 2040, 'photographer:': 2041, 'staring': 2042, 'hits': 2043, 'recommend': 2044, 'crappy': 2045, 'financial': 2046, '\"it\\'s': 2047, 'aunt': 2048, 'couldnt': 2049, 'kids.': 2050, \"dad's\": 2051, 'answers': 2052, 'evening.': 2053, \"judge's\": 2054, 'nose': 2055, 'amazing.': 2056, 'spanish': 2057, 'ending': 2058, 'manager': 2059, 'which,': 2060, 'tests': 2061, 'sudden': 2062, 'spell': 2063, 'regarding': 2064, 'favourite': 2065, 'other,': 2066, 'ticket': 2067, 'animal': 2068, 'specific': 2069, 'collection': 2070, 'cast': 2071, 'apparently,': 2072, 'nine': 2073, 'andy': 2074, 'younger': 2075, 'chose': 2076, 'shit,': 2077, 'heart.': 2078, 'head,': 2079, 'appear': 2080, 'pretend': 2081, 'programs': 2082, 'grocery': 2083, 'alive': 2084, 'opposite': 2085, 'depressed': 2086, 'birthday.': 2087, 'degree': 2088, 'thrown': 2089, 'hell.': 2090, 'dating': 2091, 'music:': 2092, 'awake': 2093, 'screw': 2094, 'burning': 2095, 'trade': 2096, 'sorry.': 2097, 'talks': 2098, 'sick.': 2099, 'laid': 2100, 'prepared': 2101, '(like': 2102, 'bike': 2103, 'food,': 2104, 'awful': 2105, 'plus,': 2106, 'cats': 2107, 'essay': 2108, 'tape': 2109, 'literally': 2110, 'read.': 2111, 'country.': 2112, 'time!': 2113, 'geo.': 2114, 'complain': 2115, 'future.': 2116, 'statement': 2117, 'fixed': 2118, 'milk': 2119, 'november': 2120, 'crush': 2121, 'matrix': 2122, '(with': 2123, 'city.': 2124, 'cried': 2125, 'fourth': 2126, 'jason': 2127, 'rice': 2128, 'want.': 2129, 'industry': 2130, 'convinced': 2131, 'happy,': 2132, 'connection': 2133, 'uses': 2134, 'bit,': 2135, \"they'd\": 2136, 'wore': 2137, 'christ': 2138, 'everywhere': 2139, 'baseball': 2140, 'ignore': 2141, 'wait,': 2142, 'significant': 2143, 'monday,': 2144, 'hitting': 2145, 'puts': 2146, 'file': 2147, 'will.': 2148, 'liberal': 2149, 'sigh.': 2150, 'passion': 2151, 'attend': 2152, 'correct': 2153, 'religion': 2154, 'smoking': 2155, 'brief': 2156, 'teeth': 2157, 'saved': 2158, 'couch': 2159, 'updated': 2160, 'saddam': 2161, 'song,': 2162, 'powerful': 2163, \"everyone's\": 2164, 'hard.': 2165, 'anyways': 2166, 'professional': 2167, 'helps': 2168, 'unlike': 2169, 'crap.': 2170, 'packed': 2171, 'that...': 2172, 'london': 2173, 'again!': 2174, 'computer.': 2175, 'impossible': 2176, 'familiar': 2177, 'can.': 2178, 'nasty': 2179, 'and/or': 2180, 'developed': 2181, 'wine': 2182, 'soccer': 2183, 'awhile': 2184, 'heck': 2185, 'expensive': 2186, 'promised': 2187, 'material': 2188, 'included': 2189, 'october': 2190, 'shes': 2191, 'bucks': 2192, \"bush's\": 2193, 'name,': 2194, 'melissa': 2195, 'lang': 2196, \"year's\": 2197, 'messed': 2198, 'different.': 2199, 'katie': 2200, 'leader': 2201, 'laundry': 2202, 'swimming': 2203, 'thousand': 2204, 'distance': 2205, 'streets': 2206, 'emily': 2207, 'water.': 2208, \"(i'm\": 2209, 'negative': 2210, 'massive': 2211, 'concerned': 2212, 'proper': 2213, 'sharing': 2214, 'stands': 2215, 'bread': 2216, 'discuss': 2217, 'drank': 2218, 'ears': 2219, 'stupid.': 2220, 'same.': 2221, 'federal': 2222, 'that!': 2223, 'global': 2224, 'standard': 2225, 'heaven': 2226, 'yung': 2227, 'thought,': 2228, 'happiness': 2229, 'thru': 2230, 'path': 2231, 'island': 2232, 'shared': 2233, 'musical': 2234, 'experience.': 2235, 'candy': 2236, 'airport': 2237, 'medical': 2238, 'weeks,': 2239, '\"this': 2240, 'signs': 2241, '(but': 2242, 'letters': 2243, 'necessary': 2244, 'hair.': 2245, 'corporate': 2246, 'edge': 2247, 'director': 2248, 'lol!': 2249, 'laptop': 2250, 'lunch.': 2251, 'hundred': 2252, 'help.': 2253, 'hopes': 2254, 'relationships': 2255, 'door.': 2256, 'hill': 2257, 'papers': 2258, 'truck': 2259, 'so...': 2260, 'iraq.': 2261, 'mission': 2262, 'rent': 2263, 'used:': 2264, 'parties': 2265, 'old,': 2266, 'challenge': 2267, 'guide': 2268, 'site,': 2269, 'chair': 2270, 'johnny': 2271, 'friendship': 2272, 'riding': 2273, 'lessons': 2274, 'tight': 2275, 'haha!': 2276, 'all!': 2277, 'faces': 2278, 'mixed': 2279, 'todd': 2280, 'story,': 2281, '(that': 2282, 'sees': 2283, 'golden': 2284, 'addition': 2285, 'required': 2286, 'fail': 2287, 'author': 2288, 'office.': 2289, 'january': 2290, 'december': 2291, 'overall': 2292, 'cleaned': 2293, 'news.': 2294, 'outta': 2295, 'minutes,': 2296, 'come.': 2297, 'fake': 2298, 'searching': 2299, 'classic': 2300, 'magic': 2301, 'hated': 2302, 'rate': 2303, 'defense': 2304, 'lately,': 2305, 'finger': 2306, 'moral': 2307, 'studies': 2308, 'base': 2309, 'corsair': 2310, 'sexy': 2311, 'master': 2312, 'monkey': 2313, 'late,': 2314, 'placed': 2315, 'mountain': 2316, 'sucked': 2317, 'writer': 2318, 'dealing': 2319, 'raised': 2320, 'fantastic': 2321, 'asian': 2322, 'suggest': 2323, 'thousands': 2324, 'surely': 2325, 'recall': 2326, 'adult': 2327, 'true,': 2328, 'naked': 2329, 'apply': 2330, 'comic': 2331, 'lisa': 2332, \"she'd\": 2333, 'gold': 2334, 'perfectly': 2335, 'winning': 2336, 'wait.': 2337, 'square': 2338, 'fault': 2339, 'shake': 2340, 'healthy': 2341, 'background': 2342, 'josh': 2343, 'steven': 2344, 'hehe': 2345, 'sugar': 2346, 'war.': 2347, 'exams': 2348, 'scott': 2349, 'success': 2350, 'anime': 2351, 'cook': 2352, 'crack': 2353, 'digest': 2354, 'assume': 2355, 'what,': 2356, 'greater': 2357, 'prayer': 2358, 'studio': 2359, 'risk': 2360, 'silence': 2361, 'computers': 2362, 'civil': 2363, 'multiple': 2364, 'rolling': 2365, 'presidential': 2366, 'for,': 2367, '(this': 2368, 'data': 2369, 'images': 2370, 'regret': 2371, 'fashion': 2372, 'constant': 2373, 'increase': 2374, 'least,': 2375, 'leads': 2376, 'hour.': 2377, 'eyes,': 2378, 'economic': 2379, 'trip.': 2380, 'screwed': 2381, 'audience': 2382, 'conservative': 2383, 'visiting': 2384, 'typical': 2385, 'visited': 2386, 'judge': 2387, 'emotions': 2388, 'falls': 2389, 'left,': 2390, 'inner': 2391, 'exact': 2392, 'bedroom': 2393, 'grace': 2394, 'unfortunately': 2395, 'boring.': 2396, 'journey': 2397, 'hmm.': 2398, 'protect': 2399, 'hates': 2400, 'unique': 2401, 'forever.': 2402, 'countries': 2403, 'dare': 2404, 'angel': 2405, 'shift': 2406, 'garden': 2407, 'have,': 2408, 'know?': 2409, 'sales': 2410, 'going.': 2411, \"friend's\": 2412, 'blind': 2413, 'party,': 2414, 'recognize': 2415, 'goodbye': 2416, 'wouldnt': 2417, 'walks': 2418, 'thomas': 2419, 'published': 2420, 'successful': 2421, 'through.': 2422, 'rarely': 2423, 'canada': 2424, 'chapter': 2425, 'words.': 2426, 'direct': 2427, 'lot,': 2428, 'today...': 2429, 'receive': 2430, 'junior': 2431, 'yours': 2432, 'doing.': 2433, 'die.': 2434, 'evidence': 2435, 'friendly': 2436, 'war,': 2437, 'calm': 2438, 'great!': 2439, 'hollywood': 2440, 'readers': 2441, 'matter.': 2442, 'bothered': 2443, 'rose': 2444, \"someone's\": 2445, 'cancer': 2446, 'invite': 2447, \"'the\": 2448, 'weapons': 2449, 'yourself.': 2450, 'jumped': 2451, 'wave': 2452, 'side,': 2453, 'forth': 2454, 'thinking,': 2455, 'good!': 2456, 'loads': 2457, 'colors': 2458, 'steps': 2459, 'richard': 2460, 'lives.': 2461, 'piss': 2462, 'italian': 2463, 'silver': 2464, 'boston': 2465, 'suggested': 2466, 'adding': 2467, 'microsoft': 2468, 'management': 2469, 'sang': 2470, 'tired,': 2471, 'spiritual': 2472, 'sean': 2473, 'side.': 2474, 'reporting': 2475, 'afternoon,': 2476, 'shots': 2477, 'whoever': 2478, 'novel': 2479, 'provided': 2480, 'degrees': 2481, 'tall': 2482, 'escape': 2483, 'practically': 2484, 'shown': 2485, 'sore': 2486, 'list.': 2487, 'birth': 2488, 'shape': 2489, 'brothers': 2490, 'metal': 2491, 'anger': 2492, 'trees': 2493, 'drugs': 2494, 'sometimes.': 2495, 'feed': 2496, 'dollar': 2497, 'piano': 2498, 'freak': 2499, 'jackson': 2500, 'driver': 2501, 'clue': 2502, 'unable': 2503, 'lunch,': 2504, 'cares': 2505, 'feature': 2506, 'annoyed': 2507, 'chick': 2508, 'boy,': 2509, 'served': 2510, 'best.': 2511, 'jessica': 2512, 'twenty': 2513, 'clinton': 2514, 'rise': 2515, 'phone.': 2516, 'ultimate': 2517, 'dinner,': 2518, 'scream': 2519, 'views': 2520, 'alex': 2521, 'millions': 2522, 'kids,': 2523, 'writes': 2524, 'guilty': 2525, 'cooking': 2526, 'creating': 2527, 'democrats': 2528, 'everyone,': 2529, 'pero': 2530, 'beside': 2531, 'sight': 2532, 'remove': 2533, 'stressed': 2534, 'inspired': 2535, 'part.': 2536, 'complex': 2537, 'least.': 2538, 'more!': 2539, 'bored.': 2540, 'argument': 2541, 'product': 2542, 'midnight': 2543, 'worse.': 2544, 'attitude': 2545, 'porn': 2546, 'limited': 2547, 'extreme': 2548, 'instead,': 2549, 'german': 2550, 'host': 2551, 'incredible': 2552, 'convince': 2553, 'grandma': 2554, 'shooting': 2555, 'lame': 2556, 'doors': 2557, 'dangerous': 2558, 'finds': 2559, 'steal': 2560, 'smaller': 2561, 'billion': 2562, 'forgive': 2563, 'tony': 2564, 'anymore,': 2565, 'upcoming': 2566, 'stone': 2567, \"it'd\": 2568, 'intelligence': 2569, 'there!': 2570, 'flat': 2571, 'responsibility': 2572, 'saying,': 2573, 'test.': 2574, 'discussing': 2575, 'howard': 2576, 'tech': 2577, 'forces': 2578, 'hungry': 2579, 'advantage': 2580, 'western': 2581, 'soldiers': 2582, 'wasted': 2583, 'informed': 2584, 'jesse': 2585, 'queen': 2586, 'sad,': 2587, 'two.': 2588, 'congrats': 2589, 'believed': 2590, 'months,': 2591, 'flash': 2592, 'toilet': 2593, 'summer,': 2594, 'hmm...': 2595, 'hand.': 2596, 'types': 2597, 'clock': 2598, 'comfort': 2599, 'hidden': 2600, 'para': 2601, 'city,': 2602, 'butter': 2603, 'accepted': 2604, 'jeans': 2605, 'begins': 2606, 'faster': 2607, 'button': 2608, 'loose': 2609, 'agreed': 2610, 'honor': 2611, 'continues': 2612, 'prepare': 2613, 'allan': 2614, 'sorts': 2615, 'walls': 2616, 'approach': 2617, 'florida': 2618, 'online.': 2619, 'application': 2620, 'bags': 2621, 'fine,': 2622, 'town.': 2623, 'storm': 2624, 'thankful': 2625, 'rings': 2626, 'progress': 2627, 'animals': 2628, 'pride': 2629, 'pulling': 2630, 'articles': 2631, 'commercial': 2632, 'hey!': 2633, '(you': 2634, 'fantasy': 2635, 'santa': 2636, 'impact': 2637, 'sense.': 2638, 'others,': 2639, 'break.': 2640, 'suit': 2641, 'print': 2642, 'solid': 2643, 'physics': 2644, 'troops': 2645, 'locked': 2646, 'baby,': 2647, 'insurance': 2648, 'once.': 2649, 'split': 2650, 'messages': 2651, 'brilliant': 2652, 'goin': 2653, 'rant': 2654, 'stole': 2655, 'designed': 2656, 'counting': 2657, 'pounds': 2658, 'line.': 2659, 'word.': 2660, 'computer,': 2661, 'model': 2662, 'bits': 2663, 'february': 2664, 'once,': 2665, 'target': 2666, 'pain.': 2667, 'meal': 2668, 'matters': 2669, 'alarm': 2670, 'alright,': 2671, 'pushing': 2672, 'part,': 2673, 'excited.': 2674, 'golf': 2675, 'includes': 2676, 'treated': 2677, 'boat': 2678, 'benefit': 2679, 'shame': 2680, 'soup': 2681, 'england': 2682, 'trash': 2683, 'church.': 2684, 'entertainment': 2685, 'throat': 2686, 'haha,': 2687, 'punk': 2688, 'planet': 2689, 'features': 2690, 'exchange': 2691, 'again...': 2692, 'finishing': 2693, 'had.': 2694, 'graduation': 2695, 'neck': 2696, 'remains': 2697, 'question.': 2698, 'mom,': 2699, 'ocean': 2700, 'catching': 2701, 'surgery': 2702, 'thin': 2703, 'face,': 2704, 'shortaznpwer06:': 2705, 'alcohol': 2706, 'idiot': 2707, 'mouse': 2708, 'counter': 2709, 'separate': 2710, 'meat': 2711, 'marry': 2712, 'p.m.': 2713, 'does.': 2714, 'reply': 2715, 'candidate': 2716, 'p.s.': 2717, 'cutting': 2718, 'jumping': 2719, 'read,': 2720, 'leaders': 2721, 'grass': 2722, 'luckily': 2723, 'books,': 2724, 'vision': 2725, 'mistake': 2726, 'grabbed': 2727, 'shitty': 2728, 'guest': 2729, 'time...': 2730, 'yard': 2731, 'rare': 2732, 'nowhere': 2733, 'refuse': 2734, 'tries': 2735, 'eaten': 2736, 'appropriate': 2737, 'chances': 2738, 'larger': 2739, 'everything,': 2740, 'exist': 2741, 'struck': 2742, 'union': 2743, 'besides,': 2744, 'reported': 2745, 'is...': 2746, 'variety': 2747, 'actions': 2748, 'pointed': 2749, 'introduced': 2750, 'hair,': 2751, 'activities': 2752, 'month,': 2753, 'impressed': 2754, 'touched': 2755, 'william': 2756, 'detail': 2757, 'coast': 2758, 'short,': 2759, 'presents': 2760, 'needless': 2761, 'company.': 2762, 'tune': 2763, 'players': 2764, 'kyle': 2765, 'effects': 2766, 'necessarily': 2767, 'direction': 2768, 'originally': 2769, 'area.': 2770, 'terribly': 2771, 'bell': 2772, 'kicking': 2773, 'films': 2774, 'coolest': 2775, 'lips': 2776, 'romantic': 2777, 'active': 2778, 'pleasure': 2779, 'appointment': 2780, 'minds': 2781, 'badly': 2782, 'disappointed': 2783, 'drawn': 2784, 'drag': 2785, 'page.': 2786, 'carrying': 2787, 'upper': 2788, 'glasses': 2789, 'suffering': 2790, 'yelling': 2791, 'independent': 2792, 'sale': 2793, 'btw,': 2794, 'worship': 2795, 'literacy': 2796, 'saving': 2797, 'prom': 2798, 'quarter': 2799, 'realised': 2800, 'flowers': 2801, 'thursday,': 2802, 'moves': 2803, 'items': 2804, 'merely': 2805, 'breaks': 2806, 'tear': 2807, 'fuckin': 2808, 'reference': 2809, 'evening,': 2810, 'iraq,': 2811, 'system.': 2812, 'session': 2813, 'remember,': 2814, 'convention': 2815, 'bowling': 2816, 'death.': 2817, 'alone,': 2818, 'you...': 2819, 'hard,': 2820, 'bird': 2821, 'allows': 2822, 'books.': 2823, 'carried': 2824, 'presence': 2825, 'range': 2826, 'absolute': 2827, 'threat': 2828, 'mini': 2829, 'interesting,': 2830, 'silent': 2831, 'emails': 2832, 'situation.': 2833, 'crew': 2834, 'hot.': 2835, 'innocent': 2836, 'balance': 2837, 'resources': 2838, 'professor': 2839, 'mood.': 2840, 'complaining': 2841, 'darn': 2842, 'experienced': 2843, 'fallen': 2844, 'theater': 2845, 'pushed': 2846, 'meow': 2847, 'floor.': 2848, 'damned': 2849, 'fabulous': 2850, 'drew': 2851, 'gain': 2852, 'instead.': 2853, 'salt': 2854, 'hearts': 2855, 'oscar': 2856, 'shock': 2857, 'circle': 2858, 'production': 2859, 'gained': 2860, 'shep': 2861, 'growth': 2862, 'movies,': 2863, 'areas': 2864, 'today:': 2865, 'answered': 2866, 'repeat': 2867, 'develop': 2868, 'minor': 2869, 'jokes': 2870, 'birds': 2871, 'happens.': 2872, 'rough': 2873, 'ones.': 2874, 'announced': 2875, 'because,': 2876, 'conversations': 2877, 'free.': 2878, 'experiences': 2879, 'beautiful.': 2880, 'cold,': 2881, 'insane': 2882, 'claims': 2883, 'here!': 2884, 'paper.': 2885, 'yes!': 2886, 'praying': 2887, 'quotes': 2888, 'water,': 2889, 'blessed': 2890, 'explained': 2891, 'heather': 2892, 'seriously.': 2893, 'opinions': 2894, \"world's\": 2895, 'instant': 2896, 'partner': 2897, 'billy': 2898, 'southern': 2899, 'fancy': 2900, 'deal.': 2901, 'reaction': 2902, 'strike': 2903, '\"why': 2904, 'a.m.': 2905, 'mile': 2906, 'phrase': 2907, 'sets': 2908, 'darkness': 2909, '(yes,': 2910, 'prolly': 2911, 'yeah!': 2912, 'justice': 2913, 'pour': 2914, 'pathetic': 2915, 'chosen': 2916, 'sentence': 2917, 'families': 2918, 'wise': 2919, 'sunny': 2920, 'dick': 2921, 'highest': 2922, 'relatively': 2923, 'balls': 2924, 'opposed': 2925, 'sisters': 2926, 'scheduled': 2927, 'exercise': 2928, 'affect': 2929, 'critical': 2930, 'closest': 2931, 'houses': 2932, 'spare': 2933, 'purchase': 2934, 'dead.': 2935, 'problems.': 2936, 'easter': 2937, 'perform': 2938, 'bridge': 2939, 'reagan': 2940, 'weak': 2941, 'plot': 2942, '\"well,': 2943, 'opportunities': 2944, 'charles': 2945, 'decisions': 2946, 'discover': 2947, 'two,': 2948, 'rocks': 2949, 'county': 2950, 'holds': 2951, 'budget': 2952, 'moore': 2953, 'frank': 2954, 'body.': 2955, 'switch': 2956, 'smiling': 2957, 'lol,': 2958, 'republicans': 2959, 'country,': 2960, 'academic': 2961, \"y'all\": 2962, 'weekly': 2963, 'drawing': 2964, 'games.': 2965, 'fool': 2966, '\"how': 2967, 'greek': 2968, 'primary': 2969, 'day...': 2970, 'presentation': 2971, 'template': 2972, 'voted': 2973, 'mine,': 2974, 'stop.': 2975, 'talent': 2976, 'basically,': 2977, 'replace': 2978, 'goals': 2979, 'annual': 2980, 'described': 2981, 'definately': 2982, 'life?': 2983, 'another.': 2984, 'movement': 2985, 'sorta': 2986, 'want,': 2987, 'universe': 2988, 'website.': 2989, 'afterwards': 2990, 'possibility': 2991, 'tied': 2992, 'birthday,': 2993, 'disney': 2994, 'prime': 2995, 'understood': 2996, 'award': 2997, 'hmm,': 2998, 'tuesday,': 2999, 'seek': 3000, 'trips': 3001, \"don't.\": 3002, 'please,': 3003, 'costs': 3004, 'arts': 3005, 'purple': 3006, 'require': 3007, 'useful': 3008, 'customer': 3009, 'them!': 3010, 'cultural': 3011, 'wondered': 3012, 'store.': 3013, 'group.': 3014, 'basketball': 3015, 'bite': 3016, 'influence': 3017, 'coach': 3018, 'settle': 3019, 'ought': 3020, 'thanksgiving': 3021, 'graduate': 3022, 'amanda': 3023, 'traditional': 3024, 'supposedly': 3025, 'vegas': 3026, 'second,': 3027, 'buddy': 3028, 'wisdom': 3029, 'justin': 3030, 'regardless': 3031, 'efforts': 3032, 'honestly,': 3033, 'college.': 3034, 'argue': 3035, 'requires': 3036, 'nuclear': 3037, 'band.': 3038, 'closing': 3039, 'movies.': 3040, 'damn,': 3041, 'noise': 3042, 'hundreds': 3043, 'yeah...': 3044, 'urge': 3045, 'note:': 3046, 'singer': 3047, 'boxes': 3048, 'raining': 3049, 'easy.': 3050, 'reviews': 3051, 'neighborhood': 3052, 'hilarious': 3053, 'depends': 3054, 'accident': 3055, 'often.': 3056, 'cold.': 3057, 'belong': 3058, 'hook': 3059, 'breathe': 3060, 'care,': 3061, 'heart,': 3062, 'newspaper': 3063, 'babies': 3064, 'period.': 3065, 'status': 3066, '100%': 3067, 'removed': 3068, 'there?': 3069, 'dream.': 3070, 'neat': 3071, 'laugh.': 3072, 'comedy': 3073, '9/11': 3074, 'vice': 3075, 'associated': 3076, 'hurt.': 3077, 'respond': 3078, 'sake': 3079, 'hour,': 3080, 'canadian': 3081, 'curious': 3082, 'outside.': 3083, 'gifts': 3084, 'coke': 3085, 'film.': 3086, 'someone,': 3087, 'play.': 3088, 'population': 3089, 'workout': 3090, 'internet.': 3091, 'provides': 3092, 'chest': 3093, 'mobile': 3094, 'cute.': 3095, 'belief': 3096, 'street.': 3097, 'garbage': 3098, 'days!': 3099, 'blew': 3100, 'praise': 3101, 'blog!': 3102, 'facts': 3103, 'painful': 3104, 'apologize': 3105, 'testing': 3106, 'wrapped': 3107, 'toronto': 3108, 'handed': 3109, 'village': 3110, 'himself.': 3111, 'attached': 3112, 'entries': 3113, 'anyone.': 3114, 'appeared': 3115, '(even': 3116, 'improve': 3117, 'conclusion': 3118, 'anyhow,': 3119, 'pleased': 3120, 'office,': 3121, 'surrounded': 3122, 'realise': 3123, 'precious': 3124, 'actually.': 3125, 'baby.': 3126, 'drives': 3127, \"(it's\": 3128, 'shocked': 3129, 'wishing': 3130, 'naman': 3131, 'usual.': 3132, 'laying': 3133, 'works.': 3134, 'themselves.': 3135, 'martin': 3136, 'grey': 3137, 'license': 3138, 'mom.': 3139, 'change,': 3140, 'valley': 3141, 'ridiculous': 3142, 'wow!': 3143, 'ever,': 3144, 'humor': 3145, 'jennifer': 3146, 'sits': 3147, 'past,': 3148, \"that'll\": 3149, 'coverage': 3150, 'downstairs': 3151, 'scenes': 3152, 'gray': 3153, 'kung': 3154, 'causing': 3155, 'yea,': 3156, 'tuesday.': 3157, 'junk': 3158, 'ages': 3159, 'early.': 3160, 'mean.': 3161, 'wednesday,': 3162, 'marks': 3163, 'will,': 3164, 'hands.': 3165, 'hurting': 3166, 'someone.': 3167, 'task': 3168, 'server': 3169, 'mirror': 3170, 'returning': 3171, 'cry.': 3172, 'childhood': 3173, 'jane': 3174, 'flew': 3175, 'offers': 3176, 'funniest': 3177, 'door,': 3178, 'cool!': 3179, 'crossed': 3180, 'japan': 3181, 'amazed': 3182, 'paris': 3183, 'skip': 3184, 'hiding': 3185, 'yell': 3186, 'leave.': 3187, 'slight': 3188, 'history.': 3189, 'yahoo': 3190, 'obsessed': 3191, 'gone,': 3192, 'freaked': 3193, 'blonde': 3194, 'upstairs': 3195, 'environment': 3196, 'marketing': 3197, 'chatting': 3198, 'dropping': 3199, 'smith': 3200, 'gorgeous': 3201, 'forms': 3202, 'arrive': 3203, 'suicide': 3204, 'swing': 3205, 'stock': 3206, 'say?': 3207, 'activity': 3208, 'sides': 3209, 'supporting': 3210, 'addicted': 3211, 'anniversary': 3212, 'discussed': 3213, 'parents.': 3214, 'records': 3215, 'competition': 3216, 'headache': 3217, 'homework.': 3218, 'proof': 3219, 'spoken': 3220, 'fast.': 3221, 'levels': 3222, '(although': 3223, 'awards': 3224, 'display': 3225, 'pile': 3226, 'attacks': 3227, 'settled': 3228, 'lost.': 3229, 'relax': 3230, 'girls,': 3231, 'stepped': 3232, 'france': 3233, 'rolled': 3234, 'usual,': 3235, 'lift': 3236, 'impression': 3237, 'them?': 3238, 'kurt': 3239, 'thursday.': 3240, 'happened,': 3241, 'completed': 3242, \"ya'll\": 3243, 'campus': 3244, 'systems': 3245, 'ugh.': 3246, 'favor': 3247, 'eat.': 3248, 'children.': 3249, 'team.': 3250, 'this...': 3251, 'careful': 3252, 'itself.': 3253, 'maintain': 3254, 'deeply': 3255, 'damage': 3256, 'files': 3257, 'yay.': 3258, 'yelled': 3259, '\"oh,': 3260, 'festival': 3261, 'suck.': 3262, 'artist': 3263, 'produce': 3264, 'also.': 3265, 'picture.': 3266, 'reading.': 3267, 'duty': 3268, 'past.': 3269, 'burned': 3270, 'cheer': 3271, 'project.': 3272, 'relate': 3273, 'voting': 3274, 'crash': 3275, \"mother's\": 3276, 'false': 3277, 'fruit': 3278, 'am):': 3279, 'goodness': 3280, 'stores': 3281, 'struggle': 3282, 'jealous': 3283, 'jackie': 3284, 'band,': 3285, 'alternative': 3286, 'understand.': 3287, 'communication': 3288, 'turkey': 3289, 'town,': 3290, 'encourage': 3291, 'choices': 3292, 'out...': 3293, 'live.': 3294, 'know...': 3295, 'angels': 3296, 'back!': 3297, 'thick': 3298, 'too...': 3299, 'business.': 3300, 'from.': 3301, 'determined': 3302, 'phone,': 3303, 'teams': 3304, 'painting': 3305, 'frozen': 3306, 'whilst': 3307, 'basis': 3308, 'finals': 3309, 'technical': 3310, 'owner': 3311, 'laws': 3312, 'very,': 3313, 'stuffed': 3314, 'hooked': 3315, 'stephen': 3316, 'european': 3317, 'seats': 3318, 'generation': 3319, 'secretary': 3320, 'songs.': 3321, 'history,': 3322, 'solution': 3323, 'tennis': 3324, 'tank': 3325, 'cafe': 3326, 't-shirt': 3327, \"would've\": 3328, 'beating': 3329, 'serving': 3330, 'driven': 3331, 'depressing': 3332, 'bound': 3333, 'air.': 3334, 'daniel': 3335, 'stops': 3336, 'garage': 3337, 'extended': 3338, '\"but': 3339, 'candidates': 3340, 'worn': 3341, 'plate': 3342, 'flip': 3343, 'stupid,': 3344, 'benefits': 3345, 'largest': 3346, 'century': 3347, 'theatre': 3348, 'supreme': 3349, 'anyways.': 3350, 'machines': 3351, 'intelligent': 3352, 'anytime': 3353, 'girls.': 3354, 'early,': 3355, 'bitter': 3356, 'frustrated': 3357, \"man's\": 3358, 'college,': 3359, 'grandmother': 3360, 'wings': 3361, 'case.': 3362, 'failure': 3363, 'working.': 3364, 'applied': 3365, 'chemistry': 3366, 'facing': 3367, 'church,': 3368, 'destroy': 3369, 'terrorist': 3370, 'warning': 3371, 'night!': 3372, 'nations': 3373, 'weekends': 3374, 'inches': 3375, 'equal': 3376, 'hero': 3377, 'blocks': 3378, 'chill': 3379, 'minister': 3380, 'nathan': 3381, 'humans': 3382, 'lor.': 3383, '(well,': 3384, 'horse': 3385, 'one!': 3386, 'safety': 3387, 'clark': 3388, 'incident': 3389, 'league': 3390, 'shoulder': 3391, 'option': 3392, 'endless': 3393, 'mistakes': 3394, 'stairs': 3395, 'remember.': 3396, 'entered': 3397, 'reader': 3398, 'pregnant': 3399, 'mere': 3400, 'sweet.': 3401, 'aint': 3402, 'fireworks': 3403, 'hope.': 3404, 'realizing': 3405, 'leadership': 3406, 'officials': 3407, 'writing.': 3408, 'neighbors': 3409, 'either,': 3410, 'labor': 3411, 'portion': 3412, 'philosophy': 3413, 'packing': 3414, 'man!': 3415, 'physically': 3416, 'swim': 3417, 'wireless': 3418, 'think?': 3419, 'feeling.': 3420, 'definition': 3421, 'rachel': 3422, 'html': 3423, 'michigan': 3424, 'mexican': 3425, 'flow': 3426, 'happen,': 3427, 'parents,': 3428, 'halfway': 3429, 'keyboard': 3430, 'goes.': 3431, 'there...': 3432, 'depending': 3433, 'slip': 3434, 'equally': 3435, 'prince': 3436, 'linux': 3437, 'replaced': 3438, 'amusing': 3439, 'boy.': 3440, 'sticking': 3441, '(see': 3442, 'look.': 3443, 'package': 3444, 'allowing': 3445, 'dad,': 3446, 'hubby': 3447, 'proceeded': 3448, 'teenage': 3449, '\"and': 3450, 'wasting': 3451, 'attending': 3452, 'sooooo': 3453, 'stare': 3454, 'offering': 3455, 'freakin': 3456, 'little.': 3457, 'you.\"': 3458, 'seeking': 3459, 'directed': 3460, 'inside.': 3461, 'concern': 3462, 'fried': 3463, 'store,': 3464, 'today?': 3465, 'attention.': 3466, 'china': 3467, 'contest': 3468, 'ralph': 3469, 'prior': 3470, 'actor': 3471, 'studied': 3472, 'busy.': 3473, 'hilarious.': 3474, \"sister's\": 3475, 'utterly': 3476, 'painted': 3477, 'cousins': 3478, 'juice': 3479, 'useless': 3480, 'shirts': 3481, 'dedicated': 3482, 'sand': 3483, 'combination': 3484, 'doctors': 3485, 'acts': 3486, 'blah.': 3487, 'rode': 3488, 'look,': 3489, 'wendy': 3490, 'starbucks': 3491, 'unit': 3492, 'crime': 3493, 'thinking.': 3494, 'properly': 3495, 'suffer': 3496, 'chad': 3497, '(they': 3498, 'bomb': 3499, 'knocked': 3500, 'problem,': 3501, 'property': 3502, 'games,': 3503, 'adventure': 3504, 'scare': 3505, 'profile': 3506, 'amounts': 3507, 'guard': 3508, 'questions.': 3509, 'stretch': 3510, 'effective': 3511, 'smile.': 3512, 'died.': 3513, 'developing': 3514, 'weird,': 3515, 'liking': 3516, 'road.': 3517, 'jeremy': 3518, 'wished': 3519, 'washing': 3520, 'clouds': 3521, 'lord,': 3522, 'fits': 3523, 'everyone!': 3524, 'dust': 3525, 'holidays': 3526, 'someday': 3527, 'drivers': 3528, 'smells': 3529, 'presented': 3530, 'yummy': 3531, 'points.': 3532, 'pete': 3533, 'celebrity': 3534, 'shout': 3535, 'located': 3536, 'seen.': 3537, 'refused': 3538, 'cuts': 3539, 'writing,': 3540, 'literature': 3541, 'marshall': 3542, 'focused': 3543, 'time?': 3544, 'specifically': 3545, 'products': 3546, 'sooner': 3547, 'centre': 3548, 'please.': 3549, 'flag': 3550, 'wishes': 3551, 'on...': 3552, 'capable': 3553, 'children,': 3554, 'camp.': 3555, 'survive': 3556, 'covering': 3557, 'keys': 3558, 'description': 3559, 'folks,': 3560, 'abuse': 3561, 'senator': 3562, 'started.': 3563, 'causes': 3564, 'productive': 3565, 'still.': 3566, 'who,': 3567, 'contract': 3568, 'writers': 3569, 'jazz': 3570, 'asleep.': 3571, 'forum': 3572, 'pleasant': 3573, 'such.': 3574, 'outside,': 3575, 'issue.': 3576, 'guess,': 3577, 'parade': 3578, 'prevent': 3579, 'disease': 3580, 'woman,': 3581, 'date.': 3582, 'loser': 3583, 'keith': 3584, 'jewish': 3585, 'elizabeth': 3586, 'freshman': 3587, \"(that's\": 3588, 'listed': 3589, 'draft': 3590, \"america's\": 3591, 'killer': 3592, 'filling': 3593, 'electric': 3594, 'popped': 3595, 'street,': 3596, 'amber': 3597, 'core': 3598, 'pain,': 3599, 'tomorrow!': 3600, 'salad': 3601, 'fifth': 3602, 'woman.': 3603, \"night's\": 3604, 'crazy,': 3605, 'jimmy': 3606, 'fiction': 3607, 'previously': 3608, 'mystery': 3609, 'grades': 3610, 'bodies': 3611, 'anyway...': 3612, 'owned': 3613, 'electronic': 3614, 'know!': 3615, 'for?': 3616, 'it.\"': 3617, 'get.': 3618, 'fired': 3619, 'alright.': 3620, 'dreaming': 3621, 'entitled': 3622, 'thingy': 3623, 'line,': 3624, 'hella': 3625, 'nothing,': 3626, 'dawn': 3627, 'advertising': 3628, 'paper,': 3629, 'purchased': 3630, 'gmail': 3631, 'overly': 3632, 'economy': 3633, 'urban': 3634, 'pictures.': 3635, 'brother,': 3636, 'blowing': 3637, 'memorial': 3638, 'all...': 3639, 'wars': 3640, 'subway': 3641, 'apartment.': 3642, 'adults': 3643, 'sharp': 3644, 'users': 3645, 'standards': 3646, 'hardest': 3647, 'bars': 3648, 'leather': 3649, 'tongue': 3650, 'bills': 3651, 'halloween': 3652, 'individuals': 3653, 'wednesday.': 3654, 'question,': 3655, 'speaks': 3656, 'unknown': 3657, 'here?': 3658, \"children's\": 3659, 'wrap': 3660, 'attended': 3661, \"(don't\": 3662, 'season.': 3663, 'workers': 3664, 'catholic': 3665, 'whatever,': 3666, 'updates': 3667, 'own,': 3668, 'sick,': 3669, 'mighty': 3670, 'governor': 3671, 'stopping': 3672, 'numerous': 3673, 'try.': 3674, 'initial': 3675, 'vacation.': 3676, 'tastes': 3677, 'appearance': 3678, 'going,': 3679, 'entertaining': 3680, 'state.': 3681, 'engine': 3682, 'michelle': 3683, 'recently,': 3684, 'nite': 3685, 'maybe,': 3686, 'what.': 3687, 'europe': 3688, 'guys!': 3689, 'instruction': 3690, 'choice.': 3691, 'ancient': 3692, 'move.': 3693, 'feel.': 3694, 'refer': 3695, 'employees': 3696, 'states.': 3697, 'fought': 3698, 'episodes': 3699, 'crap,': 3700, 'user': 3701, 'dozen': 3702, 'sleepy': 3703, 'run.': 3704, \"didn't.\": 3705, 'cities': 3706, 'democracy': 3707, 'horror': 3708, 'chatted': 3709, 'knock': 3710, 'beautiful,': 3711, 'blogged': 3712, 'african': 3713, 'albums': 3714, 'jacket': 3715, \"else's\": 3716, 'sauce': 3717, 'relationship.': 3718, 'said:': 3719, 'solo': 3720, 'length': 3721, 'virginia': 3722, \"brother's\": 3723, 'providing': 3724, 'kid.': 3725, 'blast': 3726, 'word,': 3727, 'name:': 3728, 'issues.': 3729, 'along.': 3730, 'stronger': 3731, 'songs,': 3732, 'perspective': 3733, 'big,': 3734, 'socks': 3735, 'worry,': 3736, '\"don\\'t': 3737, 'bush,': 3738, 'biology': 3739, 'light.': 3740, 'power.': 3741, 'scary.': 3742, 'plant': 3743, 'organization': 3744, 'america.': 3745, 'updating': 3746, 'demand': 3747, 'bugs': 3748, 'inch': 3749, 'you\"': 3750, 'online,': 3751, 'peanut': 3752, 'congress': 3753, 'increased': 3754, 'chris,': 3755, 'begun': 3756, 'additional': 3757, 'desperate': 3758, 'confidence': 3759, 'rain.': 3760, 'charlie': 3761, 'park.': 3762, 'highlight': 3763, 'suspect': 3764, 'miserable': 3765, 'service.': 3766, 'emergency': 3767, 'washed': 3768, 'new,': 3769, 'sixth': 3770, 'intellectual': 3771, 'larry': 3772, 'bath': 3773, 'bye.': 3774, 'educational': 3775, 'about?': 3776, 'yourself,': 3777, 'alas,': 3778, 'call.': 3779, 'continuing': 3780, 'existence': 3781, 'abortion': 3782, 'youre': 3783, 'location': 3784, 'violence': 3785, 'sister,': 3786, 'night...': 3787, 'mode': 3788, 'ultimately': 3789, 'climb': 3790, 'peace,': 3791, 'confident': 3792, 'worse,': 3793, 'holla': 3794, 'beats': 3795, 'slam': 3796, 'shaking': 3797, 'latin': 3798, 'age.': 3799, 'wheel': 3800, 'tool': 3801, 'rude': 3802, 'compare': 3803, 'achieve': 3804, 'wood': 3805, 'process.': 3806, 'district': 3807, 'historical': 3808, 'closet': 3809, 'carefully': 3810, 'senate': 3811, \"valentine's\": 3812, 'dad.': 3813, '2004.': 3814, 'daddy': 3815, 'deserves': 3816, 'dream,': 3817, 'arguments': 3818, 'hindi': 3819, 'gross': 3820, 'pity': 3821, 'intense': 3822, 'can,': 3823, 'struggling': 3824, \"women's\": 3825, 'covers': 3826, 'maths': 3827, 'perfect.': 3828, 'firm': 3829, 'attractive': 3830, 'window.': 3831, 'torture': 3832, 'therefore,': 3833, 'moderate': 3834, 'farm': 3835, 'week!': 3836, 'method': 3837, 'prices': 3838, 'shot.': 3839, 'request': 3840, 'boring,': 3841, 'grant': 3842, 'factor': 3843, 'illegal': 3844, 'it..': 3845, 'importance': 3846, 'talk.': 3847, 'joining': 3848, 'merry': 3849, 'violent': 3850, 'christians': 3851, 'permanent': 3852, 'reading,': 3853, 'shorts': 3854, 'worrying': 3855, 'russian': 3856, 'worthy': 3857, 'dates': 3858, 'editor': 3859, 'road,': 3860, 'strongly': 3861, 'citizens': 3862, 'expression': 3863, 'voice.': 3864, 'start.': 3865, 'roommate': 3866, 'knees': 3867, 'cooked': 3868, 'lock': 3869, 'floating': 3870, 'connected': 3871, '(one': 3872, 'habit': 3873, 'dislike': 3874, 'signing': 3875, 'creepy': 3876, 'spin': 3877, 'ways.': 3878, 'happily': 3879, 'teen': 3880, 'murder': 3881, 'attempts': 3882, 'exciting.': 3883, 'cookies': 3884, 'cycle': 3885, 'construction': 3886, 'voices': 3887, 'deny': 3888, 'bullshit': 3889, 'anyone,': 3890, 'roof': 3891, 'phil': 3892, 'yep,': 3893, 'complicated': 3894, '(except': 3895, 'council': 3896, 'guns': 3897, 'volume': 3898, 'capital': 3899, 'age,': 3900, 'delicious': 3901, 'lauren': 3902, 'thanks,': 3903, 'pattern': 3904, 'organized': 3905, 'ministry': 3906, 'space.': 3907, 'but...': 3908, 'measure': 3909, 'bar.': 3910, 'museum': 3911, 'sources': 3912, 'amazingly': 3913, 'preparing': 3914, 'surrounding': 3915, 'ruin': 3916, 'sandwich': 3917, 'deeper': 3918, 'iron': 3919, 'earth.': 3920, 'dunno.': 3921, 'committed': 3922, 'chorus': 3923, 'prison': 3924, 'alive.': 3925, '(from': 3926, 'item': 3927, 'digest:': 3928, 'students.': 3929, 'gathering': 3930, 'limit': 3931, 'touching': 3932, 'virus': 3933, \"life's\": 3934, 'disturbing': 3935, 'decides': 3936, 'hehe,': 3937, 'somehow,': 3938, 'company,': 3939, 'capture': 3940, 'partly': 3941, 'stated': 3942, 'grad': 3943, 'torn': 3944, 'and...': 3945, 'weekend!': 3946, '\"when': 3947, 'tone': 3948, 'officer': 3949, 'goddamn': 3950, 'susan,': 3951, 'group,': 3952, 'buildings': 3953, 'gentle': 3954, 'assuming': 3955, 'otherwise,': 3956, 'chips': 3957, 'breathing': 3958, 'colin': 3959, 'jenny': 3960, 'gang': 3961, 'immediate': 3962, 'little,': 3963, 'pointless': 3964, 'cases': 3965, 'kicks': 3966, \"where's\": 3967, 'publish': 3968, 'hottie': 3969, 'julie': 3970, 'misses': 3971, '(she': 3972, 'strip': 3973, 'freezing': 3974, 'idea,': 3975, 'honest,': 3976, 'audio': 3977, 'thier': 3978, 'advanced': 3979, 'gary': 3980, 'does,': 3981, 'depression': 3982, 'shine': 3983, 'eggs': 3984, '(though': 3985, 'park,': 3986, 'erin': 3987, 'emailed': 3988, 'longest': 3989, 'test,': 3990, 'strategy': 3991, 'lecture': 3992, 'chief': 3993, 'rented': 3994, 'clothing': 3995, 'awkward': 3996, 'kitty': 3997, 'opens': 3998, 'selfish': 3999, 'granted': 4000, 'come,': 4001, 'regularly': 4002, 'write.': 4003, 'naturally': 4004, 'austin': 4005, 'control.': 4006, 'smiled': 4007, 'occasionally': 4008, 'you:': 4009, 'order.': 4010, 'dennis': 4011, 'reaching': 4012, 'everywhere.': 4013, 'canon': 4014, 'attempting': 4015, 'break,': 4016, 'accidentally': 4017, 'drum': 4018, 'phase': 4019, 'edit': 4020, 'concentrate': 4021, 'watch.': 4022, 'palm': 4023, 'again?': 4024, 'pointing': 4025, 'excitement': 4026, 'death,': 4027, 'cops': 4028, 'scale': 4029, 'artists': 4030, 'free,': 4031, 'glory': 4032, 'answer.': 4033, 'bloggers': 4034, 'panic': 4035, 'vanilla': 4036, 'solve': 4037, 'register': 4038, 'nuts': 4039, 'korean': 4040, 'academy': 4041, 'indeed,': 4042, 'survey': 4043, 'believes': 4044, 'powers': 4045, 'shell': 4046, 'simon': 4047, 'america,': 4048, 'thirty': 4049, 'elementary': 4050, 'film,': 4051, 'identify': 4052, 'ship': 4053, 'traveling': 4054, 'laughter': 4055, 'deleted': 4056, 'function': 4057, 'assistant': 4058, 'life...': 4059, 'reflect': 4060, 'directions': 4061, 'ghost': 4062, \"father's\": 4063, 'sensitive': 4064, 'choir': 4065, 'ignored': 4066, 'thoughts.': 4067, 'beneath': 4068, 'remembering': 4069, 'interviews': 4070, 'outsourcing': 4071, 'linked': 4072, 'jones': 4073, 'sheer': 4074, 'seemingly': 4075, 'essentially': 4076, 'drunken': 4077, 'relaxing': 4078, 'digest.': 4079, 'chain': 4080, 'shoe': 4081, '\"hey,': 4082, 'suggestions': 4083, 'graduated': 4084, 'wherever': 4085, 'hmmm...': 4086, 'cruise': 4087, 'shower.': 4088, 'ones,': 4089, 'supply': 4090, 'replied': 4091, 'sex,': 4092, '(just': 4093, 'clever': 4094, 'women.': 4095, 'cigarette': 4096, 'mins': 4097, 'shining': 4098, 'chem': 4099, 'album,': 4100, 'poll': 4101, 'rolls': 4102, 'sport': 4103, 'situations': 4104, 'prize': 4105, 'meets': 4106, 'truth.': 4107, 'arent': 4108, 'wierd': 4109, 'claimed': 4110, 'attempted': 4111, 'copies': 4112, 'treatment': 4113, 'rainy': 4114, 'awhile.': 4115, 'happier': 4116, 'christmas.': 4117, 'english,': 4118, 'visual': 4119, 'produced': 4120, 'were.': 4121, 'victory': 4122, 'vast': 4123, 'awesome,': 4124, 'hot,': 4125, 'root': 4126, 'quest': 4127, 'string': 4128, 'videos': 4129, 'editing': 4130, 'lives,': 4131, 'stolen': 4132, 'exception': 4133, 'bitch.': 4134, 'meanwhile,': 4135, 'recording': 4136, 'faced': 4137, 'customers': 4138, 'emotion': 4139, 'intended': 4140, 'involves': 4141, 'goes,': 4142, 'values': 4143, 'remaining': 4144, 'best,': 4145, 'buzz': 4146, 'dirt': 4147, 'collect': 4148, 'pillow': 4149, 'noon': 4150, 'luck.': 4151, 'god!': 4152, 'smooth': 4153, 'proved': 4154, 'topics': 4155, 'society.': 4156, \"week's\": 4157, 'soul.': 4158, 'committee': 4159, 'elected': 4160, 'hence': 4161, 'scored': 4162, 'up...': 4163, 'recognized': 4164, 'taylor': 4165, 'cute,': 4166, 'terror': 4167, 'kissed': 4168, 'like...': 4169, 'exhausted': 4170, 'rooms': 4171, 'strange.': 4172, 'authority': 4173, 'sex.': 4174, 'aspect': 4175, 'penny': 4176, 'beer.': 4177, 'news:': 4178, 'learnt': 4179, 'routine': 4180, 'though!': 4181, 'relief': 4182, 'voters': 4183, 'need.': 4184, 'truth,': 4185, 'weather.': 4186, 'basement': 4187, 'transfer': 4188, 'light,': 4189, 'wedding.': 4190, 'destruction': 4191, 'peace.': 4192, 'patient': 4193, 'mommy': 4194, 'well!': 4195, 'often,': 4196, 'mouth.': 4197, 'reasons.': 4198, 'inside,': 4199, 'knows.': 4200, 'folks.': 4201, 'high.': 4202, 'already,': 4203, 'explains': 4204, 'hire': 4205, 'conversation.': 4206, 'poems': 4207, 'awesome!': 4208, 'later!': 4209, 'chasing': 4210, 'ripped': 4211, 'thoroughly': 4212, 'bruce': 4213, 'enemy': 4214, 'potato': 4215, 'geek': 4216, 'installed': 4217, 'genius': 4218, 'joey': 4219, \"freakin'\": 4220, 'possible,': 4221, 'cartoon': 4222, 'dog.': 4223, 'sweat': 4224, 'knife': 4225, 'princess': 4226, 'britney': 4227, 'beliefs': 4228, 'fate': 4229, 'edwards': 4230, 'hopefully,': 4231, 'table.': 4232, 'hosting': 4233, 'layout': 4234, '\"that': 4235, 'education.': 4236, 'bass': 4237, 'typed': 4238, 'terrorists': 4239, 'area,': 4240, 'motion': 4241, 'less.': 4242, 'frame': 4243, 'grateful': 4244, 'mad.': 4245, 'caring': 4246, 'analysis': 4247, 'knee': 4248, 'waves': 4249, 'involving': 4250, 'homeless': 4251, 'damn.': 4252, 'resume': 4253, 'colour': 4254, 'real.': 4255, 'printed': 4256, 'native': 4257, 'explaining': 4258, 'wicked': 4259, 'downloaded': 4260, 'state,': 4261, 'practical': 4262, 'eastern': 4263, 'mentally': 4264, 'furniture': 4265, 'reality.': 4266, 'pound': 4267, 'proposed': 4268, 'me..': 4269, \"80's\": 4270, 'disgusting': 4271, 'belly': 4272, 'him...': 4273, 'holes': 4274, 'courtesy': 4275, 'here...': 4276, 'attacked': 4277, 'passes': 4278, 'feet.': 4279, 'patience': 4280, 'beloved': 4281, 'cloud': 4282, 'fifteen': 4283, 'idol': 4284, 'made.': 4285, 'insane.': 4286, 'edition': 4287, 'janet': 4288, 'else?': 4289, 'beef': 4290, 'nearby': 4291, 'men.': 4292, 'quickly.': 4293, 'aspects': 4294, 'flower': 4295, 'mother,': 4296, 'lesbian': 4297, 'matter,': 4298, 'die,': 4299, 'classes.': 4300, 'drive.': 4301, 'bush.': 4302, 'hugs': 4303, 'column': 4304, 'wonderful.': 4305, 'route': 4306, 'plants': 4307, 'tradition': 4308, 'rock.': 4309, 'retarded': 4310, 'kasi': 4311, 'blank': 4312, 'kidding': 4313, 'experience,': 4314, 'recorded': 4315, 'trial': 4316, 'switched': 4317, 'rest.': 4318, 'relevant': 4319, 'site:': 4320, 'pretending': 4321, \"guy's\": 4322, 'shuchomouff:': 4323, 'bored,': 4324, 'label': 4325, 'interests': 4326, 'important.': 4327, 'report.': 4328, 'underneath': 4329, 'new.': 4330, 'blown': 4331, 'forgetting': 4332, 'taco': 4333, 'dallas': 4334, 'wears': 4335, 'reasonable': 4336, 'spelling': 4337, 'rides': 4338, 'personally,': 4339, 'kisses': 4340, 'behavior': 4341, 'inspiration': 4342, 'run,': 4343, 'fuck.': 4344, 'help,': 4345, 'occasional': 4346, 'scientific': 4347, 'action.': 4348, 'burger': 4349, 'clothes,': 4350, 'ground.': 4351, 'feedback': 4352, 'heh,': 4353, 'answering': 4354, 'year!': 4355, 'newly': 4356, 'revolution': 4357, 'appeal': 4358, 'accomplished': 4359, 'randomly': 4360, 'talented': 4361, 'drops': 4362, 'tracks': 4363, 'gods': 4364, 'wal-mart': 4365, 'open.': 4366, 'had,': 4367, 'registered': 4368, 'zero': 4369, 'cookie': 4370, 'hunt': 4371, 'twisted': 4372, 'desert': 4373, 'attracted': 4374, '\"no,': 4375, 'though...': 4376, 'floor,': 4377, 'honey': 4378, 'ordinary': 4379, 'shortly': 4380, 'comments.': 4381, 'tale': 4382, 'rehearsal': 4383, 'sucked.': 4384, 'slide': 4385, 'chilled': 4386, 'confusing': 4387, 'confirmed': 4388, 'brave': 4389, 'wall.': 4390, 'concerning': 4391, 'surprisingly': 4392, 'sushi': 4393, 'distant': 4394, 'defend': 4395, 'install': 4396, 'longer.': 4397, 'danced': 4398, 'atleast': 4399, 'banana': 4400, 'do...': 4401, 'engaged': 4402, 'dude,': 4403, 'combined': 4404, 'neighbor': 4405, 'cheney': 4406, 'hockey': 4407, 'danger': 4408, 'women,': 4409, 'rushed': 4410, 'one?': 4411, 'electricity': 4412, 'error': 4413, 'fault.': 4414, 'association': 4415, 'delete': 4416, 'advance': 4417, 'protest': 4418, 'frankly,': 4419, 'pocket': 4420, 'utter': 4421, 'cure': 4422, 'shoes.': 4423, 'promises': 4424, 'johnson': 4425, 'cared': 4426, 'monster': 4427, 'jerry': 4428, 'lighter': 4429, 'wing': 4430, 'mean?': 4431, 'bottles': 4432, 'diary': 4433, 'out?': 4434, 'men,': 4435, 'dressing': 4436, 'plan.': 4437, \"fuckin'\": 4438, 'maybe.': 4439, 'use.': 4440, 'ideal': 4441, 'building.': 4442, 'email.': 4443, 'marching': 4444, 'sadly': 4445, 'miller': 4446, 'earn': 4447, 'commission': 4448, 'system,': 4449, 'verse': 4450, 'strikes': 4451, 'internal': 4452, 'manner': 4453, 'monitor': 4454, 'somewhere.': 4455, 'nicely': 4456, 'apparent': 4457, 'robin': 4458, 'condition': 4459, 'not?': 4460, 'pays': 4461, 'promote': 4462, 'dreams.': 4463, 'breast': 4464, 'shopping.': 4465, 'determine': 4466, 'extent': 4467, 'winner': 4468, 'income': 4469, '\"hey': 4470, 'megan': 4471, 'pouring': 4472, 'northern': 4473, 'rather,': 4474, 'hurts.': 4475, 'featured': 4476, 'voice,': 4477, 'tools': 4478, 'greatly': 4479, 'people!': 4480, 'clothes.': 4481, 'ass,': 4482, 'both.': 4483, 'storage': 4484, 'ceremony': 4485, 'fears': 4486, 'table,': 4487, 'programming': 4488, 'next.': 4489, 'bar,': 4490, 'coat': 4491, 'mama': 4492, 'brush': 4493, 'were,': 4494, 'reflection': 4495, 'emotionally': 4496, 'celebration': 4497, 'largely': 4498, 'stays': 4499, 'this!': 4500, 'participate': 4501, 'them...': 4502, 'trick': 4503, 'sun.': 4504, 'cough': 4505, 'roskilly': 4506, 'play,': 4507, 'oral': 4508, 'medicine': 4509, 'increasingly': 4510, 'recently.': 4511, 'referring': 4512, 'doing,': 4513, 'agent': 4514, 'instance,': 4515, 'taxes': 4516, \"can't.\": 4517, 'sense,': 4518, 'cups': 4519, 'whore': 4520, 'places.': 4521, 'uncomfortable': 4522, 'referred': 4523, 'forest': 4524, 'houston': 4525, 'thing...': 4526, 'being.': 4527, 'spam': 4528, 'cheesy': 4529, '2003.': 4530, 'chase': 4531, 'sony': 4532, 'pepper': 4533, 'employee': 4534, \"how's\": 4535, 'program.': 4536, 'meetings': 4537, 'entry.': 4538, 'policies': 4539, 'options': 4540, 'executive': 4541, 'coffee.': 4542, 'sounding': 4543, 'kid,': 4544, 'dull': 4545, 'amongst': 4546, 'cheaper': 4547, 'armed': 4548, 'study.': 4549, 'brad': 4550, 'website,': 4551, 'kingdom': 4552, 'documentary': 4553, 'dish': 4554, 'soon!': 4555, 'drink.': 4556, 'sweater': 4557, 'quietly': 4558, '(because': 4559, 'says.': 4560, 'way...': 4561, 'resolution': 4562, 'script': 4563, 'pledge': 4564, 'chip': 4565, 'thesis': 4566, 'another,': 4567, 'oven': 4568, 'poet': 4569, 'always,': 4570, 'him!': 4571, 'failing': 4572, 'lied': 4573, '[listening': 4574, 'couples': 4575, 'domestic': 4576, 'ready.': 4577, 'cancelled': 4578, 'same,': 4579, 'luckily,': 4580, 'bizarre': 4581, 'funeral': 4582, 'define': 4583, 'sunshine': 4584, 'desperately': 4585, 'leave,': 4586, 'essential': 4587, 'although,': 4588, 'smile,': 4589, 'high,': 4590, 'captain': 4591, 'backed': 4592, 'parked': 4593, 'commitment': 4594, 'surface': 4595, 'tube': 4596, 'him?': 4597, 'twin': 4598, 'charged': 4599, 'bond': 4600, 'rising': 4601, 'actors': 4602, 'context': 4603, 'poster': 4604, 'patrick': 4605, 'intend': 4606, 'foundation': 4607, 'reporters': 4608, 'kills': 4609, 'thanks.': 4610, 'gather': 4611, 'muscle': 4612, 'crashed': 4613, 'through,': 4614, 'hmmm.': 4615, 'chicks': 4616, 'prayers': 4617, 'represent': 4618, 'close.': 4619, 'dark,': 4620, 'states,': 4621, 'disagree': 4622, 'lane': 4623, 'courage': 4624, 'element': 4625, 'seattle': 4626, 'annoying.': 4627, 'body,': 4628, 'ways,': 4629, 'lasted': 4630, 'shadow': 4631, 'information.': 4632, 'dragged': 4633, 'sweet,': 4634, 'bare': 4635, 'wound': 4636, 'vehicle': 4637, 'one...': 4638, 'trip,': 4639, 'matthew': 4640, 'laugh,': 4641, 'slice': 4642, 'set.': 4643, 'soda': 4644, 'steak': 4645, 'follows': 4646, \"yesterday's\": 4647, 'stared': 4648, 'ask?': 4649, 'practice.': 4650, 'technically': 4651, 'smiles': 4652, 'president,': 4653, 'skinny': 4654, 'creation': 4655, 'horribly': 4656, 'sucks,': 4657, 'formal': 4658, 'pace': 4659, 'asshole': 4660, 'navy': 4661, 'domain': 4662, 'chance.': 4663, 'roads': 4664, 'volunteer': 4665, 'tower': 4666, 'cheers': 4667, 'ruined': 4668, 'deciding': 4669, 'peaceful': 4670, 'fast,': 4671, 'protection': 4672, 'hands,': 4673, 'been.': 4674, 'performed': 4675, 'bothers': 4676, 'coming.': 4677, 'supported': 4678, 'porch': 4679, 'wandering': 4680, 'spotted': 4681, 'frequently': 4682, 'brother.': 4683, 'much!': 4684, 'thoughts,': 4685, 'yet?': 4686, 'witness': 4687, 'list,': 4688, 'twelve': 4689, 'nails': 4690, 'unusual': 4691, 'anthony': 4692, 'delivered': 4693, 'went.': 4694, 'changed.': 4695, 'severe': 4696, 'cooler': 4697, 'ensure': 4698, 'guessed': 4699, 'song:': 4700, 'assignment': 4701, 'hired': 4702, 'reveal': 4703, 'admire': 4704, 'peoples': 4705, 'rain,': 4706, 'wooden': 4707, 'eternal': 4708, 'frankly': 4709, 'viewing': 4710, 'hills': 4711, 'yall': 4712, 'hurry': 4713, 'future,': 4714, 'indeed.': 4715, 'engineering': 4716, 'semester.': 4717, 'sour': 4718, 'henry': 4719, 'borrow': 4720, 'talk,': 4721, 'debt': 4722, 'feelings.': 4723, 'revealed': 4724, 'suppose.': 4725, 'bend': 4726, 'event.': 4727, 'camping': 4728, 'live,': 4729, '21st': 4730, 'expressed': 4731, 'concert.': 4732, 'aaron': 4733, 'tables': 4734, 'cinema': 4735, 'ford': 4736, 'landed': 4737, 'shoulders': 4738, 'kissing': 4739, 'oldest': 4740, 'secure': 4741, 'fire.': 4742, 'pictures,': 4743, 'curse': 4744, 'figures': 4745, 'contains': 4746, 'bryan': 4747, 'translation': 4748, 'yeay!': 4749, 'sadly,': 4750, 'nicole': 4751, 'highway': 4752, 'temporary': 4753, 'classical': 4754, 'thus,': 4755, 'shed': 4756, 'missions': 4757, 'situation,': 4758, 'go...': 4759, 'recipe': 4760, 'afterwards,': 4761, 'relaxed': 4762, 'sacrifice': 4763, 'germany': 4764, 'internet,': 4765, '(including': 4766, 'stranger': 4767, 'ashley': 4768, 'pulls': 4769, 'jake': 4770, 'performing': 4771, 'metro': 4772, 'box.': 4773, 'sticks': 4774, 'criminal': 4775, 'subjects': 4776, 'granted,': 4777, 'yup,': 4778, 'mexico': 4779, \"everything's\": 4780, 'mountains': 4781, '\"all': 4782, 'update:': 4783, 'believing': 4784, 'normal.': 4785, 'carol': 4786, 'spinning': 4787, 'monthly': 4788, 'itself,': 4789, 'royal': 4790, \"school's\": 4791, 'feeling:': 4792, 'sink': 4793, 'funny!': 4794, 'level.': 4795, 'buried': 4796, 'connect': 4797, 'stream': 4798, 'english.': 4799, 'romance': 4800, 'automatically': 4801, 'page,': 4802, 'ease': 4803, 'motivation': 4804, 'resist': 4805, 'adams': 4806, 'meaningful': 4807, 'everyday.': 4808, 'format': 4809, 'offended': 4810, 'describes': 4811, 'dunno,': 4812, 'beach.': 4813, 'ebay': 4814, 'teacher,': 4815, 'steel': 4816, '(all': 4817, 'examples': 4818, 'punch': 4819, 'conflict': 4820, '(what': 4821, 'stir': 4822, 'burst': 4823, 'helpful': 4824, 'enjoyable': 4825, 'mars': 4826, 'zone': 4827, 'guessing': 4828, 'nail': 4829, 'satisfied': 4830, 'pants.': 4831, 'launch': 4832, 'twins': 4833, 'child,': 4834, 'grade.': 4835, 'anxious': 4836, 'virtual': 4837, 'cape': 4838, 'visit.': 4839, 'win.': 4840, 'top.': 4841, 'chemical': 4842, 'cruel': 4843, 'increasing': 4844, 'celebrating': 4845, '...and': 4846, 'remote': 4847, 'hahaha': 4848, 'hoped': 4849, 'trained': 4850, 'approximately': 4851, 'colored': 4852, 'small,': 4853, 'busy,': 4854, 'featuring': 4855, 'operating': 4856, 'lightning': 4857, 'congratulations': 4858, 'courses': 4859, 'happening.': 4860, 'sheet': 4861, 'entering': 4862, 'dramatic': 4863, 'slap': 4864, 'assigned': 4865, 'marriage.': 4866, 'crawl': 4867, 'irish': 4868, 'assumed': 4869, 'wins': 4870, 'work!': 4871, 'card.': 4872, 'bday': 4873, 'surfing': 4874, 'cat.': 4875, 'meds': 4876, 'night?': 4877, 'indie': 4878, 'marked': 4879, 'dead,': 4880, 'mother.': 4881, 'ties': 4882, 'different,': 4883, 'acted': 4884, 'fortune': 4885, 'soap': 4886, 'argh.': 4887, 'problems,': 4888, 'hmmm,': 4889, 'ashamed': 4890, 'rite': 4891, 'accomplish': 4892, 'affected': 4893, 'form.': 4894, 'politicians': 4895, 'object': 4896, 'wonders': 4897, 'minute.': 4898, '\"that\\'s': 4899, 'logical': 4900, 'accused': 4901, 'style.': 4902, 'randy': 4903, 'fields': 4904, 'celebrated': 4905, 'differences': 4906, 'scares': 4907, 'phones': 4908, 'ironic': 4909, '\"there': 4910, 'thing?': 4911, 'surprising': 4912, 'lol..': 4913, 'general.': 4914, 'bastard': 4915, 'telephone': 4916, 'lawn': 4917, 'article,': 4918, 'skirt': 4919, 'explore': 4920, 'write,': 4921, 'folk': 4922, 'select': 4923, 'hyper': 4924, 'hurt,': 4925, 'closely': 4926, 'embrace': 4927, 'day?': 4928, 'bent': 4929, 'choosing': 4930, 'fever': 4931, 'leaning': 4932, 'archives': 4933, 'mysterious': 4934, 'fascinating': 4935, 'temperature': 4936, 'himself,': 4937, 'people?': 4938, 'glorious': 4939, 'trouble.': 4940, 'important,': 4941, 'nicki': 4942, 'worries': 4943, 'involve': 4944, 'imagined': 4945, 'submit': 4946, 'soundtrack': 4947, 'tends': 4948, 'easy,': 4949, 'real,': 4950, 'terry': 4951, 'raising': 4952, 'expected.': 4953, 'carpet': 4954, 'sun,': 4955, 'reunion': 4956, 'williams': 4957, 'shirt.': 4958, 'virtually': 4959, 'joint': 4960, 'leslie': 4961, 'lewis': 4962, 'young,': 4963, 'victoria': 4964, 'son,': 4965, 'permission': 4966, 'wife,': 4967, 'votes': 4968, 'trail': 4969, 'medium': 4970, '\"just': 4971, 'toys': 4972, 'rage': 4973, \"tomorrow's\": 4974, 'working,': 4975, 'knitting': 4976, 'it).': 4977, 'singapore': 4978, 'minus': 4979, 'wanted.': 4980, 'picture,': 4981, 'jen,': 4982, 'cracking': 4983, \"kerry's\": 4984, 'roses': 4985, 'along,': 4986, 'expectations': 4987, 'como': 4988, 'clip': 4989, 'explanation': 4990, 'shadows': 4991, 'skipped': 4992, 'environmental': 4993, 'team,': 4994, 'number.': 4995, 'flesh': 4996, 'forcing': 4997, 'messing': 4998, 'document': 4999}\n",
      "['that', 'have', 'with', 'this', 'just']\n"
     ]
    }
   ],
   "source": [
    "word_dict = dict_from_data(words_data)\n",
    "print(word_dict)\n",
    "train_data = [key for key in word_dict.keys()]\n",
    "print(train_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are gonna look a little how our words are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length word of the vocabulary: 6.2262\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths_vocab = [len(w) for w in train_data]\n",
    "mean_vocab = np.mean(lengths_vocab)\n",
    "print('Mean length word of the vocabulary: {}'.format(mean_vocab))\n",
    "\n",
    "plt.hist(lengths_vocab)\n",
    "plt.gca().set(title='Dictionary Histogram', ylabel='count words', xlabel='# letters per word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph contrast with the distribution of the length on the english words, which have a mean of 8.23 characters (as commented in http://www.ravi.io/language-word-lengths). But we have to take into acount that we didnt take as sample all the english dictionary, as the distribution of the english words lengths doesnt take into account the frequency, which is normally higher for shorter words, being more easily represented in smaller samples.\n",
    "\n",
    "Now we are gonna take a quick peek at the letter distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = []\n",
    "values = []\n",
    "\n",
    "for letter in string.ascii_lowercase:\n",
    "    sum_letter = 0\n",
    "    for word in train_data:\n",
    "        sum_letter += word.count(letter)\n",
    "    \n",
    "    count.append(sum_letter)\n",
    "    values.append(letter)\n",
    "    \n",
    "plt.bar(values, count)\n",
    "plt.gca().set(title='Letter Histogram', ylabel='count letters', xlabel='letter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution is almost identical as the distribution of the letters in the english vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "import random\n",
    "test_data = random.choices(words_data, k=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have checked that the dataset we are going to use consist of the 5000 most common words in the blogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 The dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we are going to construct in this notebook we will construct a feature representation which consists in representing each letter of each word as an integer. We will be using the Latin alphabet as a dictionary between letters and integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26} \n",
      "\n",
      "26 {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "letter2int = dict(zip(string.ascii_lowercase, range(1,27)))\n",
    "int2letter = {v: k for k, v in letter2int.items()}\n",
    "\n",
    "print(len(letter2int), letter2int, '\\n')\n",
    "print(len(int2letter), int2letter, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Transform the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word dictionary which allows us to transform the letters appearing in the words into integers, it is time to make use of it and convert our posts words to their integer sequence representation. Since we will be using a recurrent neural network, it will be convenient if the length of each word is the same. To do this, we will use the previously defined max size (the length of the word supercalifragilisticexpialidocious) as the fixed size for our words and then pad short words with the category 'no letter' (which we will label 0) and truncate long words.\n",
    "\n",
    "For now, the punctuations and numbers will be ignored, as it will simplify drastically the task. If there's enough time in the future, it could be possible to implement some kind of punctuation mark holder that allows to save the punctuation of the sentence and put them back after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2integer(letter_dict, word, pad = 34):\n",
    "    padding = 0 #we will pad with zeros\n",
    "    word_padded = []\n",
    "    length = len(word)\n",
    "    \n",
    "    #conversion\n",
    "    for letter_index, letter in enumerate(word):\n",
    "        if letter in letter_dict:\n",
    "            if letter_dict[letter] >= 0:\n",
    "                word_padded.append(letter_dict[letter])\n",
    "        else:\n",
    "            length -= 1\n",
    "    \n",
    "    #padding\n",
    "    if len(word_padded) < pad:\n",
    "        word_padded = (word_padded + pad * [padding])[:pad]\n",
    "            \n",
    "    return word_padded, length\n",
    "\n",
    "def sentence2integer(letter_dict, data, pad = 34):   \n",
    "    result = []\n",
    "    lengths = []\n",
    "    perc = 0        \n",
    "    \n",
    "    for idx_w, word in enumerate(data):\n",
    "        \n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} word = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "        \n",
    "        converted_word, len_word = word2integer(letter_dict, word, pad)\n",
    "        result.append(converted_word)\n",
    "        lengths.append(len_word)\n",
    "        \n",
    "    return result, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 5000 word = 0%\n",
      "500 / 5000 word = 10.0%\n",
      "1000 / 5000 word = 20.0%\n",
      "1501 / 5000 word = 30.0%\n",
      "2000 / 5000 word = 40.0%\n",
      "2500 / 5000 word = 50.0%\n",
      "3000 / 5000 word = 60.0%\n",
      "3500 / 5000 word = 70.0%\n",
      "4000 / 5000 word = 80.0%\n",
      "4500 / 5000 word = 90.0%\n",
      "0 / 10000 word = 0%\n",
      "1000 / 10000 word = 10.0%\n",
      "2000 / 10000 word = 20.0%\n",
      "3001 / 10000 word = 30.0%\n",
      "4000 / 10000 word = 40.0%\n",
      "5000 / 10000 word = 50.0%\n",
      "6000 / 10000 word = 60.0%\n",
      "7000 / 10000 word = 70.0%\n",
      "8000 / 10000 word = 80.0%\n",
      "9000 / 10000 word = 90.0%\n",
      "Previous lengths: 5000-5000 \n",
      "New length: 4745-4745\n"
     ]
    }
   ],
   "source": [
    "train_data_padded, train_data_padded_len = sentence2integer(letter2int, train_data)\n",
    "test_data_p, len_test_p = sentence2integer(letter2int, test_data)\n",
    "\n",
    "# There are some words in training that after the transformation will stay at less than 4 letters, so they will be disposed now.\n",
    "train_data_p = [] \n",
    "len_train_p = []\n",
    "for word, lenw in zip(train_data_padded, train_data_padded_len):\n",
    "    if lenw > 3:\n",
    "            train_data_p.append(word)\n",
    "            len_train_p.append(lenw)\n",
    "            \n",
    "print('Previous lengths: {}-{} \\nNew length: {}-{}'.format(len(train_data_padded), len(train_data_padded_len), \n",
    "                                                           len(train_data_p), len(len_train_p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 9, 20, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 4\n",
      "[6, 18, 15, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 4\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "for ind_w in [2, 8]:\n",
    "    print(train_data_p[ind_w], len_train_p[ind_w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = None\n",
    "blogs_data = None\n",
    "blogs_data_shuffled = None\n",
    "test_data = None\n",
    "train_data = None\n",
    "train_data_padded = None\n",
    "train_data_padded_len = None\n",
    "word = None\n",
    "word1 = None\n",
    "word_dict = None\n",
    "words = None\n",
    "words_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Jumble the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the objective is to get back the words after being jumbled, we have now to jumble this data. We create a function to jumble the words, and then another function to handle all the words in a sentence.\n",
    "\n",
    "Probabilistically, the result of the function could be the same word passed as input, but we will ignore this possible case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jumble_word(word, word_len):\n",
    "    if word_len <= 2:\n",
    "        word_j = word\n",
    "        \n",
    "    else:\n",
    "        sub_word = []\n",
    "        for w in word:\n",
    "            sub_word.append(w)\n",
    "\n",
    "        sub_word = sub_word[1:word_len-1]\n",
    "        shufled_word = shuffle(sub_word)\n",
    "        \n",
    "        word_j = word.copy()\n",
    "        word_j[1:word_len-1] = shufled_word\n",
    "        \n",
    "    return word_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 3, 2, 6, 5, 7, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Check it\n",
    "word = [1,2,3,4,5,6,7,0,0,0,0,0,0,0]\n",
    "len_w = 7\n",
    "print(jumble_word(word, len_w))\n",
    "\n",
    "word = [1,2,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "len_w = 2\n",
    "print(jumble_word(word, len_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jumble_data(data, data_len):\n",
    "    jumbled_data = []\n",
    "    \n",
    "    idx_w = 0\n",
    "    perc = 0\n",
    "    for word, w_len in zip(data, data_len):\n",
    "        jumbled_sentence = []\n",
    "\n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} words = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "\n",
    "        jumbled_word = jumble_word(word, w_len)\n",
    "\n",
    "        jumbled_data.append(jumbled_word)\n",
    "        idx_w+=1\n",
    "        \n",
    "    return jumbled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4745 words = 0%\n",
      "475 / 4745 words = 10.0%\n",
      "949 / 4745 words = 20.0%\n",
      "1424 / 4745 words = 30.0%\n",
      "1898 / 4745 words = 40.0%\n",
      "2373 / 4745 words = 50.0%\n",
      "2847 / 4745 words = 60.0%\n",
      "3322 / 4745 words = 70.0%\n",
      "3796 / 4745 words = 80.0%\n",
      "4271 / 4745 words = 90.0%\n",
      "0 / 10000 words = 0%\n",
      "1000 / 10000 words = 10.0%\n",
      "2000 / 10000 words = 20.0%\n",
      "3001 / 10000 words = 30.0%\n",
      "4000 / 10000 words = 40.0%\n",
      "5000 / 10000 words = 50.0%\n",
      "6000 / 10000 words = 60.0%\n",
      "7000 / 10000 words = 70.0%\n",
      "8000 / 10000 words = 80.0%\n",
      "9000 / 10000 words = 90.0%\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "jumbled_train = jumble_data(train_data_p, len_train_p)\n",
    "jumbled_test = jumble_data(test_data_p, len_test_p)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: [10, 21, 19, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length: 4\n",
      "Jumbled: [10, 21, 19, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Word: [12, 9, 22, 9, 14, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length: 6\n",
      "Jumbled: [12, 14, 9, 22, 9, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Word: [1, 2, 15, 21, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length: 5\n",
      "Jumbled: [1, 2, 21, 15, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Word: [15, 20, 8, 5, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length: 5\n",
      "Jumbled: [15, 20, 8, 5, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "for ind_w in [4, 6]:\n",
    "    print('Word: {}'.format(train_data_p[ind_w]))\n",
    "    print('Length: {}'.format(len_train_p[ind_w]))\n",
    "    print('Jumbled: {}\\n'.format(jumbled_train[ind_w]))\n",
    "    \n",
    "    print('Word: {}'.format(test_data_p[ind_w]))\n",
    "    print('Length: {}'.format(len_test_p[ind_w]))\n",
    "    print('Jumbled: {}\\n'.format(jumbled_test[ind_w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train input: 4745, train output: 4745, lengths train: 4745 \n",
      "test input: 10000, test output: 10000, lengths test: 10000\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Check sizes coincide\n",
    "print('train input: {}, train output: {}, lengths train: {} \\ntest input: {}, test output: {}, lengths test: {}'.format(\n",
    "    len(jumbled_train), len(train_data_p), len(len_train_p),\n",
    "    len(jumbled_test), len(test_data_p), len(len_test_p)))\n",
    "\n",
    "# Check the same numbers are used in both input and output on same sentence number \n",
    "for word1, word2 in zip(jumbled_train, train_data_p):\n",
    "    sum_num=0\n",
    "    \n",
    "    if len(word1) != len(word2):\n",
    "        print('errror of size in training!')\n",
    "\n",
    "    for indx in range(len(word1)):\n",
    "        sum_num += word1[indx]-word2[indx]\n",
    "    \n",
    "    if sum_num != 0:\n",
    "        print('error of numbers in training!')\n",
    "\n",
    "for word1, word2 in zip(jumbled_test, test_data_p):\n",
    "    sum_num=0\n",
    "    \n",
    "    if len(word1) != len(word2):\n",
    "        print('errror of size in test!')\n",
    "\n",
    "    for indx in range(len(word1)):\n",
    "        sum_num += word1[indx]-word2[indx]\n",
    "    \n",
    "    if sum_num != 0:\n",
    "        print('error of numbers in test!')\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1 Uploading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data to file: cache/data/input_train_data.pkl\n",
      "Wrote data to file: cache/data/output_train_data.pkl\n",
      "Wrote data to file: cache/data/length_train_data.pkl\n",
      "Wrote data to file: cache/data/input_test_data.pkl\n",
      "Wrote data to file: cache/data/output_test_data.pkl\n",
      "Wrote data to file: cache/data/length_test_data.pkl\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(data_dir): # Make sure that the folder exists, if not create it\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "# Uploading train and test files separately, we check it doesn't upload an empty file\n",
    "data_files = [jumbled_train, train_data_p, len_train_p,\n",
    "              jumbled_test, test_data_p, len_test_p]\n",
    "cache_files = ['input_train_data.pkl', 'output_train_data.pkl', 'length_train_data.pkl',\n",
    "               'input_test_data.pkl', 'output_test_data.pkl', 'length_test_data.pkl']\n",
    "\n",
    "for data_file, cache_file in zip(data_files, cache_files):\n",
    "    cache_data = None\n",
    "    cache_data = dict(data_file=data_file)\n",
    "    file_dir = os.path.join(data_dir, cache_file)\n",
    "\n",
    "    with open(file_dir, \"wb\") as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "        \n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        print(\"Wrote data to file:\", file_dir)\n",
    "    else:\n",
    "        print('Wrote empty file on file', file_dir)\n",
    "        \n",
    "print('Done')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are gonna clean some more variables as we will download them in the next sections or not gonna use them more\n",
    "len_data_p = None\n",
    "words_data_p = None\n",
    "jumbled_data = None\n",
    "trainX = None\n",
    "trainY = None\n",
    "train_len = None\n",
    "testX = None\n",
    "testY = None\n",
    "test_len = None\n",
    "word2 = None\n",
    "word1 = None\n",
    "word = None\n",
    "data_file = None\n",
    "data_files = None\n",
    "files = None\n",
    "cache_files = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2 Loading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cache_file, data_dir=data_dir):\n",
    "    cache_data = None\n",
    "    file_dir = os.path.join(data_dir, cache_file)\n",
    "\n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read data from file:\", file_dir)\n",
    "        except:\n",
    "            print('Problem reading the file', file_dir)\n",
    "    else:\n",
    "        print('File empty')\n",
    "\n",
    "    if cache_data is None:\n",
    "        print('Didnt read anything')\n",
    "        resulting_files = []\n",
    "    else:\n",
    "        resulting_files = (cache_data['data_file'])\n",
    "        \n",
    "    return resulting_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already done all the previous steps in the past, you should have the training and test split files in the cache/data folder. We will load them and the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from file: cache/data/input_train_data.pkl\n",
      "Read data from file: cache/data/input_test_data.pkl\n",
      "Read data from file: cache/data/output_train_data.pkl\n",
      "Read data from file: cache/data/output_test_data.pkl\n",
      "Read data from file: cache/data/length_train_data.pkl\n",
      "Read data from file: cache/data/length_test_data.pkl\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "cache_files = ['input_train_data.pkl', 'input_test_data.pkl',\n",
    "               'output_train_data.pkl', 'output_test_data.pkl',\n",
    "               'length_train_data.pkl', 'length_test_data.pkl']\n",
    "\n",
    "resulting_files = []\n",
    "for cache_file in cache_files:\n",
    "    resulting_files.append(load_data(cache_file, data_dir))\n",
    "        \n",
    "print('Done')\n",
    "\n",
    "trainX, testX, trainY, testY, train_len, test_len = resulting_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word trainX -> [12, 9, 11, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word trainY -> [12, 9, 11, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word train_len -> 4\n",
      "word testX -> [13, 11, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word testY -> [13, 1, 11, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "word test_len -> 4\n"
     ]
    }
   ],
   "source": [
    "# Check they have been loaded correctly\n",
    "ind = 5\n",
    "\n",
    "print('word trainX ->', trainX[ind])\n",
    "print('word trainY ->', trainY[ind])\n",
    "print('word train_len ->', str(train_len[ind]))\n",
    "\n",
    "print('word testX ->', testX[ind])\n",
    "print('word testY ->', testY[ind])\n",
    "print('word test_len ->', str(test_len[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Upload data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to upload the training dataset to S3 in order for our training code to access it. For now we will save both training and test locally and we will upload to S3 later on.\n",
    "\n",
    "It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form output[34], length, input[34] where input[34] represents the word, which is a sequence of 34 integers the letters in the words.\n",
    "\n",
    "We will save the training CSV and both dictionaries on the model directory, to upload later together in s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_dir): # Make sure that the folder exists if not, create it\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "pd.concat([pd.DataFrame(trainY), pd.DataFrame(train_len), pd.DataFrame(trainX)], axis=1) \\\n",
    "        .to_csv(os.path.join(model_dir, 'train.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([pd.DataFrame(testY), pd.DataFrame(test_len), pd.DataFrame(testX)], axis=1) \\\n",
    "        .to_csv(os.path.join(cache_dir, 'test.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, when we construct an endpoint which processes a submitted review we will need to make use of the dictionaries which we have created. As such, we will save them to a file now for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, 'letter2int_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(letter2int, f)\n",
    "with open(os.path.join(model_dir, 'int2letter_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(int2letter, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to load the dictionaries for any case from the files, it can be done using the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from file: letter2int_dict.pkl\n",
      "Read data from file: int2letter_dict.pkl\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "def load_dictionary(dict_file, dict_dir = cache_dir):\n",
    "    cache_data = None\n",
    "    file_dir = os.path.join(dict_dir, dict_file)\n",
    "\n",
    "    if os.path.getsize(file_dir) > 0:\n",
    "        try:\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read data from file:\", dict_file)\n",
    "        except:\n",
    "            print('Problem reading file', dict_file)\n",
    "    else:\n",
    "        print('File empty')\n",
    "\n",
    "    if cache_data is None:\n",
    "        print('Didnt read anything')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return cache_data\n",
    "\n",
    "# For example\n",
    "letter2int = load_dictionary('letter2int_dict.pkl', model_dir)\n",
    "int2letter = load_dictionary('int2letter_dict.pkl', model_dir)\n",
    "\n",
    "print(letter2int)\n",
    "print(int2letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = None\n",
    "testX = None\n",
    "trainY = None\n",
    "testY = None\n",
    "train_len = None\n",
    "test_len = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 To S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to upload the training data and the dictionaries to the SageMaker default S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-670005714529/sagemaker/capstoneProject\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/capstoneProject'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# upload training data to S3\n",
    "train_dir = os.path.join(cache_dir, 'train.csv')\n",
    "input_data = sagemaker_session.upload_data(path=model_dir, bucket=bucket, key_prefix=prefix)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you've uploaded the data, by printing the contents of the default bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-pytorch-2020-08-05-22-28-43-837/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-05-22-38-53-751/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-05-22-44-45-660/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-05-22-49-44-963/source/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-05-22-54-11-206/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-05-22-54-11-374/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-05-23-17-01-170/sourcedir.tar.gz\n",
      "sagemaker-pytorch-2020-08-05-23-40-21-590/sourcedir.tar.gz\n",
      "sagemaker/capstoneProject/int2letter_dict.pkl\n",
      "sagemaker/capstoneProject/letter2int_dict.pkl\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-05-22-28-43-837/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-05-22-28-43-837/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-05-22-38-53-751/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-05-22-38-53-751/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-05-22-44-45-660/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-05-22-44-45-660/output/model.tar.gz\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-05-22-49-44-963/debug-output/training_job_end.ts\n",
      "sagemaker/capstoneProject/sagemaker-pytorch-2020-08-05-22-49-44-963/output/model.tar.gz\n",
      "sagemaker/capstoneProject/train.csv\n"
     ]
    }
   ],
   "source": [
    "# iterate through S3 objects and print contents\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "     print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by implementing our own neural network in PyTorch along with a training script. The necessary files for them are in the source folder, being them: train.py, model.py, predict.py and util.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mEncoder\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_dim, hid_dim):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(Encoder, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.hid_dim = hid_dim\r\n",
      "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(input_dim, hid_dim, batch_first = \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, src):\r\n",
      "        outputs, (hidden, cell) = \u001b[36mself\u001b[39;49;00m.lstm(src)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m hidden, cell\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDecoder\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_dim, output_dim, hid_dim):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(Decoder, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.hid_dim = hid_dim\r\n",
      "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(output_dim, hid_dim, batch_first = \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.output_dim = output_dim\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc = nn.Linear(hid_dim, output_dim)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, src, hidden, cell):\r\n",
      "        trg, (hidden, cell) = \u001b[36mself\u001b[39;49;00m.lstm(src, (hidden, cell))\r\n",
      "        \u001b[37m# Reshaping the outputs such that it can be fit into the fully connected layer\u001b[39;49;00m\r\n",
      "        batch_size = np.shape(trg)[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        len_size = np.shape(trg)[\u001b[34m1\u001b[39;49;00m]\r\n",
      "\r\n",
      "        out = trg.contiguous().view(-\u001b[34m1\u001b[39;49;00m, \u001b[36mself\u001b[39;49;00m.hid_dim)\r\n",
      "        out = \u001b[36mself\u001b[39;49;00m.fc(out)\r\n",
      "        out = out.view(batch_size, len_size, -\u001b[34m1\u001b[39;49;00m)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m out\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mWordOrderer\u001b[39;49;00m(nn.Module):\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, encoder, decoder):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(WordOrderer, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.encoder = encoder\r\n",
      "        \u001b[36mself\u001b[39;49;00m.decoder = decoder\r\n",
      "        \u001b[34massert\u001b[39;49;00m encoder.hid_dim == decoder.hid_dim, \u001b[33m\"\u001b[39;49;00m\u001b[33mEncoder and Decoder don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt have the same dimensions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.letter2int_dict = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.int2letter_dict = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, jumbled):\r\n",
      "        hidden, cell = \u001b[36mself\u001b[39;49;00m.encoder(jumbled)\r\n",
      "\r\n",
      "        adapted = jumbled[:, \u001b[34m1\u001b[39;49;00m:, :\u001b[36mself\u001b[39;49;00m.decoder.output_dim] \u001b[37m#as we will not pass the first element to the Decoder\u001b[39;49;00m\r\n",
      "        \u001b[37m# which was the length of the word, and so the vocabulary size gets reduced to 27\u001b[39;49;00m\r\n",
      "        word = \u001b[36mself\u001b[39;49;00m.decoder(adapted, hidden, cell)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m word\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the implementation we can observe that there are is only one parameter that we may wish to tweak to improve the performance of our model. This is the hidden dimension.\n",
    "\n",
    "We didn't do it before because it increases the memory that would have been used for the CSV files, but the letters in each word passed to the model as input will have to be encoded on one-hot vectors. This can be done with the function defined two cells below (one_hot_encode).\n",
    "\n",
    "As the input we will pass the length of the word plus the word jumbled, meaning that the one-hot encoding has to be done taking into consideration these lengths too. As the maximum length will be of 34, which is bigger than the length of the letters dictionary (which is 27 considering the 0), the input dimension of the elements will be of 34, whereas the output dimension will be of 27 as we won't need the size anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will try to implement the training function to check its correct functioning. By doing this way, we avoid the larger time it takes if we would do it directly with the Pytorch Model as the loading time it takes to train every time is considerable, and we probably would have to call it several times, trying to fix errors that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (1.6.0)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from torch) (1.18.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float64)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size read from csv -> X: (4745, 35), Y: (4745, 34), len: (4745,)\n",
      "Input shape: (4745, 35, 34) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
      "Torch X: torch.Size([4745, 35, 34]) shape, torch.float32 type\n",
      "Torch Y: torch.Size([4745, 34]) shape, torch.int64 type\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Get the arrays from csv\n",
    "train_sample = pd.read_csv(os.path.join(model_dir, 'train.csv'), header=None, names=None)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = train_sample[train_sample.columns[0:34]]\n",
    "train_sample_len = train_sample[train_sample.columns[34]]\n",
    "train_sample_X = train_sample[train_sample.columns[34:69]] #this includes the word length as the first element\n",
    "print('Size read from csv -> X: {}, Y: {}, len: {}'.format(train_sample_X.shape, train_sample_y.shape, train_sample_len.shape))\n",
    "\n",
    "X_np = train_sample_X.to_numpy(copy=True)\n",
    "Y_np = train_sample_y.to_numpy(copy=True)\n",
    "len_np = train_sample_len.to_numpy(copy=True)\n",
    "\n",
    "# Encode the input sentence as one hot vectors\n",
    "dict_size = 34\n",
    "seq_len = 35\n",
    "batch_size = len(train_sample_X)\n",
    "input_seq = one_hot_encode(X_np, dict_size, seq_len, batch_size)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
    "\n",
    "train_torch_x = torch.tensor(input_seq).float().squeeze().clone()\n",
    "train_torch_x = torch.autograd.Variable(train_torch_x)\n",
    "train_torch_len = torch.tensor(len_np).float().squeeze().type(torch.long).clone()\n",
    "train_torch_y = torch.tensor(Y_np).float().squeeze().type(torch.long).clone()\n",
    "print('Torch X: {} shape, {} type\\nTorch Y: {} shape, {} type'.format(train_torch_x.shape, train_torch_x.dtype, \n",
    "                                                                      train_torch_y.shape, train_torch_y.dtype))\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_torch_x, train_torch_y, train_torch_len)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preparation and arguments as if it was the file\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_dim = 34\n",
    "output_dim = 27\n",
    "hid_dim = 128\n",
    "epochs = 15\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    total_length = len(train_loader.dataset)\n",
    "    model.train()\n",
    "    loss_return = []\n",
    "    increased = 0\n",
    "    loss_previous = 0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        batchs_done = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch_X, batch_y, batch_len = batch\n",
    "            len_batch = len(batch_X)\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            out = model(batch_X)\n",
    "            \n",
    "            batch_loss = 0\n",
    "            for result, target, len_word in zip(out, batch_y, batch_len):\n",
    "                loss = loss_fn(result[:len_word, :], target[:len_word])\n",
    "                batch_loss += loss\n",
    "    \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += batch_loss.data.item()        \n",
    "            batchs_done += len_batch\n",
    "#             print('Batch done. {} / {} inputs = {}%'.format(\n",
    "#                 batchs_done, total_length, np.round(batchs_done/total_length*100, decimals = 1)))\n",
    "\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))\n",
    "        loss_return.append(total_loss / len(train_loader))\n",
    "\n",
    "        # early stopping\n",
    "        if total_loss > loss_previous:\n",
    "            increased += 1\n",
    "            print('Increased ({})'.format(increased))\n",
    "        else: \n",
    "            increased = 0\n",
    "\n",
    "        loss_previous = total_loss\n",
    "\n",
    "        if increased >= 3:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "        \n",
    "    return loss_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 377.7928092354222\n",
      "Increased (1)\n",
      "Epoch: 2, Loss: 370.2957226602655\n",
      "Epoch: 3, Loss: 358.1821493851511\n",
      "Epoch: 4, Loss: 344.52776994203265\n",
      "Epoch: 5, Loss: 326.9527761559737\n",
      "Epoch: 6, Loss: 294.76953074806613\n",
      "Epoch: 7, Loss: 264.63692574752\n",
      "Epoch: 8, Loss: 249.554400996158\n",
      "Epoch: 9, Loss: 242.6716298555073\n",
      "Epoch: 10, Loss: 234.64951008244566\n",
      "Epoch: 11, Loss: 224.93494917217055\n",
      "Epoch: 12, Loss: 221.8300387733861\n",
      "Epoch: 13, Loss: 220.09389972686768\n",
      "Epoch: 14, Loss: 216.64295999627365\n",
      "Epoch: 15, Loss: 215.22402030543276\n"
     ]
    }
   ],
   "source": [
    "from source.model import WordOrderer, Decoder, Encoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(input_dim, hid_dim)\n",
    "decoder = Decoder(input_dim, output_dim, hid_dim)\n",
    "\n",
    "model = WordOrderer(encoder, decoder).to(device)\n",
    "\n",
    "# Train the model.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "losses = train(model, train_sample_dl, epochs, optimizer, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can make a plot to observe better\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although slow, we can observe that the model works and seems to improve over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will clean some variables that will not be used again\n",
    "train_sample = None\n",
    "train_sample_y = None\n",
    "train_sample_len = None\n",
    "train_sample_X = None\n",
    "input_seq = None\n",
    "X_np = None\n",
    "Y_np = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build and Train the PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training script works correctly, we will copy adequately on the train.py file.\n",
    "\n",
    "A typical training script:\n",
    "\n",
    "- Loads training data from a specified directory\n",
    "- Parses any training & model hyperparameters (ex. nodes in a neural network, training epochs, etc.)\n",
    "- Instantiates a model of your design, with any specified hyperparams\n",
    "- Trains that model\n",
    "- Finally, saves the model so that it can be hosted/deployed, later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m one_hot_encode\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m WordOrderer, Decoder, Encoder\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mobtain_data\u001b[39;49;00m(data_dir, namefile, batch_s):\r\n",
      "    \u001b[37m# Load the training data.\u001b[39;49;00m\r\n",
      "    train_sample = pd.read_csv(os.path.join(data_dir, namefile), header=\u001b[34mNone\u001b[39;49;00m, names=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoaded csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    train_sample_y = train_sample[train_sample.columns[\u001b[34m0\u001b[39;49;00m:\u001b[34m34\u001b[39;49;00m]]\r\n",
      "    train_sample_len = train_sample[train_sample.columns[\u001b[34m34\u001b[39;49;00m]]\r\n",
      "    train_sample_X = train_sample[train_sample.columns[\u001b[34m34\u001b[39;49;00m:\u001b[34m69\u001b[39;49;00m]]\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSize read from csv -> X: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, Y: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, len: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_sample_X.shape, train_sample_len.shape, train_sample_y.shape))\r\n",
      "\r\n",
      "    X_np = train_sample_X.to_numpy(copy=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    len_np = train_sample_len.to_numpy(copy=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    Y_np = train_sample_y.to_numpy(copy=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTo numpied\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    dict_size = \u001b[34m34\u001b[39;49;00m\r\n",
      "    seq_len = \u001b[34m35\u001b[39;49;00m\r\n",
      "    batch_size = \u001b[36mlen\u001b[39;49;00m(train_sample_X)\r\n",
      "    input_seq = one_hot_encode(X_np, dict_size, seq_len, batch_size)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mOne hot encoded\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    train_torch_x = torch.from_numpy(input_seq).float().squeeze()\r\n",
      "    train_torch_len = torch.from_numpy(len_np).float().squeeze().type(torch.long)\r\n",
      "    train_torch_y = torch.from_numpy(Y_np).float().squeeze().type(torch.long)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTorched\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    train_sample_ds = torch.utils.data.TensorDataset(train_torch_x, train_torch_y, train_torch_len)\r\n",
      "    train_loader = torch.utils.data.DataLoader(train_sample_ds, batch_size=batch_s)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain loaded\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_loader\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(model, train_loader, epochs, optimizer, loss_fn, device):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStart training\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    total_length = \u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\r\n",
      "    model.train()\r\n",
      "    loss_return = []\r\n",
      "    increased = \u001b[34m0\u001b[39;49;00m\r\n",
      "    loss_previous = \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, epochs + \u001b[34m1\u001b[39;49;00m):\r\n",
      "        batchs_done = \u001b[34m0\u001b[39;49;00m\r\n",
      "        total_loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[34mfor\u001b[39;49;00m batch \u001b[35min\u001b[39;49;00m train_loader:\r\n",
      "            batch_X, batch_y, batch_len = batch\r\n",
      "            len_batch = \u001b[36mlen\u001b[39;49;00m(batch_X)\r\n",
      "\r\n",
      "            batch_X = batch_X.to(device)\r\n",
      "            batch_y = batch_y.to(device)\r\n",
      "\r\n",
      "            out = model(batch_X)\r\n",
      "\r\n",
      "            batch_loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "            \u001b[34mfor\u001b[39;49;00m result, target, len_word \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(out, batch_y, batch_len):\r\n",
      "                loss = loss_fn(result[:len_word, :], target[:len_word])\r\n",
      "                batch_loss += loss\r\n",
      "\r\n",
      "            batch_loss.backward()\r\n",
      "            optimizer.step()\r\n",
      "\r\n",
      "            total_loss += batch_loss.data.item()\r\n",
      "            batchs_done += len_batch\r\n",
      "\u001b[37m#             print('Batch done. {} / {} inputs = {}%'.format(\u001b[39;49;00m\r\n",
      "\u001b[37m#                 batchs_done, total_length, np.round(batchs_done/total_length*100, decimals = 1)))\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, Loss: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epoch, total_loss / \u001b[36mlen\u001b[39;49;00m(train_loader)))\r\n",
      "        loss_return.append(total_loss / \u001b[36mlen\u001b[39;49;00m(train_loader))\r\n",
      "\r\n",
      "        \u001b[37m# early stopping\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m total_loss > loss_previous:\r\n",
      "            increased += \u001b[34m1\u001b[39;49;00m\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mIncreased (\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(increased))\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            increased = \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "        loss_previous = total_loss\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m increased >= \u001b[34m3\u001b[39;49;00m:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEarly stopping!\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            \u001b[34mbreak\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m loss_return\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    \u001b[37m# SageMaker Parameters\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \u001b[37m# Training Parameters\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m128\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mB\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 128)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m42\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 42)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Model Parameters\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m27\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33msize of the input dimension (default: 27)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m27\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mOD\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33msize of the output dimension (default: 27)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mHD\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33msize of the hidden dimension (default: 34)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    args = parser.parse_args()\r\n",
      "\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\r\n",
      "\r\n",
      "    torch.manual_seed(args.seed)\r\n",
      "\r\n",
      "    \u001b[37m#load dictionaries\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mletter2int_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        letter2int = pickle.load(f)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mint2letter_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        int2letter = pickle.load(f)\r\n",
      "\r\n",
      "    train_loader = obtain_data(args.data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.batch_size)\r\n",
      "\r\n",
      "    \u001b[37m# Build the model.\u001b[39;49;00m\r\n",
      "    encoder = Encoder(args.input_dim, args.hidden_dim)\r\n",
      "    decoder = Decoder(args.input_dim, args.output_dim, args.hidden_dim)\r\n",
      "    model = WordOrderer(encoder, decoder).to(device)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mint2letter_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.letter2int_dict = pickle.load(f)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mletter2int_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.int2letter_dict = pickle.load(f)\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel loaded with input_dim \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, output_dim \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, hidden_dim \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_dim, args.output_dim, args.hidden_dim))\r\n",
      "\r\n",
      "    \u001b[37m# Train the model.\u001b[39;49;00m\r\n",
      "    loss_function = nn.CrossEntropyLoss()\r\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mGoing to train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    train(model, train_loader, args.epochs, optimizer, loss_function, device)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrained\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Save the parameters used to construct the model\u001b[39;49;00m\r\n",
      "    model_info_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = {\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.input_dim,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33moutput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.output_dim,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.hidden_dim,\r\n",
      "        }\r\n",
      "        torch.save(model_info, f)\r\n",
      "\r\n",
      "\t\u001b[37m# Save the two letter2int_dict\u001b[39;49;00m\r\n",
      "    letter2int_dict_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mint2letter_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(letter2int_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        pickle.dump(model.letter2int_dict, f)\r\n",
      "\r\n",
      "    letter2int_dict_path2 = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mletter2int_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(letter2int_dict_path2, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        pickle.dump(model.int2letter_dict, f)\r\n",
      "\r\n",
      "\t\u001b[37m# Save the model parameters\u001b[39;49;00m\r\n",
      "    model_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        torch.save(model.cpu().state_dict(), f)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Create Pytorch Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point = 'train.py',\n",
    "                    source_dir = 'source',\n",
    "                    role = role,\n",
    "                    framework_version = '1.0',\n",
    "                    train_instance_count = 1,\n",
    "                    train_instance_type = 'ml.c4.xlarge',\n",
    "                    output_path = output_path,\n",
    "                    sagemaker_session = sagemaker_session,\n",
    "                    hyperparameters = {\n",
    "                        'input_dim': 34,\n",
    "                        'output_dim': 27,\n",
    "                        'hidden_dim': 128,\n",
    "                        'epochs': 60,\n",
    "                        'lr': 0.001,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Train the Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time, take it easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-06 01:08:05 Starting - Starting the training job...\n",
      "2020-08-06 01:08:07 Starting - Launching requested ML instances.........\n",
      "2020-08-06 01:09:40 Starting - Preparing the instances for training......\n",
      "2020-08-06 01:11:04 Downloading - Downloading input data...\n",
      "2020-08-06 01:11:17 Training - Downloading the training image.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-08-06 01:11:38,847 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-08-06 01:11:38,850 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-06 01:11:38,861 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-08-06 01:11:41,924 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-08-06 01:11:42,340 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-08-06 01:11:42,340 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-08-06 01:11:42,340 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-08-06 01:11:42,340 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pip (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/bd/b1/56a834acdbe23b486dea16aaf4c27ed28eb292695b90d01dff96c96597de/pip-20.2.1-py2.py3-none-any.whl (1.5MB)\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/f7/2adca20a7fa71b6a32f823bbd83992adeceab1d8bf72992bb7a55c69c19a/pandas-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/22/e7/4b2bdddb99f5f631d8c1de259897c2b7d65dcfcc1e0a6fd17a7f62923500/numpy-1.19.1-cp36-cp36m-manylinux1_x86_64.whl (13.4MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/66/25/ff030e2437265616a1e9b25ccc864e0371a0bc3adb7c5a404fd661c6f4f6/beautifulsoup4-4.9.1-py3-none-any.whl (115kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 2)) (2019.1)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2 (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 2)) (1.12.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ndqu5dwi/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pip, numpy, pandas, soupsieve, beautifulsoup4, train\n",
      "  Found existing installation: pip 18.1\n",
      "    Uninstalling pip-18.1:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled pip-18.1\n",
      "  Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.16.4\u001b[0m\n",
      "\u001b[34m  Found existing installation: pandas 0.24.2\u001b[0m\n",
      "\u001b[34m    Uninstalling pandas-0.24.2:\u001b[0m\n",
      "\n",
      "2020-08-06 01:11:36 Training - Training image download completed. Training in progress.\u001b[34m      Successfully uninstalled pandas-0.24.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed beautifulsoup4-4.9.1 numpy-1.19.1 pandas-1.1.0 pip-20.2.1 soupsieve-2.0.1 train-1.0.0\u001b[0m\n",
      "\u001b[34m2020-08-06 01:11:57,626 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-06 01:11:57,638 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"input_dim\": 34,\n",
      "        \"hidden_dim\": 128,\n",
      "        \"lr\": 0.001,\n",
      "        \"epochs\": 60,\n",
      "        \"output_dim\": 27\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-08-06-01-08-05-305\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-670005714529/sagemaker-pytorch-2020-08-06-01-08-05-305/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":60,\"hidden_dim\":128,\"input_dim\":34,\"lr\":0.001,\"output_dim\":27}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-670005714529/sagemaker-pytorch-2020-08-06-01-08-05-305/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":60,\"hidden_dim\":128,\"input_dim\":34,\"lr\":0.001,\"output_dim\":27},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2020-08-06-01-08-05-305\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-670005714529/sagemaker-pytorch-2020-08-06-01-08-05-305/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"60\",\"--hidden_dim\",\"128\",\"--input_dim\",\"34\",\"--lr\",\"0.001\",\"--output_dim\",\"27\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_DIM=34\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=128\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=60\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIM=27\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 60 --hidden_dim 128 --input_dim 34 --lr 0.001 --output_dim 27\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mLoaded csv\u001b[0m\n",
      "\u001b[34mSize read from csv -> X: (4745, 35), Y: (4745,), len: (4745, 34)\u001b[0m\n",
      "\u001b[34mTo numpied\u001b[0m\n",
      "\u001b[34mOne hot encoded\u001b[0m\n",
      "\u001b[34mTorched\u001b[0m\n",
      "\u001b[34mTrain loaded\u001b[0m\n",
      "\u001b[34mversion22\u001b[0m\n",
      "\u001b[34mModels\u001b[0m\n",
      "\u001b[34mModel loaded with input_dim 34, output_dim 27, hidden_dim 128.\u001b[0m\n",
      "\u001b[34mGoing to train\u001b[0m\n",
      "\u001b[34mEpoch: 1, Loss: 383.9351904015792\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 2, Loss: 366.5203259116725\u001b[0m\n",
      "\u001b[34mEpoch: 3, Loss: 356.67195425535505\u001b[0m\n",
      "\u001b[34mEpoch: 4, Loss: 343.3011505729274\u001b[0m\n",
      "\u001b[34mEpoch: 5, Loss: 328.1336931931345\u001b[0m\n",
      "\u001b[34mEpoch: 6, Loss: 295.1094745334826\u001b[0m\n",
      "\u001b[34mEpoch: 7, Loss: 266.57675707967655\u001b[0m\n",
      "\u001b[34mEpoch: 8, Loss: 237.29442912653872\u001b[0m\n",
      "\u001b[34mEpoch: 9, Loss: 229.84276856874166\u001b[0m\n",
      "\u001b[34mEpoch: 10, Loss: 225.7460810510736\u001b[0m\n",
      "\u001b[34mEpoch: 11, Loss: 217.30264849411813\u001b[0m\n",
      "\u001b[34mEpoch: 12, Loss: 217.53061018492045\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 13, Loss: 211.32671918367086\u001b[0m\n",
      "\u001b[34mEpoch: 14, Loss: 210.896711048327\u001b[0m\n",
      "\u001b[34mEpoch: 15, Loss: 209.6758263236598\u001b[0m\n",
      "\u001b[34mEpoch: 16, Loss: 209.59093520515844\u001b[0m\n",
      "\u001b[34mEpoch: 17, Loss: 201.21115825050757\u001b[0m\n",
      "\u001b[34mEpoch: 18, Loss: 199.65379559366326\u001b[0m\n",
      "\u001b[34mEpoch: 19, Loss: 197.94747101633172\u001b[0m\n",
      "\u001b[34mEpoch: 20, Loss: 195.17787853040193\u001b[0m\n",
      "\u001b[34mEpoch: 21, Loss: 194.2483283595035\u001b[0m\n",
      "\u001b[34mEpoch: 22, Loss: 191.12549053995232\u001b[0m\n",
      "\u001b[34mEpoch: 23, Loss: 190.0031610288118\u001b[0m\n",
      "\u001b[34mEpoch: 24, Loss: 187.59178392510665\u001b[0m\n",
      "\u001b[34mEpoch: 25, Loss: 186.2194501475284\u001b[0m\n",
      "\u001b[34mEpoch: 26, Loss: 184.73035556391665\u001b[0m\n",
      "\u001b[34mEpoch: 27, Loss: 183.44546388324937\u001b[0m\n",
      "\u001b[34mEpoch: 28, Loss: 182.93831925643119\u001b[0m\n",
      "\u001b[34mEpoch: 29, Loss: 181.1140506142064\u001b[0m\n",
      "\u001b[34mEpoch: 30, Loss: 179.37076684048301\u001b[0m\n",
      "\u001b[34mEpoch: 31, Loss: 178.43294188850805\u001b[0m\n",
      "\u001b[34mEpoch: 32, Loss: 177.83947327262476\u001b[0m\n",
      "\u001b[34mEpoch: 33, Loss: 177.17467930442407\u001b[0m\n",
      "\u001b[34mEpoch: 34, Loss: 175.74418278744346\u001b[0m\n",
      "\u001b[34mEpoch: 35, Loss: 173.96132996207788\u001b[0m\n",
      "\u001b[34mEpoch: 36, Loss: 172.72315356605932\u001b[0m\n",
      "\u001b[34mEpoch: 37, Loss: 172.417428066856\u001b[0m\n",
      "\u001b[34mEpoch: 38, Loss: 172.606286550823\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 39, Loss: 172.32960234190287\u001b[0m\n",
      "\u001b[34mEpoch: 40, Loss: 171.25649110894454\u001b[0m\n",
      "\u001b[34mEpoch: 41, Loss: 169.80005475094444\u001b[0m\n",
      "\u001b[34mEpoch: 42, Loss: 168.6427015003405\u001b[0m\n",
      "\u001b[34mEpoch: 43, Loss: 168.11098891810366\u001b[0m\n",
      "\u001b[34mEpoch: 44, Loss: 167.8944233342221\u001b[0m\n",
      "\u001b[34mEpoch: 45, Loss: 167.59990380939684\u001b[0m\n",
      "\u001b[34mEpoch: 46, Loss: 166.99352309578344\u001b[0m\n",
      "\u001b[34mEpoch: 47, Loss: 166.16087225863808\u001b[0m\n",
      "\u001b[34mEpoch: 48, Loss: 165.77844423996774\u001b[0m\n",
      "\u001b[34mEpoch: 49, Loss: 165.42355773323462\u001b[0m\n",
      "\u001b[34mEpoch: 50, Loss: 164.63608842147025\u001b[0m\n",
      "\u001b[34mEpoch: 51, Loss: 164.04729180586966\u001b[0m\n",
      "\u001b[34mEpoch: 52, Loss: 163.81530852066842\u001b[0m\n",
      "\u001b[34mEpoch: 53, Loss: 163.62691322125886\u001b[0m\n",
      "\u001b[34mEpoch: 54, Loss: 163.65262382908872\u001b[0m\n",
      "\u001b[34mIncreased (1)\u001b[0m\n",
      "\u001b[34mEpoch: 55, Loss: 163.36184697402152\u001b[0m\n",
      "\u001b[34mEpoch: 56, Loss: 162.6610689665142\u001b[0m\n",
      "\u001b[34mEpoch: 57, Loss: 162.4426779997976\u001b[0m\n",
      "\u001b[34mEpoch: 58, Loss: 161.86686626233552\u001b[0m\n",
      "\n",
      "2020-08-06 01:19:12 Uploading - Uploading generated training model\u001b[34mEpoch: 59, Loss: 161.35855127635756\u001b[0m\n",
      "\u001b[34mEpoch: 60, Loss: 160.84233374344674\u001b[0m\n",
      "\u001b[34mTrained\u001b[0m\n",
      "\u001b[34m2020-08-06 01:19:08,356 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-08-06 01:19:18 Completed - Training job completed\n",
      "Training seconds: 494\n",
      "Billable seconds: 494\n",
      "CPU times: user 1.61 s, sys: 60.7 ms, total: 1.67 s\n",
      "Wall time: 11min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# train the estimator on S3 training data\n",
    "estimator.fit({'train': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Prediction with the trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Getting letters back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function that allows obtaining the words back from the padded version to the shorter integer version, and from this integer version to the string version using the dictionaries. This will be done in the following functions.\n",
    "\n",
    "If the model returns all 0's (in the case it had a bad performance). The word will consist of a dot.\n",
    "\n",
    "Also, this will be useful if we want to show how the words in the sentence have been jumbled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer2word(number_dict, word, len_word):\n",
    "    \n",
    "    word_back = []\n",
    "    for letter_index, letter in enumerate(word):\n",
    "        if letter_index < len_word:\n",
    "            if letter in number_dict:\n",
    "                word_back.append(number_dict[letter])\n",
    "            else:\n",
    "                word_back.append('.')\n",
    "                        \n",
    "    return word_back\n",
    "\n",
    "\n",
    "def integer2sentence(number_dict, data, len_data):\n",
    "    result = []\n",
    "    \n",
    "    perc=0\n",
    "    idx_w = 0\n",
    "    for word, len_w in zip(data, len_data):\n",
    "        if idx_w / len(data) >= perc:\n",
    "            print('{} / {} words = {}%'.format(idx_w, len(data), np.round(perc*100, decimals = 1)))\n",
    "            perc = perc+0.1\n",
    "\n",
    "        converted_word = integer2word(number_dict, word, len_w)\n",
    "\n",
    "        result.append(converted_word)\n",
    "        idx_w += 1\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the input training data to check and we will see how the sentence would be seen once jumbled\n",
    "def join_sentence(sentence):\n",
    "    list_words = []\n",
    "    for word in sentence:\n",
    "        w = ''.join(word)\n",
    "        list_words.append(w)\n",
    "\n",
    "    sentence_joined = ' '.join(list_words)\n",
    "    \n",
    "    return sentence_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10 words = 0%\n",
      "1 / 10 words = 10.0%\n",
      "2 / 10 words = 20.0%\n",
      "4 / 10 words = 30.0%\n",
      "5 / 10 words = 40.0%\n",
      "6 / 10 words = 50.0%\n",
      "7 / 10 words = 60.0%\n",
      "8 / 10 words = 70.0%\n",
      "9 / 10 words = 80.0%\n",
      "0 / 10 words = 0%\n",
      "1 / 10 words = 10.0%\n",
      "2 / 10 words = 20.0%\n",
      "4 / 10 words = 30.0%\n",
      "5 / 10 words = 40.0%\n",
      "6 / 10 words = 50.0%\n",
      "7 / 10 words = 60.0%\n",
      "8 / 10 words = 70.0%\n",
      "9 / 10 words = 80.0%\n",
      "taht hvae with tihs just like abuot unlrilk form tehy \n",
      "<--> \n",
      "that have with this just like about urllink from they\n"
     ]
    }
   ],
   "source": [
    "# We will use the test dataset preparated, and assume that the sentence is the first 10 words of the file.\n",
    "test_sample = pd.read_csv(os.path.join(model_dir, 'train.csv'), header=None, names=None, nrows=10)\n",
    "test_sample_x = test_sample[test_sample.columns[34:69]].to_numpy(copy=True)\n",
    "test_sample_len = test_sample[test_sample.columns[34]].to_numpy(copy=True)\n",
    "test_sample_y = test_sample[test_sample.columns[0:34]].to_numpy(copy=True)\n",
    "\n",
    "# Check one sentence, we will consider 5 consecutive words from the training data as a sentence\n",
    "input_words = integer2sentence(int2letter, test_sample_x[:,1:], test_sample_len)\n",
    "output_words = integer2sentence(int2letter, test_sample_y, test_sample_len)\n",
    "\n",
    "print('{} \\n<--> \\n{}'.format(join_sentence(input_words), join_sentence(output_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Predict Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same as we did with the training script to check it works first. The input will be the word already transformed into the integers array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict the sentence as a string, we will transform the sentence into its integer jumbled form with the prepare_predict function. And after the prediction, we will have to transform them back with the read_prediction function, as they will expect to receive the same type of data they sended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_predict(sentence, dictionary):\n",
    "    input_data_words = post_to_words(sentence)\n",
    "    \n",
    "    print('Converting data')\n",
    "    integer_sentence, len_sentence = sentence2integer(dictionary, input_data_words)\n",
    "    print('Jumbling data')\n",
    "    jumbled_sentence = jumble_data(integer_sentence, len_sentence)\n",
    "    \n",
    "    #words with the length previously\n",
    "    for word, leng in zip(jumbled_sentence, len_sentence):\n",
    "        word.insert(0, leng)\n",
    "\n",
    "    return integer_sentence, jumbled_sentence, len_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prediction(prediction, lengths, dictionary):\n",
    "    return join_sentence(integer2sentence(dictionary, prediction, lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are less than 4 letters, will be returned as such without passing through the model, as no transformation is applied.\n",
    "\n",
    "The function will check if the input is a string array or an integer array, and in the case of the string, it will apply the functions necessary to have the adequate type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_input, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    integer_sentence = [] \n",
    "    for word in data_input:\n",
    "        word_batch = [word]\n",
    "\n",
    "        if len(word) > 3:\n",
    "            dict_size = 34\n",
    "            seq_len = 35\n",
    "            batch_size =1\n",
    "            test_seq = one_hot_encode(word_batch, dict_size, seq_len, batch_size)\n",
    "            \n",
    "            data = torch.from_numpy(test_seq).float().squeeze().to(device)\n",
    "            # Have the torch as a batch of size 1\n",
    "            data_batch = data.view(1, np.shape(data)[0], np.shape(data)[1])\n",
    "            # Make sure to put the model into evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model.forward(data_batch)\n",
    "                \n",
    "                word_integer = []\n",
    "                for letter in output[0]: #as there's only 1 batch\n",
    "                    letter_numpy = letter.numpy()\n",
    "                    max_value_ind = np.argmax(letter_numpy, axis=0)\n",
    "                    word_integer.append(max_value_ind)\n",
    "                \n",
    "        else:\n",
    "            word_integer = word_batch.copy()\n",
    "            \n",
    "        integer_sentence.append(word_integer)\n",
    "        \n",
    "    return integer_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10 words = 0%\n",
      "1 / 10 words = 10.0%\n",
      "2 / 10 words = 20.0%\n",
      "4 / 10 words = 30.0%\n",
      "5 / 10 words = 40.0%\n",
      "6 / 10 words = 50.0%\n",
      "7 / 10 words = 60.0%\n",
      "8 / 10 words = 70.0%\n",
      "9 / 10 words = 80.0%\n",
      "(1st word) Result  -> Ground Truth:\n",
      "[20, 1, 8, 20, 20, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "->\n",
      "[20  8  1 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "Resulting sentence: taht heae weth tihs just like ainnt unlrilk foon teey\n"
     ]
    }
   ],
   "source": [
    "int_result = predict(test_sample_x, model)\n",
    "str_result = read_prediction(int_result, test_sample_len, int2letter)\n",
    "print('(1st word) Result  -> Ground Truth:')\n",
    "print(int_result[0])\n",
    "print('->')\n",
    "print(test_sample_y[0])\n",
    "print('\\nResulting sentence: {}'.format(str_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to work correctly (ignoring the model performance). So on the next step now we will try it with a sentence string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data\n",
      "0 / 8 word = 0%\n",
      "1 / 8 word = 10.0%\n",
      "2 / 8 word = 20.0%\n",
      "3 / 8 word = 30.0%\n",
      "4 / 8 word = 40.0%\n",
      "5 / 8 word = 50.0%\n",
      "6 / 8 word = 60.0%\n",
      "7 / 8 word = 70.0%\n",
      "Jumbling data\n",
      "0 / 8 words = 0%\n",
      "1 / 8 words = 10.0%\n",
      "2 / 8 words = 20.0%\n",
      "3 / 8 words = 30.0%\n",
      "4 / 8 words = 40.0%\n",
      "5 / 8 words = 50.0%\n",
      "6 / 8 words = 60.0%\n",
      "7 / 8 words = 70.0%\n",
      "0 / 8 words = 0%\n",
      "1 / 8 words = 10.0%\n",
      "2 / 8 words = 20.0%\n",
      "3 / 8 words = 30.0%\n",
      "4 / 8 words = 40.0%\n",
      "5 / 8 words = 50.0%\n",
      "6 / 8 words = 60.0%\n",
      "7 / 8 words = 70.0%\n",
      "\n",
      "Resulting array: [[1, 20, 14, 14, 14, 14, 14, 14, 14, 9, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14], [19, 20, 21, 4, 25, 14, 7, 5, 5, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14], [6, 15, 15, 14, 9, 14, 14, 9, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14], [3, 1, 20, 19, 13, 4, 9, 18, 5, 5, 19, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14], [8, 1, 19, 19, 19, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14], [19, 5, 1, 20, 20, 4, 14, 5, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14], [20, 8, 5, 20, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14], [5, 5, 20, 20, 3, 5, 5, 18, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]]\n",
      "\n",
      "Resulting string: a study foon catsmdire has seattd thet eettceer\n",
      "\n",
      "Length original sentence was 8, length of returned sentence is 8\n"
     ]
    }
   ],
   "source": [
    "s = 'A study from Cambridge, has stated that etcetera'\n",
    "\n",
    "original_s_int, jumbled_s_int, data_len = prepare_predict(s, letter2int)\n",
    "output_array = predict(jumbled_s_int, model)\n",
    "output_string = read_prediction(output_array, data_len, int2letter)\n",
    "\n",
    "print('\\nResulting array:', output_array)\n",
    "print('\\nResulting string:', output_string)\n",
    "print('\\nLength original sentence was {}, length of returned sentence is {}'.format(len(s.split(' ')), \n",
    "                                                                                    len(output_string.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can observe that it works fine, the result depend on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Predict Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can deploy this custom PyTorch model, we have to take one more step: creating a PyTorchModel. This model is responsible for knowing how to execute a specific predict.py script and it is what we'll deploy to create an endpoint.\n",
    "\n",
    "Also don't forget to copy the predict function into the predict.py file and that there's a utils.py file, which contains the functions necessary to make all the others work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 ms, sys: 3.98 ms, total: 18.6 ms\n",
      "Wall time: 71.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# importing PyTorchModel\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Create a model from the trained estimator data\n",
    "# And point to the prediction script\n",
    "pmodel = PyTorchModel(model_data = estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version = '1.0',\n",
    "                     source_dir = 'source',\n",
    "                     entry_point = 'predict.py',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Deploy trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!CPU times: user 387 ms, sys: 12.4 ms, total: 400 ms\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# deploy and create a predictor\n",
    "predictor = pmodel.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is deployed, we can see how it performs when applied to the test data.\n",
    "\n",
    "The provided function below takes in a deployed predictor and the sentence and returns two metrics: the accuracy of words which has been fully correctly reconstructed and the accuracy of letters that have been positioned correctly in the sentence.\n",
    "\n",
    "Also if you want to try it with the predict function written on 4.2, you only have to replace the predictor.predict for a predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "      \n",
    "#Helping function\n",
    "def equalArrays(arr1, arr2):\n",
    "    equal = True\n",
    "    for i1, i2 in zip(arr1, arr2):\n",
    "        if i1!=i2:\n",
    "            equal = False\n",
    "    return equal\n",
    "\n",
    "# code to evaluate the endpoint on test data\n",
    "def evaluate(predictor, sentence, ground_truth, lengths, verbose=True):\n",
    "    \"\"\" Evaluate a model on a test set given the prediction endpoint.\"\"\"\n",
    "    \n",
    "    input_data = pd.DataFrame(sentence)\n",
    "    predict_array = predictor.predict(input_data)\n",
    "#     predict_array = predict(sentence, model)\n",
    "       \n",
    "    # Both sentences have to have the same number of words\n",
    "    if len(sentence) != len(ground_truth):\n",
    "        print('error in the length obtained')\n",
    "        return 0\n",
    "    \n",
    "    correct_w = 0\n",
    "    all_results = []\n",
    "    all_original = []\n",
    "    for original, result, length in zip(ground_truth, predict_array, lengths):\n",
    "        if equalArrays(original[:length], result[:length]) and length!=0:\n",
    "            correct_w += 1\n",
    "        for lo, lr in zip(original[:length], result[:length]):\n",
    "            all_results.append(lr)\n",
    "            all_original.append(lo)\n",
    "                \n",
    "            \n",
    "    accuracy_w = correct_w / len(sentence)\n",
    "#     accuracy_l = correct_l / num_letters\n",
    "    acc_l = accuracy_score(all_original, all_results)\n",
    "    cosine = np.mean(cosine_similarity(predict_array,ground_truth)[0])\n",
    "    cm = confusion_matrix(all_original, all_results)\n",
    "\n",
    "    return accuracy_w, acc_l, cm, cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to read the test.csv file to obtain all the test words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   0   1   2   3   4   5   6   7   8   ...  25  26  27  28  29  30  31  \\\n",
      "0   7   7   2   3  21  19   5   1   5   0  ...   0   0   0   0   0   0   0   \n",
      "\n",
      "   32  33  34  \n",
      "0   0   0   0  \n",
      "\n",
      "[1 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "evaluate_sample = pd.read_csv(os.path.join(cache_dir, 'test.csv'), header=None, names=None, nrows=1)\n",
    "eval_sample_x = evaluate_sample[evaluate_sample.columns[34:69]].to_numpy(copy=True)\n",
    "eval_sample_len = evaluate_sample[evaluate_sample.columns[34]].to_numpy(copy=True)\n",
    "\n",
    "test_data = pd.concat([pd.DataFrame(eval_sample_len), pd.DataFrame(eval_sample_x)], axis=1)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7, 18,  9, 21, 19,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "         5,  5]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Accuracy: 0.2294\n",
      "Letter Accuracy: 0.5989302906043859\n",
      "Mean Cosine Distance: 0.393210288529191\n"
     ]
    }
   ],
   "source": [
    "evaluate_sample = pd.read_csv(os.path.join(cache_dir, 'test.csv'), header=None, names=None).sample(n=5000)\n",
    "eval_sample_x = evaluate_sample[evaluate_sample.columns[34:69]].to_numpy(copy=True)\n",
    "eval_sample_len = evaluate_sample[evaluate_sample.columns[34]].to_numpy(copy=True)\n",
    "eval_sample_y = evaluate_sample[evaluate_sample.columns[0:34]].to_numpy(copy=True)\n",
    "\n",
    "acc_w, acc_l, cm, cosine = evaluate(predictor, eval_sample_x, eval_sample_y, eval_sample_len)\n",
    "# acc_w, acc_l, cm = evaluate(None, eval_sample_x, eval_sample_y, eval_sample_len)\n",
    "# acc_w, acc_l = evaluate(eval_sample_x, eval_sample_y, eval_sample_len, letter2int, None, None)\n",
    "print('Word Accuracy: {}'.format(acc_w))\n",
    "print('Letter Accuracy: {}'.format(acc_l))\n",
    "print('Mean Cosine Distance: {}'.format(cosine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that it doesn't reconstruct any word right and have an accuracy of 9% for the letters, what a shame! In the next section we will observe its results and evaluate them ourselves.\n",
    "\n",
    "But first, remember to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(30.5, 0.5, 'Ground Truth'), Text(0.5, 12.5, 'Predicted')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "ax = sns.heatmap(cm)\n",
    "ax.set(xlabel=\"Predicted\", ylabel = \"Ground Truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'e3f24714-646e-4cb6-bba2-c1057c1c6024',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e3f24714-646e-4cb6-bba2-c1057c1c6024',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 06 Aug 2020 04:46:42 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Use Model for Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although maybe its performance is not the best, our model is working, so it's time to create some custom inference code so that we can send the model a review which has not been processed and have it determine the sentiment of the review.\n",
    "\n",
    "As we saw above, by default the estimator which we created, when deployed, will use the entry script and directory which we provided when creating the model. However, since we now wish to accept a string as input and our model expects a processed review, we need to write some custom inference code (the predict file). For the rest of the files, we can copy the ones from the source directory to the serve directory and provide them to the SageMaker inference container to deploy a Pytorch model.\n",
    "\n",
    "For setting up everything I will follow the steps explained by Nadim Kawwa at https://github.com/NadimKawwa/rnn_sentiment_analysis/blob/master/SageMaker%20Project.ipynb, as they are well explained, understanable and concise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Deploy for Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the serve directory, the predict function will accept a text input and return a text too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m shuffle\r\n",
      "\r\n",
      "\u001b[37m# import model\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m WordOrderer, Encoder, Decoder\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m join_sentence, integer2sentence, one_hot_encode, prepare_predict, read_prediction\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    model_info = {}\r\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = torch.load(f)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\r\n",
      "\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    encoder = Encoder(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    decoder = Decoder(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33moutput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    model = WordOrderer(encoder, decoder).to(device)\r\n",
      "\r\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "\r\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "\r\n",
      "    \u001b[37m# Load the 2 saved letter2int_dict.\u001b[39;49;00m\r\n",
      "    letter2int_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mletter2int_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(letter2int_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.letter2int_dict = pickle.load(f)\r\n",
      "    int2letter_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mint2letter_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(int2letter_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.int2letter_dict = pickle.load(f)\r\n",
      "\r\n",
      "    model.to(device).eval()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m data\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mPredicting class labels for the input data...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m model.letter2int_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no letter2int_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m model.int2letter_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no int2letter_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    original_s_int, jumbled_s_int, data_len = prepare_predict(input_data, model.letter2int_dict)\r\n",
      "\r\n",
      "    integer_sentence = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m word \u001b[35min\u001b[39;49;00m jumbled_s_int:\r\n",
      "        word_batch = [word]\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(word) > \u001b[34m3\u001b[39;49;00m:\r\n",
      "            dict_size = \u001b[34m34\u001b[39;49;00m\r\n",
      "            seq_len = \u001b[34m35\u001b[39;49;00m\r\n",
      "            batch_size =\u001b[34m1\u001b[39;49;00m\r\n",
      "            test_seq = one_hot_encode(word_batch, dict_size, seq_len, batch_size)\r\n",
      "\r\n",
      "            data = torch.from_numpy(test_seq).float().squeeze().to(device)\r\n",
      "            \u001b[37m# Have the torch as a batch of size 1\u001b[39;49;00m\r\n",
      "            data_batch = data.view(\u001b[34m1\u001b[39;49;00m, np.shape(data)[\u001b[34m0\u001b[39;49;00m], np.shape(data)[\u001b[34m1\u001b[39;49;00m])\r\n",
      "\r\n",
      "            \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m\r\n",
      "            model.eval()\r\n",
      "            \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "                output = model.forward(data_batch)\r\n",
      "                word_integer = []\r\n",
      "\r\n",
      "                \u001b[34mfor\u001b[39;49;00m letter \u001b[35min\u001b[39;49;00m output[\u001b[34m0\u001b[39;49;00m]: \u001b[37m#as there's only 1 batch\u001b[39;49;00m\r\n",
      "                    letter_numpy = letter.numpy()\r\n",
      "                    max_value_ind = np.argmax(letter_numpy, axis=\u001b[34m0\u001b[39;49;00m)\r\n",
      "                    word_integer.append(max_value_ind)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            word_integer = word_batch.copy()\r\n",
      "\r\n",
      "        integer_sentence.append(word_integer)\r\n",
      "\r\n",
      "    output_string = read_prediction(integer_sentence, data_len, model.int2letter_dict)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m output_string\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize serve/predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-pytorch-2020-08-06-04-46-45-497'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Setting up lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we are going to do is set up a Lambda function. This Lambda function will be executed whenever our public API has data sent to it. When it is executed it will receive the data, perform any sort of processing that is required, send the data (the review) to the SageMaker endpoint we've created and then return the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Create an IAM Role for the Lambda function\n",
    "\n",
    "Since we want the Lambda function to call a SageMaker endpoint, we need to make sure that it has permission to do so. To do this, we will construct a role that we can later give the Lambda function.\n",
    "\n",
    "Using the AWS Console, navigate to the IAM page and click on Roles. Then, click on Create role. Make sure that the AWS service is the type of trusted entity selected and choose Lambda as the service that will use this role, then click Next: Permissions.\n",
    "\n",
    "In the search box type sagemaker and select the check box next to the AmazonSageMakerFullAccess policy. Then, click on Next: Review.\n",
    "\n",
    "Lastly, give this role a name. Make sure you use a name that you will remember later on, for example LambdaSageMakerRole. Then, click on Create role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Create a Lambda function\n",
    "\n",
    "Now it is time to actually create the Lambda function.\n",
    "\n",
    "Using the AWS Console, navigate to the AWS Lambda page and click on Create a function. When you get to the next page, make sure that Author from scratch is selected. Now, name your Lambda function, using a name that you will remember later on. Make sure that the Python 3.6 runtime is selected and then choose the role that you created in the previous part. Then, click on Create Function.\n",
    "\n",
    "On the next page you will see some information about the Lambda function you've just created. If you scroll down you should see an editor in which you can write the code that will be executed when your Lambda function is triggered. In this project, we will use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT_HERE',    # The name of the endpoint we created\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
    "                                       Body = event['body'])                       # The actual review\n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : result\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have copy and pasted the code above into the Lambda code editor, replace the **ENDPOINT NAME HERE** portion with the name of the endpoint that we deployed earlier. You can determine the name of the endpoint using the code cell below.\n",
    "\n",
    "And once you have added the endpoint name to the Lambda function, click on Save. Your Lambda function is now up and running. Next we need to create a way for our web app to execute the Lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Setting Up API Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our Lambda function is set up, it is time to create a new API using API Gateway that will trigger the Lambda function we have just created.\n",
    "\n",
    "Using AWS Console, navigate to Amazon API Gateway and then click on Get started.\n",
    "\n",
    "On the next page, make sure that New API is selected and give the new api a name. Then, click on Create API.\n",
    "\n",
    "Now we have created an API, however it doesn't currently do anything. What we want it to do is to trigger the Lambda function that we created earlier.\n",
    "\n",
    "Select the Actions dropdown menu and click Create Method. A new blank method will be created, select its dropdown menu and select POST, then click on the check mark beside it.\n",
    "\n",
    "For the integration point, make sure that Lambda Function is selected and click on the Use Lambda Proxy integration. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway.\n",
    "\n",
    "Type the name of the Lambda function you created earlier into the Lambda Function text entry box and then click on Save. Click on OK in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function you created.\n",
    "\n",
    "The last step in creating the API Gateway is to select the Actions dropdown and click on Deploy API. You will need to create a new Deployment stage and name it anything you like, for example prod.\n",
    "\n",
    "You have now successfully set up a public API to access your SageMaker model. Make sure to copy or write down the URL provided to invoke your newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text Invoke URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Deploying our web app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a publicly available API, we can start using it in a web app with the static HTML file which will make use of the public api you created earlier.\n",
    "\n",
    "In the website folder there should be a file called index.html. Download the file to your computer and open that file up in a text editor of your choice. You should replace the value of the **action = ...** line on the code with the url that you wrote down in the last step and then save the file.\n",
    "\n",
    "Now, if you open index.html on your local computer, your browser will behave as a local web server and you can use the provided site to interact with your SageMaker model.\n",
    "\n",
    "If you'd like to go further, you can host this html file anywhere you'd like, for example using github or hosting a static site on Amazon's S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Web results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finally deploying the web, we can try it with some sentences and see what it returns:\n",
    "    \n",
    "- this isn't an example sentence -> **this inst an elwalpe stanenne**\n",
    "\n",
    "- Despite the apparences, she stod among the best -> **detiese the acteeeeens she stod acnog the best**\n",
    "\n",
    "- You have to be more careful and read between the lines -> **you have to be more courfel and read betewen the lenns**\n",
    "\n",
    "- Did you know that, along with gorgeous architecture, it's home to the largest tamale? -> **did you know taat annig watt gugslues acerrctrhice ins hume to the laggest tallle*\n",
    "\n",
    "This results coincide with the numerical results obtained earlier, but you can try twisting the model parameters and see if yours obtain better results.\n",
    "\n",
    "**With this we have finally completed this project notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Delet the endpoint & Final cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the predictor endpoint \n",
    "boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to always shut down your endpoint if you are no longer using it. You are charged for the length of time that the endpoint is running so if you forget and leave it on you could end up with an unexpectedly large bill. Also look to clean the S3 bucket, the models, endpoints configurations, lambda functions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
